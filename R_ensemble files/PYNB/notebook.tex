
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{fraud\_model}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{earnings-manipulation}{%
\section{Earnings Manipulation}\label{earnings-manipulation}}

\hypertarget{by-kumar-rahul}{%
\subsection{By Kumar Rahul}\label{by-kumar-rahul}}

The analysis is on company financial manipulations and devise algorithm
to identify a manipulater from a non manipulater based on the financial
ratios reported by the companies. There are a total of 1239 observations
in the data set. Out of these 1239 observations, there are 1200 non
manipulaters and 39 manipulaters.

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{http://topepo.github.io/caret/train-models-by-tag.html}{Look}
  for different types of model which can be built using R. Also has a
  guideline for fine tuning paramters
\item
  Refer
  \href{http://stats.stackexchange.com/questions/163799/training-a-random-forest-in-r-with-a-maximum-false-positive-rate}{link}
  to know random forest and
  \href{http://stackoverflow.com/questions/18541923/what-is-out-of-bag-error-in-random-forests}{Refer}
  to know about OOB error
\item
  \href{https://rpubs.com/chengjiun/52658}{Demonstration} of some of the
  bagging and boosting algorithm
\item
  \href{https://www.r-bloggers.com/improve-predictive-performance-in-r-with-bagging/}{Understand}
  the logic for bagging in logistic regression
\item
  \href{http://stackoverflow.com/questions/14996619/random-forest-output-interpretation}{Interpret}
  the tree structure generated out of random forest model
\end{enumerate}
\end{quote}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    Not all the packages are available for installation through anaconda
r-essentials. To install the packages which are not available through
anaconda framework, use the below code chunk:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{}install.packages(\PYZdq{}inTrees\PYZdq{}, \PYZdq{}/Users/Rahul/anaconda3/lib/R/library\PYZdq{})}
        \PY{c+c1}{\PYZsh{}install.packages(\PYZdq{}DMwR\PYZdq{}, \PYZdq{}/Users/Rahul/anaconda3/lib/R/library\PYZdq{})}
        \PY{c+c1}{\PYZsh{}install.packages(\PYZdq{}UBL\PYZdq{}, \PYZdq{}/Users/Rahul/anaconda3/lib/R/library\PYZdq{})}
        \PY{c+c1}{\PYZsh{}install.packages(\PYZdq{}adabag\PYZdq{}, \PYZdq{}/Users/Rahul/anaconda3/lib/R/library\PYZdq{})}
        \PY{c+c1}{\PYZsh{}install.packages(\PYZdq{}tictoc\PYZdq{}, \PYZdq{}/Users/Rahul/anaconda3/lib/R/library\PYZdq{})}
        \PY{c+c1}{\PYZsh{}install.packages(\PYZdq{}doMC\PYZdq{}, \PYZdq{}/Users/Rahul/anaconda3/lib/R/library\PYZdq{})}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{library}\PY{p}{(}caret\PY{p}{)}          \PY{c+c1}{\PYZsh{}for split and model accuracy}
        \PY{k+kn}{library}\PY{p}{(}DMwR\PY{p}{)}           \PY{c+c1}{\PYZsh{}for SMOTE Sampling}
        \PY{k+kn}{library}\PY{p}{(}randomForest\PY{p}{)}
        \PY{k+kn}{library}\PY{p}{(}ROCR\PY{p}{)}           \PY{c+c1}{\PYZsh{}for ROC Plot}
        \PY{k+kn}{library}\PY{p}{(}e1071\PY{p}{)}
        \PY{k+kn}{library}\PY{p}{(}xgboost\PY{p}{)}        \PY{c+c1}{\PYZsh{}to implement xgbTree}
        \PY{c+c1}{\PYZsh{}library(rattle)        \PYZsh{}print the business rules for the model}
        \PY{k+kn}{library}\PY{p}{(}inTrees\PY{p}{)}        \PY{c+c1}{\PYZsh{}to extract the business rules from rf model}
        \PY{k+kn}{library}\PY{p}{(}UBL\PY{p}{)}
        \PY{k+kn}{library}\PY{p}{(}tictoc\PY{p}{)}         \PY{c+c1}{\PYZsh{}to record the time elapsed}
        \PY{k+kn}{library}\PY{p}{(}parallel\PY{p}{)}
        \PY{k+kn}{library}\PY{p}{(}doParallel\PY{p}{)}
        \PY{k+kn}{library}\PY{p}{(}doMC\PY{p}{)}
        \PY{k+kp}{setwd}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{/Users/Rahul/Documents/Rahul Office/IIMB/Work @ IIMB/Company Fraud\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Loading required package: lattice
Loading required package: ggplot2
Loading required package: grid
randomForest 4.6-12
Type rfNews() to see new features/changes/bug fixes.

Attaching package: ‘randomForest’

The following object is masked from ‘package:ggplot2’:

    margin

Loading required package: gplots

Attaching package: ‘gplots’

The following object is masked from ‘package:stats’:

    lowess

Loading required package: MBA
Loading required package: gstat
Loading required package: automap
Loading required package: sp
Loading required package: foreach
Loading required package: iterators

    \end{Verbatim}

    \hypertarget{preparing-data}{%
\subsection{Preparing data}\label{preparing-data}}

\hypertarget{read-data-from-a-specified-location}{%
\paragraph{Read data from a specified
location}\label{read-data-from-a-specified-location}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} raw\PYZus{}data \PY{o}{\PYZlt{}\PYZhy{}} read.csv\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{/Users/Rahul/Documents/Rahul Office/IIMB/Work @ IIMB/Company Fraud/fraud\PYZus{}data.csv\PYZdq{}}\PY{p}{,}
                             head\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{,}na.strings\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{ \PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{NA\PYZdq{}}\PY{p}{)}\PY{p}{,} sep\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{,\PYZdq{}}\PY{p}{)}
        
        filter\PYZus{}data \PY{o}{\PYZlt{}\PYZhy{}} raw\PYZus{}data\PY{p}{[}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    \hypertarget{define-an-7030-traintest-split-of-the-dataset}{%
\paragraph{Define an 70\%/30\% train/test split of the
dataset}\label{define-an-7030-traintest-split-of-the-dataset}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
        trainIndex \PY{o}{\PYZlt{}\PYZhy{}} createDataPartition\PY{p}{(}filter\PYZus{}data\PY{o}{\PYZdl{}}Manipulater\PY{p}{,} p \PY{o}{=} \PY{l+m}{0.70}\PY{p}{,} \PY{k+kt}{list}\PY{o}{=}\PY{k+kc}{FALSE}\PY{p}{)}
        train\PYZus{}df \PY{o}{\PYZlt{}\PYZhy{}} filter\PYZus{}data\PY{p}{[} trainIndex\PY{p}{,}\PY{p}{]}
        test\PYZus{}df \PY{o}{\PYZlt{}\PYZhy{}} filter\PYZus{}data\PY{p}{[}\PY{o}{\PYZhy{}}trainIndex\PY{p}{,}\PY{p}{]}
\end{Verbatim}


    \hypertarget{prepare-and-run-numerical-summaries}{%
\paragraph{Prepare and run numerical
summaries}\label{prepare-and-run-numerical-summaries}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kp}{summary}\PY{p}{(}train\PYZus{}df\PY{p}{)} \PY{c+c1}{\PYZsh{}summary of the data}
        train\PYZus{}df \PY{o}{\PYZlt{}\PYZhy{}} na.omit\PY{p}{(}train\PYZus{}df\PY{p}{)} \PY{c+c1}{\PYZsh{} listwise deletion of missing}
        test\PYZus{}df \PY{o}{\PYZlt{}\PYZhy{}} na.omit\PY{p}{(}test\PYZus{}df\PY{p}{)} \PY{c+c1}{\PYZsh{} listwise deletion of missing}
\end{Verbatim}


    
    \begin{verbatim}
      DSRI              GMI                AQI                SGI          
 Min.   : 0.0000   Min.   :-20.8118   Min.   :-21.7338   Min.   : 0.06454  
 1st Qu.: 0.8876   1st Qu.:  0.9253   1st Qu.:  0.7856   1st Qu.: 0.97341  
 Median : 1.0200   Median :  1.0000   Median :  1.0079   Median : 1.09614  
 Mean   : 1.1387   Mean   :  0.9778   Mean   :  1.0763   Mean   : 1.13740  
 3rd Qu.: 1.1872   3rd Qu.:  1.0507   3rd Qu.:  1.2110   3rd Qu.: 1.20608  
 Max.   :15.3435   Max.   : 46.4667   Max.   : 52.8867   Max.   :13.06465  
      DEPI              SGAI              ACCR               LEVI        
 Min.   :0.06882   Min.   : 0.0000   Min.   :-0.68226   Min.   : 0.0000  
 1st Qu.:0.93554   1st Qu.: 0.9008   1st Qu.:-0.07631   1st Qu.: 0.9232  
 Median :1.00000   Median : 1.0002   Median :-0.03004   Median : 1.0133  
 Mean   :1.02915   Mean   : 1.1073   Mean   :-0.03045   Mean   : 1.0574  
 3rd Qu.:1.07637   3rd Qu.: 1.1290   3rd Qu.: 0.02016   3rd Qu.: 1.1154  
 Max.   :5.39387   Max.   :49.3018   Max.   : 0.95989   Max.   :13.0586  
 Manipulater C.MANIPULATOR    
 No :840     Min.   :0.00000  
 Yes: 28     1st Qu.:0.00000  
             Median :0.00000  
             Mean   :0.03226  
             3rd Qu.:0.00000  
             Max.   :1.00000  
    \end{verbatim}

    
    \hypertarget{train-and-test-dataset-with-needed-variables}{%
\paragraph{Train and test dataset with needed
variables}\label{train-and-test-dataset-with-needed-variables}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} model\PYZus{}df \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{as.data.frame}\PY{p}{(}filter\PYZus{}data\PY{p}{[}\PY{p}{,}\PY{k+kt}{c}\PY{p}{(}\PY{c+c1}{\PYZsh{}\PYZdq{}DSRI\PYZdq{},}
                                               \PY{c+c1}{\PYZsh{}\PYZdq{}GMI\PYZdq{},}
                                               \PY{l+s}{\PYZdq{}}\PY{l+s}{AQI\PYZdq{}}\PY{p}{,}
                                               \PY{c+c1}{\PYZsh{}\PYZdq{}SGI\PYZdq{},}
                                               \PY{l+s}{\PYZdq{}}\PY{l+s}{DEPI\PYZdq{}}\PY{p}{,}
                                               \PY{l+s}{\PYZdq{}}\PY{l+s}{SGAI\PYZdq{}}\PY{p}{,}
                                               \PY{l+s}{\PYZdq{}}\PY{l+s}{ACCR\PYZdq{}}\PY{p}{,}
                                               \PY{l+s}{\PYZdq{}}\PY{l+s}{LEVI\PYZdq{}}\PY{p}{,}
                                               \PY{l+s}{\PYZdq{}}\PY{l+s}{Manipulater\PYZdq{}}
        \PY{p}{)}\PY{p}{]}\PY{p}{)}
        
        model\PYZus{}train\PYZus{}df \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{as.data.frame}\PY{p}{(}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{k+kt}{c}\PY{p}{(}\PY{c+c1}{\PYZsh{}\PYZdq{}DSRI\PYZdq{},}
                                                  \PY{c+c1}{\PYZsh{}\PYZdq{}GMI\PYZdq{},}
                                                  \PY{l+s}{\PYZdq{}}\PY{l+s}{AQI\PYZdq{}}\PY{p}{,}
                                                  \PY{c+c1}{\PYZsh{}\PYZdq{}SGI\PYZdq{},}
                                                  \PY{l+s}{\PYZdq{}}\PY{l+s}{DEPI\PYZdq{}}\PY{p}{,}
                                                  \PY{l+s}{\PYZdq{}}\PY{l+s}{SGAI\PYZdq{}}\PY{p}{,}
                                                  \PY{l+s}{\PYZdq{}}\PY{l+s}{ACCR\PYZdq{}}\PY{p}{,}
                                                  \PY{l+s}{\PYZdq{}}\PY{l+s}{LEVI\PYZdq{}}\PY{p}{,}
                                                  \PY{l+s}{\PYZdq{}}\PY{l+s}{Manipulater\PYZdq{}}
        \PY{p}{)}\PY{p}{]}\PY{p}{)}
        
        model\PYZus{}test\PYZus{}df \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{as.data.frame}\PY{p}{(}test\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{k+kt}{c}\PY{p}{(}\PY{c+c1}{\PYZsh{}\PYZdq{}DSRI\PYZdq{},}
                                                \PY{c+c1}{\PYZsh{}\PYZdq{}GMI\PYZdq{},}
                                                \PY{l+s}{\PYZdq{}}\PY{l+s}{AQI\PYZdq{}}\PY{p}{,}
                                                \PY{c+c1}{\PYZsh{}\PYZdq{}SGI\PYZdq{},}
                                                \PY{l+s}{\PYZdq{}}\PY{l+s}{DEPI\PYZdq{}}\PY{p}{,}
                                                \PY{l+s}{\PYZdq{}}\PY{l+s}{SGAI\PYZdq{}}\PY{p}{,}
                                                \PY{l+s}{\PYZdq{}}\PY{l+s}{ACCR\PYZdq{}}\PY{p}{,}
                                                \PY{l+s}{\PYZdq{}}\PY{l+s}{LEVI\PYZdq{}}\PY{p}{,}
                                                \PY{l+s}{\PYZdq{}}\PY{l+s}{Manipulater\PYZdq{}}
        \PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \hypertarget{corelation-amongst-variable}{%
\paragraph{Corelation amongst
variable}\label{corelation-amongst-variable}}

The below chunk of code will show the co-relation if any between the
numerical variables. The function \textbf{highlyCorelated()} shows the
variables which are corelated with an absolute corelation of more than
0.6. In this case there are no variables which are highly corelated.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} correlation\PYZus{}matrix \PY{o}{\PYZlt{}\PYZhy{}} cor\PY{p}{(}model\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{)}\PY{p}{]}\PY{p}{)}
        \PY{k+kp}{print}\PY{p}{(}correlation\PYZus{}matrix\PY{p}{)}
        \PY{c+c1}{\PYZsh{} find attributes that are highly corrected (ideally \PYZgt{}0.7)}
        highly\PYZus{}correlated \PY{o}{\PYZlt{}\PYZhy{}} findCorrelation\PY{p}{(}correlation\PYZus{}matrix\PY{p}{,} cutoff \PY{o}{=} \PY{l+m}{0.6}\PY{p}{,} names \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{)}
        \PY{k+kp}{print}\PY{p}{(}highly\PYZus{}correlated\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
              AQI        DEPI         SGAI        ACCR        LEVI
AQI   1.000000000 -0.02124161  0.003712316 -0.04542383  0.07027302
DEPI -0.021241615  1.00000000 -0.067247329 -0.01661336 -0.01271157
SGAI  0.003712316 -0.06724733  1.000000000 -0.09066795  0.02174950
ACCR -0.045423827 -0.01661336 -0.090667950  1.00000000 -0.01163113
LEVI  0.070273016 -0.01271157  0.021749500 -0.01163113  1.00000000
character(0)

    \end{Verbatim}

    \hypertarget{caret-package}{%
\subsection{Caret Package}\label{caret-package}}

\textbf{caret} is a useful and a robust package which helps to set a
generic framework to implement any kind of model in R. Some of the
algorithm's which can be implemented using caret package are:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k+kp}{names}\PY{p}{(}getModelInfo\PY{p}{(}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}getModelInfo()\PYZdl{}glm}
\end{Verbatim}


    \begin{enumerate*}
\item 'ada'
\item 'AdaBag'
\item 'AdaBoost.M1'
\item 'adaboost'
\item 'amdai'
\item 'ANFIS'
\item 'avNNet'
\item 'awnb'
\item 'awtan'
\item 'bag'
\item 'bagEarth'
\item 'bagEarthGCV'
\item 'bagFDA'
\item 'bagFDAGCV'
\item 'bam'
\item 'bartMachine'
\item 'bayesglm'
\item 'binda'
\item 'blackboost'
\item 'blasso'
\item 'blassoAveraged'
\item 'bridge'
\item 'brnn'
\item 'BstLm'
\item 'bstSm'
\item 'bstTree'
\item 'C5.0'
\item 'C5.0Cost'
\item 'C5.0Rules'
\item 'C5.0Tree'
\item 'cforest'
\item 'chaid'
\item 'CSimca'
\item 'ctree'
\item 'ctree2'
\item 'cubist'
\item 'dda'
\item 'deepboost'
\item 'DENFIS'
\item 'dnn'
\item 'dwdLinear'
\item 'dwdPoly'
\item 'dwdRadial'
\item 'earth'
\item 'elm'
\item 'enet'
\item 'evtree'
\item 'extraTrees'
\item 'fda'
\item 'FH.GBML'
\item 'FIR.DM'
\item 'foba'
\item 'FRBCS.CHI'
\item 'FRBCS.W'
\item 'FS.HGD'
\item 'gam'
\item 'gamboost'
\item 'gamLoess'
\item 'gamSpline'
\item 'gaussprLinear'
\item 'gaussprPoly'
\item 'gaussprRadial'
\item 'gbm\_h2o'
\item 'gbm'
\item 'gcvEarth'
\item 'GFS.FR.MOGUL'
\item 'GFS.LT.RS'
\item 'GFS.THRIFT'
\item 'glm.nb'
\item 'glm'
\item 'glmboost'
\item 'glmnet\_h2o'
\item 'glmnet'
\item 'glmStepAIC'
\item 'gpls'
\item 'hda'
\item 'hdda'
\item 'hdrda'
\item 'HYFIS'
\item 'icr'
\item 'J48'
\item 'JRip'
\item 'kernelpls'
\item 'kknn'
\item 'knn'
\item 'krlsPoly'
\item 'krlsRadial'
\item 'lars'
\item 'lars2'
\item 'lasso'
\item 'lda'
\item 'lda2'
\item 'leapBackward'
\item 'leapForward'
\item 'leapSeq'
\item 'Linda'
\item 'lm'
\item 'lmStepAIC'
\item 'LMT'
\item 'loclda'
\item 'logicBag'
\item 'LogitBoost'
\item 'logreg'
\item 'lssvmLinear'
\item 'lssvmPoly'
\item 'lssvmRadial'
\item 'lvq'
\item 'M5'
\item 'M5Rules'
\item 'manb'
\item 'mda'
\item 'Mlda'
\item 'mlp'
\item 'mlpKerasDecay'
\item 'mlpKerasDecayCost'
\item 'mlpKerasDropout'
\item 'mlpKerasDropoutCost'
\item 'mlpML'
\item 'mlpSGD'
\item 'mlpWeightDecay'
\item 'mlpWeightDecayML'
\item 'monmlp'
\item 'msaenet'
\item 'multinom'
\item 'mxnet'
\item 'mxnetAdam'
\item 'naive\_bayes'
\item 'nb'
\item 'nbDiscrete'
\item 'nbSearch'
\item 'neuralnet'
\item 'nnet'
\item 'nnls'
\item 'nodeHarvest'
\item 'null'
\item 'OneR'
\item 'ordinalNet'
\item 'ORFlog'
\item 'ORFpls'
\item 'ORFridge'
\item 'ORFsvm'
\item 'ownn'
\item 'pam'
\item 'parRF'
\item 'PART'
\item 'partDSA'
\item 'pcaNNet'
\item 'pcr'
\item 'pda'
\item 'pda2'
\item 'penalized'
\item 'PenalizedLDA'
\item 'plr'
\item 'pls'
\item 'plsRglm'
\item 'polr'
\item 'ppr'
\item 'PRIM'
\item 'protoclass'
\item 'pythonKnnReg'
\item 'qda'
\item 'QdaCov'
\item 'qrf'
\item 'qrnn'
\item 'randomGLM'
\item 'ranger'
\item 'rbf'
\item 'rbfDDA'
\item 'Rborist'
\item 'rda'
\item 'regLogistic'
\item 'relaxo'
\item 'rf'
\item 'rFerns'
\item 'RFlda'
\item 'rfRules'
\item 'ridge'
\item 'rlda'
\item 'rlm'
\item 'rmda'
\item 'rocc'
\item 'rotationForest'
\item 'rotationForestCp'
\item 'rpart'
\item 'rpart1SE'
\item 'rpart2'
\item 'rpartCost'
\item 'rpartScore'
\item 'rqlasso'
\item 'rqnc'
\item 'RRF'
\item 'RRFglobal'
\item 'rrlda'
\item 'RSimca'
\item 'rvmLinear'
\item 'rvmPoly'
\item 'rvmRadial'
\item 'SBC'
\item 'sda'
\item 'sdwd'
\item 'simpls'
\item 'SLAVE'
\item 'slda'
\item 'smda'
\item 'snn'
\item 'sparseLDA'
\item 'spikeslab'
\item 'spls'
\item 'stepLDA'
\item 'stepQDA'
\item 'superpc'
\item 'svmBoundrangeString'
\item 'svmExpoString'
\item 'svmLinear'
\item 'svmLinear2'
\item 'svmLinear3'
\item 'svmLinearWeights'
\item 'svmLinearWeights2'
\item 'svmPoly'
\item 'svmRadial'
\item 'svmRadialCost'
\item 'svmRadialSigma'
\item 'svmRadialWeights'
\item 'svmSpectrumString'
\item 'tan'
\item 'tanSearch'
\item 'treebag'
\item 'vbmpRadial'
\item 'vglmAdjCat'
\item 'vglmContRatio'
\item 'vglmCumulative'
\item 'widekernelpls'
\item 'WM'
\item 'wsrf'
\item 'xgbDART'
\item 'xgbLinear'
\item 'xgbTree'
\item 'xyf'
\end{enumerate*}


    
    \hypertarget{bagging-model}{%
\subsection{Bagging Model}\label{bagging-model}}

Bagging is the process of taking bootstrap sample and then aggreagting
the model learned on each sample. Each of the models are trained
independently on the N observations picked randomly from N observations
in the original dataset (with replacement). The models can be trained
parallely as the training is based on independent samples. Since models
are trained on different but overlapping samples of the original data,
the predictions from different models will be different.

\hypertarget{bagging-models-in-r}{%
\subsubsection{Bagging models in R}\label{bagging-models-in-r}}

The algorithms in bagging are:

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Bagged Adaboost: \textbf{\emph{adabag()}} Required Package is
  \textbf{adabag, plyr}
\item
  Bagged CART: \textbf{\emph{treebag()}} Required Package is
  \textbf{ipred, e1071, plyr}
\item
  Bagged Flexible Discriminant Analysis: \textbf{\emph{bagFDA()}}
  Required Package is \textbf{earth, mda}
\item
  Bagged Logic Regression: \textbf{\emph{logicBag()}} Required Package
  is \textbf{logicFS}
\item
  Bagged MARS: \textbf{\emph{bagEarth()}} Required Package is
  \textbf{earth}
\item
  Bagged Model: \textbf{\emph{bag()}} Required Package is \textbf{caret}
\item
  Ensemble of Generalized Linear Models: \textbf{\emph{randomGLM()}}
  Required Package is \textbf{randomGLM}
\item
  Model Averaged Neural Network: \textbf{\emph{avNNET()}} Required
  Package is \textbf{nnet}
\item
  Quantile Regression Neural Network: \textbf{\emph{qrnn()}} Required
  Package is \textbf{qrnn}
\item
  Random Ferns: \textbf{\emph{rFerns()}} Required Package is
  \textbf{rFerns}
\end{enumerate}
\end{quote}

\emph{The below methods are all applicable to implement random forest as
a bagging algorithm:}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  Parallel Random Forest: \textbf{\emph{parRF()}} Required Package is
  \textbf{e1071, randomForest, foreach}
\item
  Quantile Random Forest: \textbf{\emph{qrf()}} Required Package is
  \textbf{quantregForest}
\item
  Conditional Inference Random Forest: \textbf{\emph{cforest()}}
  Required Package is \textbf{party}
\item
  Random Forest: \textbf{\emph{ranger()}} Required Package is
  \textbf{e1071, ranger}
\item
  Random Forest: \textbf{\emph{Rborist()}} Required Package is
  \textbf{Rborist}
\item
  Random Forest: \textbf{\emph{rf()}} Required Package is
  \textbf{randomForest}
\item
  Random Forest by Randomization: \textbf{\emph{extraTrees()}} Required
  Package is \textbf{extraTrees}
\item
  Random Forest rule based Model: \textbf{\emph{rfRules()}} Required
  Package is \textbf{randomForest, inTrees, plyr}
\item
  Regularized Random Forest: \textbf{\emph{RRF()}} Required Package is
  \textbf{randomForest, RRF}
\item
  Regularized Random Forest: \textbf{\emph{RRFglobal()}} Required
  Package is \textbf{RRF}
\item
  Weighted Subspace Random Forest: \textbf{\emph{wsrf()}} Required
  Package is \textbf{wsrf}
\end{enumerate}
\end{quote}

\#\#\#Random Forest with bootstrap sampling Random forests is one of the
algorithm which uses bagging as a technique. In the below code chunk we
will use bootstrap sampling to implement bagging using rf method. This
means that if there are 100 observations in a training dataset the
resulting sample will select 100 samples with replacement.

The below code chunk sets some of the control parameters

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} tic\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Total Time for Bagging and Boosting\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} objControl \PY{o}{\PYZlt{}\PYZhy{}} trainControl\PY{p}{(}method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{boot\PYZsq{}}\PY{p}{,} number \PY{o}{=} \PY{l+m}{1}\PY{p}{,}
                                    returnResamp\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{none\PYZsq{}}\PY{p}{,}
                                    summaryFunction \PY{o}{=} twoClassSummary\PY{p}{,}
                                    savePredictions \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    classProbs \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}allowParallel\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{)}
\end{Verbatim}


    After setting the control paramters, the model is run

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} num\PYZus{}cores \PY{o}{\PYZlt{}\PYZhy{}} makeCluster\PY{p}{(}detectCores\PY{p}{(}\PY{p}{)}\PY{l+m}{\PYZhy{}5}\PY{p}{)}
         num\PYZus{}cores
\end{Verbatim}


    
    \begin{verbatim}
socket cluster with 3 nodes on host ‘localhost’
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} num\PYZus{}cores \PY{o}{\PYZlt{}\PYZhy{}} makeCluster\PY{p}{(}detectCores\PY{p}{(}\PY{p}{)}\PY{l+m}{\PYZhy{}5}\PY{p}{)}
         registerDoParallel\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         tic\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{RF Bagging with Bootstrap Sample\PYZdq{}}\PY{p}{)}
         
         \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         rf\PYZus{}bootstrap\PYZus{}model \PY{o}{\PYZlt{}\PYZhy{}} train\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{,}
                           method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{rf\PYZsq{}}\PY{p}{,}
                           trControl\PY{o}{=}objControl\PY{p}{,} ntree \PY{o}{=} \PY{l+m}{500}\PY{p}{,}
                           metric \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{ROC\PYZdq{}}\PY{p}{)}
         stopCluster\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         toc\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
RF Bagging with Bootstrap Sample: 5.247 sec elapsed

    \end{Verbatim}

    Confusion Matrix for bootstrap sampling on train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{}rf\PYZus{}bootstrap\PYZus{}model\PYZdl{}finalModel \PYZsh{}rf\PYZus{}bootstrap\PYZus{}model\PYZdl{}results}
         \PY{k+kp}{print}\PY{p}{(}rf\PYZus{}bootstrap\PYZus{}model\PY{p}{)}
         confusionMatrix.train\PY{p}{(}rf\PYZus{}bootstrap\PYZus{}model\PY{p}{)}
         plot\PY{p}{(}varImp\PY{p}{(}rf\PYZus{}bootstrap\PYZus{}model\PY{p}{)}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable importance from Bootstrap Random Forest\PYZdq{}}\PY{p}{,} col \PY{o}{=} \PY{l+m}{2}\PY{p}{,} lwd \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Random Forest 

868 samples
  5 predictor
  2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Bootstrapped (1 reps) 
Summary of sample sizes: 868 
Resampling results across tuning parameters:

  mtry  ROC        Sens       Spec
  2     0.9036885  0.9967213  0   
  3     0.9180328  0.9934426  0   
  5     0.9206967  0.9901639  0   

ROC was used to select the optimal model using the largest value.
The final value used for the model was mtry = 5.

    \end{Verbatim}

    
    \begin{verbatim}
Bootstrapped (1 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   No  Yes
       No  96.5  2.6
       Yes  1.0  0.0
                            
 Accuracy (average) : 0.9649

    \end{verbatim}

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_23_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Confusion Matrix for bootstrap sampling on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} caretPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}rf\PYZus{}bootstrap\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{raw\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}caretPredictedClass\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  358  10
       Yes   2   1
                                          
               Accuracy : 0.9677          
                 95% CI : (0.9442, 0.9832)
    No Information Rate : 0.9704          
    P-Value [Acc > NIR] : 0.69036         
                                          
                  Kappa : 0.1318          
 Mcnemar's Test P-Value : 0.04331         
                                          
            Sensitivity : 0.99444         
            Specificity : 0.09091         
         Pos Pred Value : 0.97283         
         Neg Pred Value : 0.33333         
             Prevalence : 0.97035         
         Detection Rate : 0.96496         
   Detection Prevalence : 0.99191         
      Balanced Accuracy : 0.54268         
                                          
       'Positive' Class : No              
                                          
    \end{verbatim}

    
    ROC plot for bootstrap random forest on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} rf\PYZus{}bootstrap\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}rf\PYZus{}bootstrap\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         rf\PYZus{}bootstrap\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}rf\PYZus{}bootstrap\PYZus{}pred\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         rf\PYZus{}bootstrap\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}rf\PYZus{}bootstrap\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}rf\PYZus{}bootstrap\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for bootstrap Random Forest\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}rf\PYZus{}bootstrap\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.8438131


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_27_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The best model was

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} rf\PYZus{}bootstrap\PYZus{}model\PY{o}{\PYZdl{}}bestTune
\end{Verbatim}


    \begin{tabular}{r|l}
  & mtry\\
\hline
	3 & 5\\
\end{tabular}


    
    Visulaizing the rules coming out of random forest. We can loop and print
all the trees built using up sampling. For simplicity, printing just one
of the trees

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} getTree\PY{p}{(}rf\PYZus{}bootstrap\PYZus{}model\PY{o}{\PYZdl{}}finalModel\PY{p}{,}\PY{l+m}{3}\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|llllll}
  & left daughter & right daughter & split var & split point & status & prediction\\
\hline
	1 &  2           &  3           & 5            &  0.337891381 &  1           & 0           \\
	2 &  4           &  5           & 4            & -0.003885957 &  1           & 0           \\
	3 &  6           &  7           & 5            &  9.654494271 &  1           & 0           \\
	4 &  0           &  0           & 0            &  0.000000000 & -1           & 1           \\
	5 &  0           &  0           & 0            &  0.000000000 & -1           & 2           \\
	6 &  8           &  9           & 3            & 26.834578311 &  1           & 0           \\
	7 &  0           &  0           & 0            &  0.000000000 & -1           & 2           \\
	8 & 10           & 11           & 4            &  0.471948375 &  1           & 0           \\
	9 &  0           &  0           & 0            &  0.000000000 & -1           & 2           \\
	10 & 12           & 13           & 3            &  0.129103965 &  1           & 0           \\
	11 &  0           &  0           & 0            &  0.000000000 & -1           & 2           \\
	12 & 14           & 15           & 4            &  0.011196943 &  1           & 0           \\
	13 & 16           & 17           & 1            & 32.886056170 &  1           & 0           \\
	14 &  0           &  0           & 0            &  0.000000000 & -1           & 1           \\
	15 &  0           &  0           & 0            &  0.000000000 & -1           & 2           \\
	16 & 18           & 19           & 4            & -0.013528032 &  1           & 0           \\
	17 &  0           &  0           & 0            &  0.000000000 & -1           & 2           \\
	18 & 20           & 21           & 5            &  0.534575428 &  1           & 0           \\
	19 & 22           & 23           & 4            & -0.013412714 &  1           & 0           \\
	20 &  0           &  0           & 0            &  0.000000000 & -1           & 2           \\
	21 & 24           & 25           & 1            &  7.003424847 &  1           & 0           \\
	22 &  0           &  0           & 0            &  0.000000000 & -1           & 2           \\
	23 & 26           & 27           & 3            &  1.154956261 &  1           & 0           \\
	24 &  0           &  0           & 0            &  0.000000000 & -1           & 1           \\
	25 & 28           & 29           & 3            &  1.555515027 &  1           & 0           \\
	26 & 30           & 31           & 5            &  2.470942669 &  1           & 0           \\
	27 & 32           & 33           & 3            &  1.159900667 &  1           & 0           \\
	28 &  0           &  0           & 0            &  0.000000000 & -1           & 1           \\
	29 &  0           &  0           & 0            &  0.000000000 & -1           & 2           \\
	30 & 34           & 35           & 4            & -0.004324249 &  1           & 0           \\
	31 &  0           &  0           & 0            &  0.000000000 & -1           & 2           \\
	32 &  0           &  0           & 0            &  0.000000000 & -1           & 2           \\
	33 & 36           & 37           & 1            &  0.964475340 &  1           & 0           \\
	34 & 38           & 39           & 4            & -0.005291429 &  1           & 0           \\
	35 &  0           &  0           & 0            &  0.000000000 & -1           & 1           \\
	36 & 40           & 41           & 1            &  0.905780112 &  1           & 0           \\
	37 &  0           &  0           & 0            &  0.000000000 & -1           & 1           \\
	38 &  0           &  0           & 0            &  0.000000000 & -1           & 1           \\
	39 &  0           &  0           & 0            &  0.000000000 & -1           & 2           \\
	40 & 42           & 43           & 4            &  0.138495443 &  1           & 0           \\
	41 &  0           &  0           & 0            &  0.000000000 & -1           & 2           \\
	42 &  0           &  0           & 0            &  0.000000000 & -1           & 1           \\
	43 & 44           & 45           & 3            &  1.548379099 &  1           & 0           \\
	44 &  0           &  0           & 0            &  0.000000000 & -1           & 1           \\
	45 &  0           &  0           & 0            &  0.000000000 & -1           & 2           \\
\end{tabular}


    
    \hypertarget{random-forest-with-up-sampling}{%
\subsubsection{Random Forest with up
sampling}\label{random-forest-with-up-sampling}}

To incorporate up-sampling (sample the minority class to make their
frequencies closer to the majority class.), random forest can use an
upsampling strategy

The below code chunk sets some of the control parameters

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} objControl \PY{o}{\PYZlt{}\PYZhy{}} trainControl\PY{p}{(}method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{boot\PYZsq{}}\PY{p}{,} number \PY{o}{=} \PY{l+m}{1}\PY{p}{,}
                                    returnResamp\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{final\PYZsq{}}\PY{p}{,}
                                    summaryFunction \PY{o}{=} twoClassSummary\PY{p}{,}
                                    savePredictions \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    classProbs \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    sampling\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{up\PYZdq{}}\PY{p}{,}allowParallel\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{)}
\end{Verbatim}


    Parallel processing using doMC needs the below setup: \textgreater{} *
num\_cores \textless{}- (detectCores()-1) * registerDoMC(num\_cores)

doMC may give added benefit but is OS dependent. May not work on
Windows.

The below code chunk uses doParallel library for parallel processing.
After setting the control paramters, the model is run

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} num\PYZus{}cores \PY{o}{\PYZlt{}\PYZhy{}} makeCluster\PY{p}{(}detectCores\PY{p}{(}\PY{p}{)}\PY{l+m}{\PYZhy{}5}\PY{p}{)}
         registerDoParallel\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         tic\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{RF Bagging with Up Sample\PYZdq{}}\PY{p}{)}
         
         \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         rf\PYZus{}up\PYZus{}model \PY{o}{\PYZlt{}\PYZhy{}} train\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{,}
                           method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{rf\PYZsq{}}\PY{p}{,}
                           trControl\PY{o}{=}objControl\PY{p}{,}
                           metric \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{ROC\PYZdq{}}\PY{p}{,}
                           prox\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{)}
         stopCluster\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         toc\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
RF Bagging with Up Sample: 9.747 sec elapsed

    \end{Verbatim}

    Confusion Matrix for upsampling on train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{}rf\PYZus{}up\PYZus{}model\PYZdl{}finalModel \PYZsh{}rf\PYZus{}up\PYZus{}model\PYZdl{}results}
         
         \PY{k+kp}{print}\PY{p}{(}rf\PYZus{}up\PYZus{}model\PY{p}{)}
         confusionMatrix.train\PY{p}{(}rf\PYZus{}up\PYZus{}model\PY{p}{)}
         plot\PY{p}{(}varImp\PY{p}{(}rf\PYZus{}up\PYZus{}model\PY{p}{)}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable importance from Up Sample RF\PYZdq{}}\PY{p}{,} col \PY{o}{=} \PY{l+m}{2}\PY{p}{,} lwd \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Random Forest 

868 samples
  5 predictor
  2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Bootstrapped (1 reps) 
Summary of sample sizes: 868 
Addtional sampling using up-sampling

Resampling results across tuning parameters:

  mtry  ROC        Sens       Spec
  2     0.8698770  0.9967213  0   
  3     0.8858607  0.9934426  0   
  5     0.7725410  0.9803279  0   

ROC was used to select the optimal model using the largest value.
The final value used for the model was mtry = 3.

    \end{Verbatim}

    
    \begin{verbatim}
Bootstrapped (1 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   No  Yes
       No  96.8  2.6
       Yes  0.6  0.0
                            
 Accuracy (average) : 0.9681

    \end{verbatim}

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Confusion Matrix for upsampling on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} caretPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}rf\PYZus{}up\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{raw\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}caretPredictedClass\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  357  10
       Yes   3   1
                                          
               Accuracy : 0.965           
                 95% CI : (0.9408, 0.9812)
    No Information Rate : 0.9704          
    P-Value [Acc > NIR] : 0.78410         
                                          
                  Kappa : 0.1194          
 Mcnemar's Test P-Value : 0.09609         
                                          
            Sensitivity : 0.99167         
            Specificity : 0.09091         
         Pos Pred Value : 0.97275         
         Neg Pred Value : 0.25000         
             Prevalence : 0.97035         
         Detection Rate : 0.96226         
   Detection Prevalence : 0.98922         
      Balanced Accuracy : 0.54129         
                                          
       'Positive' Class : No              
                                          
    \end{verbatim}

    
    ROC plot for upsample random forest on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} rf\PYZus{}up\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}rf\PYZus{}up\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         rf\PYZus{}up\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}rf\PYZus{}up\PYZus{}pred\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         rf\PYZus{}up\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}rf\PYZus{}up\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}rf\PYZus{}up\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for Up Sample Random Forest\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}rf\PYZus{}up\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.7763889


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_41_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Extracting all the rules from the trees built using random forest

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} rf\PYZus{}up\PYZus{}treelist \PY{o}{\PYZlt{}\PYZhy{}} RF2List\PY{p}{(}rf\PYZus{}up\PYZus{}model\PY{o}{\PYZdl{}}finalModel\PY{p}{)}
         rf\PYZus{}up\PYZus{}rules \PY{o}{\PYZlt{}\PYZhy{}} extractRules\PY{p}{(}rf\PYZus{}up\PYZus{}treelist\PY{p}{,}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{)}\PY{p}{]}\PY{p}{,} ntree \PY{o}{=} \PY{l+m}{10}\PY{p}{)}
         rf\PYZus{}up\PYZus{}rules\PYZus{}metric \PY{o}{\PYZlt{}\PYZhy{}} getRuleMetric\PY{p}{(}rf\PYZus{}up\PYZus{}rules\PY{p}{,}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{)}\PY{p}{]}\PY{p}{,}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{)}
         rf\PYZus{}up\PYZus{}rules\PYZus{}metric \PY{o}{\PYZlt{}\PYZhy{}} pruneRule\PY{p}{(}rf\PYZus{}up\PYZus{}rules\PYZus{}metric\PY{p}{,}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{)}\PY{p}{]}\PY{p}{,}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{)}
         rf\PYZus{}up\PYZus{}rules\PYZus{}metric \PY{o}{\PYZlt{}\PYZhy{}} selectRuleRRF\PY{p}{(}rf\PYZus{}up\PYZus{}rules\PYZus{}metric\PY{p}{,}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{)}\PY{p}{]}\PY{p}{,}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}readable rules}
         \PY{k+kp}{print}\PY{p}{(}presentRules\PY{p}{(}rf\PYZus{}up\PYZus{}rules\PYZus{}metric\PY{p}{,} \PY{k+kp}{colnames}\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}rf.up.learner \PYZlt{}\PYZhy{} buildLearner(rf.up.rules.metric,model\PYZus{}df[,c(1:6)],model\PYZus{}df[,7])}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
229 rules (length<=6) were extracted from the first 10 trees.
      len freq    err    
 [1,] "2" "0.007" "0.167"
 [2,] "5" "0.007" "0.167"
 [3,] "2" "0.002" "0"    
 [4,] "2" "0.002" "0"    
 [5,] "1" "0.794" "0.033"
 [6,] "3" "0.002" "0"    
 [7,] "2" "0.001" "0"    
 [8,] "2" "0.001" "0"    
 [9,] "4" "0.003" "0.333"
[10,] "1" "0.679" "0.017"
[11,] "1" "0.594" "0.01" 
[12,] "1" "0.143" "0.048"
[13,] "1" "0.893" "0.025"
      condition                                                                                          
 [1,] "ACCR>-0.0137397365 \& LEVI<=0.405980794"                                                           
 [2,] "DEPI<=1.32222965 \& DEPI>0.9754958525 \& SGAI<=0.672654396 \& ACCR>-0.0137397365 \& LEVI>1.0273631615"
 [3,] "ACCR<=-0.545217622 \& ACCR>-0.637260803"                                                           
 [4,] "AQI>6.559077466 \& ACCR<=-0.1654674745"                                                            
 [5,] "DEPI<=1.09348791"                                                                                 
 [6,] "SGAI<=0.661013693 \& SGAI>0.6425966655 \& LEVI>1.070554392"                                         
 [7,] "ACCR>-0.036607684 \& ACCR<=-0.0363856715"                                                          
 [8,] "ACCR>0.3937475925 \& LEVI<=0.537491711"                                                            
 [9,] "AQI>1.183785029 \& DEPI<=1.085872075 \& SGAI>2.399295019 \& ACCR>-0.013528032"                       
[10,] "ACCR<=0.001618745"                                                                                
[11,] "ACCR<=-0.013528032"                                                                               
[12,] "LEVI>1.1831149185"                                                                                
[13,] "SGAI<=1.3562452445"                                                                               
      pred  impRRF              
 [1,] "Yes" "1"                 
 [2,] "Yes" "0.961093022902289" 
 [3,] "Yes" "0.474948322651951" 
 [4,] "Yes" "0.468119662217227" 
 [5,] "No"  "0.307865365223766" 
 [6,] "Yes" "0.289905653533515" 
 [7,] "Yes" "0.227946850114761" 
 [8,] "Yes" "0.219275994918584" 
 [9,] "Yes" "0.116948887047343" 
[10,] "No"  "0.0973335668127188"
[11,] "No"  "0.0340600883621492"
[12,] "No"  "0.0182289479010012"
[13,] "No"  "0.018123514100372" 

    \end{Verbatim}

    \hypertarget{random-forest-with-down-sampling---first-approach}{%
\subsubsection{Random Forest with down sampling - First
Approach}\label{random-forest-with-down-sampling---first-approach}}

To incorporate down-sampling (sample the majority class to make their
frequencies closer to the minority class.), random forest can use an
downsampling strategy

The below code chunk sets some of the control parameters

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} objControl \PY{o}{\PYZlt{}\PYZhy{}} trainControl\PY{p}{(}method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{boot\PYZsq{}}\PY{p}{,} number \PY{o}{=} \PY{l+m}{1}\PY{p}{,}
                                    returnResamp\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{final\PYZsq{}}\PY{p}{,}
                                    summaryFunction \PY{o}{=} twoClassSummary\PY{p}{,}
                                    savePredictions \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    classProbs \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    sampling\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{down\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    After setting the control parameters, the model is run

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} num\PYZus{}cores \PY{o}{\PYZlt{}\PYZhy{}} makeCluster\PY{p}{(}detectCores\PY{p}{(}\PY{p}{)}\PY{l+m}{\PYZhy{}5}\PY{p}{)}
         registerDoParallel\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         tic\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{RF Bagging with Down Sample\PYZdq{}}\PY{p}{)}
         
         \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         rf\PYZus{}down1\PYZus{}model \PY{o}{\PYZlt{}\PYZhy{}} train\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{,}
                           method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{rf\PYZsq{}}\PY{p}{,}
                           trControl\PY{o}{=}objControl\PY{p}{,}
                           metric \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{ROC\PYZdq{}}\PY{p}{)}
         stopCluster\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         toc\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
RF Bagging with Down Sample: 3.122 sec elapsed

    \end{Verbatim}

    Confusion Matrix for down sampling RF on train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{c+c1}{\PYZsh{}rf\PYZus{}down1\PYZus{}model\PYZdl{}finalModel \PYZsh{}rf\PYZus{}down1\PYZus{}model\PYZdl{}results}
         \PY{k+kp}{print}\PY{p}{(}rf\PYZus{}down1\PYZus{}model\PY{p}{)}
         confusionMatrix.train\PY{p}{(}rf\PYZus{}down1\PYZus{}model\PY{p}{)}
         plot\PY{p}{(}varImp\PY{p}{(}rf\PYZus{}down1\PYZus{}model\PY{p}{)}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable importance from down sample Random Forest\PYZdq{}}\PY{p}{,} col \PY{o}{=} \PY{l+m}{2}\PY{p}{,} lwd \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Random Forest 

868 samples
  5 predictor
  2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Bootstrapped (1 reps) 
Summary of sample sizes: 868 
Addtional sampling using down-sampling

Resampling results across tuning parameters:

  mtry  ROC        Sens       Spec 
  2     0.7186475  0.8327869  0.375
  3     0.6973361  0.7967213  0.125
  5     0.6540984  0.7639344  0.250

ROC was used to select the optimal model using the largest value.
The final value used for the model was mtry = 2.

    \end{Verbatim}

    
    \begin{verbatim}
Bootstrapped (1 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   No  Yes
       No  81.2  1.6
       Yes 16.3  1.0
                            
 Accuracy (average) : 0.8211

    \end{verbatim}

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_49_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Confusion Matrix for down sampling RF on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} caretPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}rf\PYZus{}down1\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{raw\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}caretPredictedClass\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  251   3
       Yes 109   8
                                          
               Accuracy : 0.6981          
                 95% CI : (0.6486, 0.7444)
    No Information Rate : 0.9704          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.0749          
 Mcnemar's Test P-Value : <2e-16          
                                          
            Sensitivity : 0.69722         
            Specificity : 0.72727         
         Pos Pred Value : 0.98819         
         Neg Pred Value : 0.06838         
             Prevalence : 0.97035         
         Detection Rate : 0.67655         
   Detection Prevalence : 0.68464         
      Balanced Accuracy : 0.71225         
                                          
       'Positive' Class : No              
                                          
    \end{verbatim}

    
    ROC plot for down sample random forest on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} rf\PYZus{}down1\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}rf\PYZus{}down1\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         rf\PYZus{}down1\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}rf\PYZus{}down1\PYZus{}pred\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         rf\PYZus{}down1\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}rf\PYZus{}down1\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}rf\PYZus{}down1\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for Down Sample Random Forest\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}rf\PYZus{}down1\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.7656566


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_53_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{random-forest-with-down-sampling---second-approach}{%
\subsubsection{Random Forest with down sampling - Second
Approach}\label{random-forest-with-down-sampling---second-approach}}

To incorporate down-sampling (sample the majority class to make their
frequencies closer to the rarest class.), random forest can take a
random sample of size c*nmin, where c is the number of classes and nmin
is the number of samples in the minority class.

\textbf{\emph{THIS IMPLEMENTATION IS WITHOUT CARET PACKAGE}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} nmin \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{sum}\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{o}{\PYZdl{}}Manipulater \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{Yes\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{}total minority cases}
         \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         tic\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{RF Bagging with Down\PYZdq{}}\PY{p}{)}
         rf\PYZus{}down2\PYZus{}model \PY{o}{\PYZlt{}\PYZhy{}} randomForest\PY{p}{(}Manipulater \PY{o}{\PYZti{}} \PY{l+m}{.}\PY{p}{,}
                                  data\PY{o}{=}model\PYZus{}train\PYZus{}df\PY{p}{,} importance\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{,} mtry \PY{o}{=} \PY{l+m}{2}\PY{p}{,}
                                  \PY{c+c1}{\PYZsh{}if strata is not defined RF does bootstrap sample}
                                  strata \PY{o}{=} model\PYZus{}train\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{,}
                                  \PY{c+c1}{\PYZsh{}selecting nmin cases from positive and negative class}
                                  sampsize \PY{o}{=} \PY{k+kp}{rep}\PY{p}{(}nmin\PY{p}{,}\PY{l+m}{2}\PY{p}{)}\PY{p}{,}
                                  \PY{c+c1}{\PYZsh{}cutoff: ‘winning’ class for an observation is the one}
                                  \PY{c+c1}{\PYZsh{}with the maximum ratio of proportion of votes to cutoff.}
                                  cutoff \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{/}\PY{l+m}{2}\PY{p}{,} \PY{l+m}{1}\PY{o}{/}\PY{l+m}{2}\PY{p}{)}\PY{p}{,}ntree\PY{o}{=}\PY{l+m}{1024}\PY{p}{,}  nodesize \PY{o}{=} \PY{l+m}{10}\PY{p}{,}
                                  keep.forest \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{)}\PY{c+c1}{\PYZsh{}, xtest = model\PYZus{}test\PYZus{}df[,\PYZhy{}12])}
         toc\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
RF Bagging with Down: 0.162 sec elapsed

    \end{Verbatim}

    Variable importance and Confusion matrix on downsample random forest on
train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{c+c1}{\PYZsh{}To plot the error rate.}
         \PY{c+c1}{\PYZsh{}plot(rf\PYZus{}down1\PYZus{}model, main = \PYZdq{}Error rate vs. number of trees (RF with downsample\PYZdq{}, type = \PYZdq{}l\PYZdq{}, lwd = 3)}
         
         \PY{c+c1}{\PYZsh{}To know the legends, type rf\PYZus{}down1\PYZus{}model to get the confusion matrix and \PYZsh{}see the error}
         
         \PY{k+kp}{print}\PY{p}{(}rf\PYZus{}down2\PYZus{}model\PY{p}{)}
         
         varImpPlot\PY{p}{(}rf\PYZus{}down2\PYZus{}model\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable Importance Plot with Down Sample\PYZdq{}}\PY{p}{,} pch \PY{o}{=} \PY{l+m}{16}\PY{p}{,} col \PY{o}{=} \PY{l+s}{\PYZsq{}}\PY{l+s}{darkred\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

Call:
 randomForest(formula = Manipulater \textasciitilde{} ., data = model\_train\_df,      importance = TRUE, mtry = 2, strata = model\_train\_df\$Manipulater,      sampsize = rep(nmin, 2), cutoff = c(1/2, 1/2), ntree = 1024,      nodesize = 10, keep.forest = TRUE) 
               Type of random forest: classification
                     Number of trees: 1024
No. of variables tried at each split: 2

        OOB estimate of  error rate: 21.54\%
Confusion matrix:
     No Yes class.error
No  663 177   0.2107143
Yes  10  18   0.3571429

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_57_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Variable importance and Confusion matrix on downsample random forest on
test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} testPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}rf\PYZus{}down2\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{response\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}testPredictedClass\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  275   1
       Yes  85  10
                                          
               Accuracy : 0.7682          
                 95% CI : (0.7219, 0.8102)
    No Information Rate : 0.9704          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.1431          
 Mcnemar's Test P-Value : <2e-16          
                                          
            Sensitivity : 0.7639          
            Specificity : 0.9091          
         Pos Pred Value : 0.9964          
         Neg Pred Value : 0.1053          
             Prevalence : 0.9704          
         Detection Rate : 0.7412          
   Detection Prevalence : 0.7439          
      Balanced Accuracy : 0.8365          
                                          
       'Positive' Class : No              
                                          
    \end{verbatim}

    
    ROC plot for Random Forest with downsampling on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} rf\PYZus{}down2\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}rf\PYZus{}down2\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         rf\PYZus{}down2\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}rf\PYZus{}down2\PYZus{}pred\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         rf\PYZus{}down2\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}rf\PYZus{}down2\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}rf\PYZus{}down2\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for RF with Down Sampling\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}rf\PYZus{}down2\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.9109848


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_61_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{random-forest-with-smote}{%
\subsubsection{Random Forest with
SMOTE}\label{random-forest-with-smote}}

Synthetic minority oversampling technique (SMOTE) blends under-sampling
of the majority class with a special form of over-sampling the minority
class. SMOTE oversamples the rare event by using bootstrapping and
k-nearest neighbor to synthetically create additional observations of
that event.

The below code chunk sets some of the control parameters

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} objControl \PY{o}{\PYZlt{}\PYZhy{}} trainControl\PY{p}{(}method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{boot\PYZsq{}}\PY{p}{,} number \PY{o}{=} \PY{l+m}{1}\PY{p}{,}
                                    returnResamp\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{final\PYZsq{}}\PY{p}{,}
                                    summaryFunction \PY{o}{=} twoClassSummary\PY{p}{,}
                                    savePredictions \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    classProbs \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    sampling\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{smote\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    After setting the control parameters, the model is run

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} num\PYZus{}cores \PY{o}{\PYZlt{}\PYZhy{}} makeCluster\PY{p}{(}detectCores\PY{p}{(}\PY{p}{)}\PY{l+m}{\PYZhy{}5}\PY{p}{)}
         registerDoParallel\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         tic\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{RF Bagging with SMOTE Sample\PYZdq{}}\PY{p}{)}
         
         \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         rf\PYZus{}smote\PYZus{}model \PY{o}{\PYZlt{}\PYZhy{}} train\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{,}
                           method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{rf\PYZsq{}}\PY{p}{,}
                           trControl\PY{o}{=}objControl\PY{p}{,}
                           metric \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{ROC\PYZdq{}}\PY{p}{,}
                           prox\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{,}allowParallel\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{)}
         stopCluster\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         toc\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
RF Bagging with SMOTE Sample: 3.496 sec elapsed

    \end{Verbatim}

    Confusion Matrix for RF on train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{c+c1}{\PYZsh{}rf\PYZus{}smote\PYZus{}model\PYZdl{}finalModel \PYZsh{}rf\PYZus{}smote\PYZus{}model\PYZdl{}results}
         \PY{k+kp}{print}\PY{p}{(}rf\PYZus{}smote\PYZus{}model\PY{p}{)}
         confusionMatrix.train\PY{p}{(}rf\PYZus{}smote\PYZus{}model\PY{p}{)}
         plot\PY{p}{(}varImp\PY{p}{(}rf\PYZus{}smote\PYZus{}model\PY{p}{)}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable importance from SMOTE Random Forest\PYZdq{}}\PY{p}{,} col \PY{o}{=} \PY{l+m}{2}\PY{p}{,} lwd \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Random Forest 

868 samples
  5 predictor
  2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Bootstrapped (1 reps) 
Summary of sample sizes: 868 
Addtional sampling using SMOTE

Resampling results across tuning parameters:

  mtry  ROC        Sens       Spec 
  2     0.7354508  0.8819672  0.125
  3     0.6907787  0.8852459  0.125
  5     0.7579918  0.8852459  0.250

ROC was used to select the optimal model using the largest value.
The final value used for the model was mtry = 5.

    \end{Verbatim}

    
    \begin{verbatim}
Bootstrapped (1 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   No  Yes
       No  86.3  1.9
       Yes 11.2  0.6
                           
 Accuracy (average) : 0.869

    \end{verbatim}

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_67_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Confusion Matrix for RF on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} caretPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}rf\PYZus{}smote\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{raw\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}caretPredictedClass\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  292   5
       Yes  68   6
                                          
               Accuracy : 0.8032          
                 95% CI : (0.7591, 0.8425)
    No Information Rate : 0.9704          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.0944          
 Mcnemar's Test P-Value : 3.971e-13       
                                          
            Sensitivity : 0.81111         
            Specificity : 0.54545         
         Pos Pred Value : 0.98316         
         Neg Pred Value : 0.08108         
             Prevalence : 0.97035         
         Detection Rate : 0.78706         
   Detection Prevalence : 0.80054         
      Balanced Accuracy : 0.67828         
                                          
       'Positive' Class : No              
                                          
    \end{verbatim}

    
    ROC plot for random forest on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} rf\PYZus{}smote\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}rf\PYZus{}smote\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         rf\PYZus{}smote\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}rf\PYZus{}smote\PYZus{}pred\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         rf\PYZus{}smote\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}rf\PYZus{}smote\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}rf\PYZus{}smote\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for Random Forest with SMOTE\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}rf\PYZus{}smote\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.8258838


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_71_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{boosting}{%
\subsection{Boosting}\label{boosting}}

Boosting is an ensemble technique which tries to create a strong
classifier from several weak classifier. The model buidling through
boosting is sequential. 1. The first model is build based on the random
sample on N observations picked from original dataset (with
replacement). Equal weight is assigned to each observation. These
weights decide the probability of observations which will be picked up
in the training set. 2. In the second step, all the original dataset is
passed through the model. For regressor model, the observations whose
predicted value differs the most from the actual value is defined to be
most in error. 3. The sampling probabilities of the observations which
are most in error, is adjusted such that their chance of getting picked
up for the second model is higher. 4. As the model buidling progresses,
in each of the sequence of models, the pattern which are more difficult
are picked up. Different models are better in different part of the
observation space. 5. Rgeressors are combined using weighted median.
Models which are more confident about their predictions are weighted
more heavily.

\hypertarget{boosting-algorithms-in-r}{%
\subsubsection{Boosting algorithms in
R}\label{boosting-algorithms-in-r}}

Adaboost is one of the ways to boost the performance of decision trees
on binary classification problems. The decision trees with just one
level will mostly be a weak learner. These weak learners will achieve an
accuracy just above random chance on a classification problem.

Adaboost is also referred to as discrete AdaBoost as it is used for
classification rather than regression. The algorithms in boosting are:

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Adaboost classification trees: \textbf{\emph{adaboost()}} Required
  Package is \textbf{fastAdaboost}
\item
  Adaboost.M1: \textbf{\emph{AdaBoost.M1()}} Required Package is
  \textbf{adabag, plyr}
\item
  Boosted Classification Trees: \textbf{\emph{ada()}} Required Package
  is \textbf{adabag, plyr}
\item
  Boosted Generalized Additive Model: \textbf{\emph{gamBoost()}}
  Required Package is \textbf{mboost, plyr}
\item
  Boosted Generalized Linear Model: \textbf{\emph{glmboost()}} Required
  Package is \textbf{mboost, plyr}
\item
  Boosted Linear Model: \textbf{\emph{Bstlm()}} Required Package is
  \textbf{bst, plyr}
\item
  Boosted Logistic Regression: \textbf{\emph{LogitBoost()}} Required
  Package is \textbf{caTools}
\item
  Boosted Smoothing Spline: \textbf{\emph{bstSm()}} Required Package is
  \textbf{bst, plyr}
\item
  Boosted Tree: \textbf{\emph{blackboost()}} Required Package is
  \textbf{party, mboost, plyr}
\item
  Boosted Tree: \textbf{\emph{bstTree()}} Required Package is
  \textbf{bst, plyr}
\item
  C5.0: \textbf{\emph{C5.0()}} Required Package is \textbf{C50, plyr}
\item
  Cost Sensitive C5.0: \textbf{\emph{C5.0Cost()}} Required Package is
  \textbf{C50, plyr}
\item
  Cubist: \textbf{\emph{glmboost()}} Required Package is \textbf{cubist}
\item
  DeepBoost: \textbf{\emph{deepboost()}} Required Package is
  \textbf{deepboost}
\item
  eXtreme Gradient Boosting: \textbf{\emph{xgbLinear()}} Required
  Package is \textbf{xgboost}
\item
  eXtreme Gradient Boosting: \textbf{\emph{xgbTree()}} Required Package
  is \textbf{xgboost, plyr}
\item
  Stochastic Gradient Boosting: \textbf{\emph{gbm()}} Required Package
  is \textbf{gbm, plyr}
\end{enumerate}
\end{quote}

\hypertarget{boosting-with-adaboost-normal}{%
\subsubsection{Boosting with adaboost (normal
)}\label{boosting-with-adaboost-normal}}

The below code chunk sets some of the control parameters for adaboost

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} objControl \PY{o}{\PYZlt{}\PYZhy{}} trainControl\PY{p}{(}method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{boot\PYZsq{}}\PY{p}{,} number \PY{o}{=} \PY{l+m}{1}\PY{p}{,}
                                    returnResamp\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{all\PYZsq{}}\PY{p}{,}
                                    summaryFunction \PY{o}{=} twoClassSummary\PY{p}{,}
                                    savePredictions \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    classProbs \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{)}\PY{c+c1}{\PYZsh{}, p = 0.70) \PYZsh{}in case method = \PYZsh{}\PYZdq{}LGOCV\PYZdq{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} search\PYZus{}grid \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{expand.grid}\PY{p}{(}mfinal \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{20}\PY{o}{:}\PY{l+m}{100}\PY{p}{)}\PY{p}{,} maxdepth \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{2}\PY{o}{:}\PY{l+m}{4}\PY{p}{)}\PY{p}{,}
                             coeflearn \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Breiman\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{Freund\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{Zhu\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    Look for the documentation of library \textbf{adabag}. The
\textbf{boosting()} function of adabag implments `AdaBoost.M1'. The
\emph{boos} paramter of boosting function is set to TRUE by default.
This meand a bootstrap sample of the training set is drawn using the
weights for each observation on that iteration. If FALSE, every
observation is used with its weights.

After setting the control paramters, the model is run

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} num\PYZus{}cores \PY{o}{\PYZlt{}\PYZhy{}} makeCluster\PY{p}{(}detectCores\PY{p}{(}\PY{p}{)}\PY{l+m}{\PYZhy{}5}\PY{p}{)}
         registerDoParallel\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         tic\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Adaptive Boosting with Bootstrap Sample\PYZdq{}}\PY{p}{)}
         
         \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         ada\PYZus{}model \PY{o}{\PYZlt{}\PYZhy{}} train\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{,}
                           method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{AdaBoost.M1\PYZsq{}}\PY{p}{,}
                           trControl\PY{o}{=}objControl\PY{p}{,}
                           tuneGrid \PY{o}{=} search\PYZus{}grid\PY{p}{,}
                           metric \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{ROC\PYZdq{}}\PY{p}{)}
         stopCluster\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         toc\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Adaptive Boosting with Bootstrap Sample: 121.42 sec elapsed

    \end{Verbatim}

    Confusion Matrix for adaboost on train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{c+c1}{\PYZsh{}ada\PYZus{}model\PYZdl{}finalModel \PYZsh{}ada\PYZus{}model\PYZdl{}results}
         \PY{k+kp}{print}\PY{p}{(}ada\PYZus{}model\PY{p}{)}
         confusionMatrix.train\PY{p}{(}ada\PYZus{}model\PY{p}{)}
         plot\PY{p}{(}varImp\PY{p}{(}ada\PYZus{}model\PY{p}{)}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable importance from Adaboost with Bootstrap\PYZdq{}}\PY{p}{,} col \PY{o}{=} \PY{l+m}{2}\PY{p}{,} lwd \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
AdaBoost.M1 

868 samples
  5 predictor
  2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Bootstrapped (1 reps) 
Summary of sample sizes: 868 
Resampling results across tuning parameters:

  coeflearn  maxdepth  mfinal  ROC        Sens       Spec 
  Breiman    2          20     0.6795082  1.0000000  0.000
  Breiman    2          21     0.6827869  1.0000000  0.000
  Breiman    2          22     0.6774590  1.0000000  0.000
  Breiman    2          23     0.6676230  1.0000000  0.000
  Breiman    2          24     0.7036885  1.0000000  0.000
  Breiman    2          25     0.6889344  1.0000000  0.000
  Breiman    2          26     0.6885246  1.0000000  0.000
  Breiman    2          27     0.6905738  1.0000000  0.000
  Breiman    2          28     0.6913934  0.9967213  0.000
  Breiman    2          29     0.6942623  1.0000000  0.000
  Breiman    2          30     0.6729508  0.9967213  0.000
  Breiman    2          31     0.6729508  0.9967213  0.000
  Breiman    2          32     0.6729508  1.0000000  0.000
  Breiman    2          33     0.6643443  1.0000000  0.000
  Breiman    2          34     0.6631148  1.0000000  0.000
  Breiman    2          35     0.6704918  1.0000000  0.000
  Breiman    2          36     0.6844262  1.0000000  0.000
  Breiman    2          37     0.6766393  1.0000000  0.000
  Breiman    2          38     0.6918033  0.9967213  0.000
  Breiman    2          39     0.6918033  1.0000000  0.000
  Breiman    2          40     0.6926230  0.9967213  0.000
  Breiman    2          41     0.6885246  0.9967213  0.000
  Breiman    2          42     0.6881148  0.9934426  0.000
  Breiman    2          43     0.6913934  0.9934426  0.000
  Breiman    2          44     0.6959016  0.9934426  0.000
  Breiman    2          45     0.6905738  0.9934426  0.000
  Breiman    2          46     0.6967213  0.9934426  0.000
  Breiman    2          47     0.6967213  0.9967213  0.000
  Breiman    2          48     0.6918033  0.9967213  0.000
  Breiman    2          49     0.6774590  0.9901639  0.000
  Breiman    2          50     0.6950820  0.9967213  0.000
  Breiman    2          51     0.6926230  0.9967213  0.000
  Breiman    2          52     0.6909836  1.0000000  0.000
  Breiman    2          53     0.6725410  1.0000000  0.000
  Breiman    2          54     0.6725410  1.0000000  0.000
  Breiman    2          55     0.6725410  1.0000000  0.000
  Breiman    2          56     0.6778689  1.0000000  0.000
  Breiman    2          57     0.6713115  1.0000000  0.000
  Breiman    2          58     0.6717213  1.0000000  0.000
  Breiman    2          59     0.6639344  1.0000000  0.000
  Breiman    2          60     0.6696721  1.0000000  0.000
  Breiman    2          61     0.6696721  1.0000000  0.000
  Breiman    2          62     0.6733607  1.0000000  0.000
  Breiman    2          63     0.6524590  0.9967213  0.000
  Breiman    2          64     0.6553279  0.9967213  0.000
  Breiman    2          65     0.6540984  0.9967213  0.000
  Breiman    2          66     0.6536885  0.9967213  0.000
  Breiman    2          67     0.6532787  0.9967213  0.000
  Breiman    2          68     0.6397541  1.0000000  0.000
  Breiman    2          69     0.6512295  1.0000000  0.000
  Breiman    2          70     0.6573770  1.0000000  0.000
  Breiman    2          71     0.6577869  1.0000000  0.000
  Breiman    2          72     0.6553279  1.0000000  0.000
  Breiman    2          73     0.6553279  1.0000000  0.000
  Breiman    2          74     0.6561475  1.0000000  0.000
  Breiman    2          75     0.6561475  1.0000000  0.000
  Breiman    2          76     0.6508197  1.0000000  0.000
  Breiman    2          77     0.6614754  0.9967213  0.000
  Breiman    2          78     0.6643443  1.0000000  0.000
  Breiman    2          79     0.6655738  0.9967213  0.000
  Breiman    2          80     0.6713115  1.0000000  0.000
  Breiman    2          81     0.6655738  1.0000000  0.000
  Breiman    2          82     0.6655738  1.0000000  0.000
  Breiman    2          83     0.6659836  1.0000000  0.000
  Breiman    2          84     0.6659836  1.0000000  0.000
  Breiman    2          85     0.6655738  1.0000000  0.000
  Breiman    2          86     0.6651639  1.0000000  0.000
  Breiman    2          87     0.6635246  0.9967213  0.000
  Breiman    2          88     0.6745902  1.0000000  0.000
  Breiman    2          89     0.6565574  1.0000000  0.000
  Breiman    2          90     0.6508197  0.9967213  0.000
  Breiman    2          91     0.6508197  1.0000000  0.000
  Breiman    2          92     0.6586066  0.9967213  0.000
  Breiman    2          93     0.6586066  1.0000000  0.000
  Breiman    2          94     0.6356557  1.0000000  0.000
  Breiman    2          95     0.6471311  1.0000000  0.000
  Breiman    2          96     0.6471311  1.0000000  0.000
  Breiman    2          97     0.6446721  1.0000000  0.000
  Breiman    2          98     0.6422131  1.0000000  0.000
  Breiman    2          99     0.6635246  1.0000000  0.000
  Breiman    2         100     0.6586066  1.0000000  0.000
  Breiman    3          20     0.7979508  0.9901639  0.000
  Breiman    3          21     0.7901639  0.9901639  0.000
  Breiman    3          22     0.7934426  0.9868852  0.000
  Breiman    3          23     0.7891393  0.9901639  0.000
  Breiman    3          24     0.7825820  0.9901639  0.000
  Breiman    3          25     0.7793033  0.9901639  0.000
  Breiman    3          26     0.7739754  0.9868852  0.000
  Breiman    3          27     0.7735656  0.9901639  0.000
  Breiman    3          28     0.7727459  0.9868852  0.000
  Breiman    3          29     0.7850410  0.9868852  0.000
  Breiman    3          30     0.7911885  0.9901639  0.000
  Breiman    3          31     0.7977459  0.9901639  0.000
  Breiman    3          32     0.7821721  0.9901639  0.000
  Breiman    3          33     0.7809426  0.9868852  0.000
  Breiman    3          34     0.7846311  0.9868852  0.000
  Breiman    3          35     0.7993852  0.9901639  0.000
  Breiman    3          36     0.8053279  0.9901639  0.000
  Breiman    3          37     0.8016393  0.9934426  0.000
  Breiman    3          38     0.8010246  0.9967213  0.000
  Breiman    3          39     0.7774590  0.9934426  0.000
  Breiman    3          40     0.8032787  0.9934426  0.000
  Breiman    3          41     0.8061475  0.9934426  0.000
  Breiman    3          42     0.8102459  0.9934426  0.000
  Breiman    3          43     0.8094262  0.9967213  0.000
  Breiman    3          44     0.8086066  0.9967213  0.000
  Breiman    3          45     0.7934426  0.9967213  0.000
  Breiman    3          46     0.8016393  0.9934426  0.000
  Breiman    3          47     0.8000000  0.9967213  0.000
  Breiman    3          48     0.7811475  0.9934426  0.000
  Breiman    3          49     0.7938525  0.9934426  0.000
  Breiman    3          50     0.8008197  0.9934426  0.000
  Breiman    3          51     0.7938525  0.9934426  0.000
  Breiman    3          52     0.7721311  0.9934426  0.000
  Breiman    3          53     0.7729508  0.9934426  0.000
  Breiman    3          54     0.7680328  0.9901639  0.000
  Breiman    3          55     0.7598361  0.9868852  0.000
  Breiman    3          56     0.7647541  0.9901639  0.000
  Breiman    3          57     0.7618852  0.9868852  0.000
  Breiman    3          58     0.7670082  0.9868852  0.000
  Breiman    3          59     0.7596311  0.9934426  0.000
  Breiman    3          60     0.7514344  0.9901639  0.000
  Breiman    3          61     0.7411885  0.9901639  0.000
  Breiman    3          62     0.7485656  0.9868852  0.000
  Breiman    3          63     0.7452869  0.9901639  0.000
  Breiman    3          64     0.7452869  0.9901639  0.000
  Breiman    3          65     0.7428279  0.9901639  0.000
  Breiman    3          66     0.7588115  0.9901639  0.000
  Breiman    3          67     0.7588115  0.9934426  0.000
  Breiman    3          68     0.7588115  0.9934426  0.000
  Breiman    3          69     0.7563525  0.9901639  0.000
  Breiman    3          70     0.7563525  0.9934426  0.000
  Breiman    3          71     0.7547131  0.9934426  0.000
  Breiman    3          72     0.7678279  0.9934426  0.000
  Breiman    3          73     0.7715164  0.9934426  0.000
  Breiman    3          74     0.7731557  0.9934426  0.000
  Breiman    3          75     0.7637295  0.9901639  0.000
  Breiman    3          76     0.7625000  0.9934426  0.000
  Breiman    3          77     0.7620902  0.9934426  0.000
  Breiman    3          78     0.7764344  0.9901639  0.000
  Breiman    3          79     0.7764344  0.9934426  0.000
  Breiman    3          80     0.7756148  0.9934426  0.000
  Breiman    3          81     0.7711066  0.9934426  0.000
  Breiman    3          82     0.7723361  0.9901639  0.000
  Breiman    3          83     0.7731557  0.9934426  0.000
  Breiman    3          84     0.7706967  0.9901639  0.000
  Breiman    3          85     0.7674180  0.9934426  0.000
  Breiman    3          86     0.7637295  0.9934426  0.000
  Breiman    3          87     0.7588115  0.9934426  0.000
  Breiman    3          88     0.7604508  0.9934426  0.000
  Breiman    3          89     0.7625000  0.9934426  0.000
  Breiman    3          90     0.7633197  0.9934426  0.000
  Breiman    3          91     0.7588115  0.9934426  0.000
  Breiman    3          92     0.7584016  0.9901639  0.000
  Breiman    3          93     0.7584016  0.9934426  0.000
  Breiman    3          94     0.7584016  0.9934426  0.000
  Breiman    3          95     0.7764344  0.9967213  0.000
  Breiman    3          96     0.7784836  0.9934426  0.000
  Breiman    3          97     0.7743852  0.9967213  0.000
  Breiman    3          98     0.7809426  0.9967213  0.000
  Breiman    3          99     0.7661885  0.9967213  0.000
  Breiman    3         100     0.7661885  0.9967213  0.000
  Breiman    4          20     0.5399590  0.9934426  0.000
  Breiman    4          21     0.5311475  0.9934426  0.000
  Breiman    4          22     0.5227459  0.9934426  0.000
  Breiman    4          23     0.5329918  0.9934426  0.000
  Breiman    4          24     0.5284836  0.9934426  0.000
  Breiman    4          25     0.5252049  0.9934426  0.000
  Breiman    4          26     0.5495902  0.9934426  0.000
  Breiman    4          27     0.5551230  0.9934426  0.000
  Breiman    4          28     0.5698770  0.9934426  0.000
  Breiman    4          29     0.5915984  0.9934426  0.000
  Breiman    4          30     0.6030738  0.9934426  0.000
  Breiman    4          31     0.5731557  0.9934426  0.000
  Breiman    4          32     0.5639344  0.9934426  0.000
  Breiman    4          33     0.5602459  0.9934426  0.000
  Breiman    4          34     0.5770492  0.9901639  0.000
  Breiman    4          35     0.5741803  0.9934426  0.000
  Breiman    4          36     0.5655738  0.9967213  0.000
  Breiman    4          37     0.5573770  0.9934426  0.000
  Breiman    4          38     0.5581967  0.9934426  0.000
  Breiman    4          39     0.5553279  0.9934426  0.000
  Breiman    4          40     0.5508197  0.9934426  0.000
  Breiman    4          41     0.5336066  0.9934426  0.000
  Breiman    4          42     0.5704918  0.9901639  0.000
  Breiman    4          43     0.5622951  0.9901639  0.000
  Breiman    4          44     0.5811475  0.9934426  0.000
  Breiman    4          45     0.5790984  0.9934426  0.000
  Breiman    4          46     0.5729508  0.9967213  0.000
  Breiman    4          47     0.5717213  0.9967213  0.000
  Breiman    4          48     0.5942623  0.9967213  0.000
  Breiman    4          49     0.5926230  0.9967213  0.000
  Breiman    4          50     0.6250000  0.9934426  0.000
  Breiman    4          51     0.6444672  0.9967213  0.000
  Breiman    4          52     0.6358607  0.9934426  0.000
  Breiman    4          53     0.6440574  0.9934426  0.000
  Breiman    4          54     0.6399590  0.9934426  0.000
  Breiman    4          55     0.6366803  0.9934426  0.000
  Breiman    4          56     0.6247951  0.9901639  0.000
  Breiman    4          57     0.6297131  0.9901639  0.000
  Breiman    4          58     0.6383197  0.9901639  0.000
  Breiman    4          59     0.6436475  0.9901639  0.000
  Breiman    4          60     0.6362705  0.9901639  0.000
  Breiman    4          61     0.6706967  0.9901639  0.000
  Breiman    4          62     0.6815574  0.9934426  0.000
  Breiman    4          63     0.6721311  0.9901639  0.000
  Breiman    4          64     0.6795082  0.9901639  0.000
  Breiman    4          65     0.6877049  0.9901639  0.000
  Breiman    4          66     0.6807377  0.9901639  0.000
  Breiman    4          67     0.6778689  0.9901639  0.000
  Breiman    4          68     0.6762295  0.9901639  0.000
  Breiman    4          69     0.6868852  0.9901639  0.000
  Breiman    4          70     0.6848361  0.9901639  0.000
  Breiman    4          71     0.6983607  0.9901639  0.000
  Breiman    4          72     0.6979508  0.9901639  0.000
  Breiman    4          73     0.6901639  0.9901639  0.000
  Breiman    4          74     0.6897541  0.9901639  0.000
  Breiman    4          75     0.6905738  0.9901639  0.000
  Breiman    4          76     0.6868852  0.9901639  0.000
  Breiman    4          77     0.6864754  0.9901639  0.000
  Breiman    4          78     0.6807377  0.9901639  0.000
  Breiman    4          79     0.6905738  0.9901639  0.000
  Breiman    4          80     0.6942623  0.9901639  0.000
  Breiman    4          81     0.6983607  0.9901639  0.000
  Breiman    4          82     0.6901639  0.9901639  0.000
  Breiman    4          83     0.6897541  0.9901639  0.000
  Breiman    4          84     0.6881148  0.9901639  0.000
  Breiman    4          85     0.6827869  0.9901639  0.000
  Breiman    4          86     0.6696721  0.9901639  0.000
  Breiman    4          87     0.6758197  0.9901639  0.000
  Breiman    4          88     0.6631148  0.9901639  0.000
  Breiman    4          89     0.6627049  0.9901639  0.000
  Breiman    4          90     0.6618852  0.9901639  0.000
  Breiman    4          91     0.6454918  0.9901639  0.000
  Breiman    4          92     0.6512295  0.9934426  0.000
  Breiman    4          93     0.6483607  0.9934426  0.000
  Breiman    4          94     0.6467213  0.9934426  0.000
  Breiman    4          95     0.6475410  0.9934426  0.000
  Breiman    4          96     0.6487705  0.9934426  0.000
  Breiman    4          97     0.6491803  0.9934426  0.000
  Breiman    4          98     0.6450820  0.9934426  0.000
  Breiman    4          99     0.6340164  0.9934426  0.000
  Breiman    4         100     0.6397541  0.9934426  0.000
  Freund     2          20     0.7725410  0.9934426  0.000
  Freund     2          21     0.7709016  0.9967213  0.000
  Freund     2          22     0.7694672  0.9934426  0.000
  Freund     2          23     0.7694672  0.9967213  0.000
  Freund     2          24     0.7672131  0.9934426  0.000
  Freund     2          25     0.7647541  0.9934426  0.000
  Freund     2          26     0.7756148  0.9934426  0.000
  Freund     2          27     0.7557377  0.9868852  0.000
  Freund     2          28     0.7323770  0.9868852  0.000
  Freund     2          29     0.7422131  0.9901639  0.000
  Freund     2          30     0.7719262  0.9901639  0.000
  Freund     2          31     0.7706967  0.9934426  0.000
  Freund     2          32     0.7555328  0.9967213  0.000
  Freund     2          33     0.7772541  0.9934426  0.000
  Freund     2          34     0.7604508  0.9901639  0.000
  Freund     2          35     0.7649590  0.9967213  0.000
  Freund     2          36     0.7727459  0.9901639  0.000
  Freund     2          37     0.7694672  0.9967213  0.000
  Freund     2          38     0.7625000  0.9967213  0.000
  Freund     2          39     0.7596311  0.9967213  0.000
  Freund     2          40     0.7469262  0.9967213  0.000
  Freund     2          41     0.7334016  0.9934426  0.000
  Freund     2          42     0.7081967  0.9967213  0.000
  Freund     2          43     0.7196721  1.0000000  0.000
  Freund     2          44     0.7286885  0.9967213  0.000
  Freund     2          45     0.7286885  1.0000000  0.000
  Freund     2          46     0.7590164  0.9967213  0.000
  Freund     2          47     0.7639344  0.9967213  0.000
  Freund     2          48     0.7647541  0.9967213  0.000
  Freund     2          49     0.7434426  1.0000000  0.000
  Freund     2          50     0.7282787  1.0000000  0.000
  Freund     2          51     0.7282787  1.0000000  0.000
  Freund     2          52     0.7389344  1.0000000  0.000
  Freund     2          53     0.7393443  1.0000000  0.000
  Freund     2          54     0.7340164  1.0000000  0.000
  Freund     2          55     0.7303279  1.0000000  0.000
  Freund     2          56     0.7032787  1.0000000  0.000
  Freund     2          57     0.7028689  1.0000000  0.000
  Freund     2          58     0.7159836  1.0000000  0.000
  Freund     2          59     0.7147541  1.0000000  0.000
  Freund     2          60     0.7049180  1.0000000  0.000
  Freund     2          61     0.7045082  1.0000000  0.000
  Freund     2          62     0.7122951  1.0000000  0.000
  Freund     2          63     0.7122951  1.0000000  0.000
  Freund     2          64     0.7098361  1.0000000  0.000
  Freund     2          65     0.7053279  1.0000000  0.000
  Freund     2          66     0.6942623  0.9967213  0.000
  Freund     2          67     0.6967213  1.0000000  0.000
  Freund     2          68     0.7045082  0.9967213  0.000
  Freund     2          69     0.6979508  0.9967213  0.000
  Freund     2          70     0.6848361  0.9934426  0.000
  Freund     2          71     0.7094262  0.9967213  0.000
  Freund     2          72     0.7061475  0.9967213  0.000
  Freund     2          73     0.7049180  0.9967213  0.000
  Freund     2          74     0.6971311  0.9967213  0.000
  Freund     2          75     0.6864754  0.9967213  0.000
  Freund     2          76     0.6823770  0.9967213  0.000
  Freund     2          77     0.6856557  0.9967213  0.000
  Freund     2          78     0.6729508  0.9967213  0.000
  Freund     2          79     0.6782787  0.9967213  0.000
  Freund     2          80     0.6762295  0.9967213  0.000
  Freund     2          81     0.6758197  0.9967213  0.000
  Freund     2          82     0.6762295  0.9967213  0.000
  Freund     2          83     0.6762295  0.9967213  0.000
  Freund     2          84     0.6704918  0.9967213  0.000
  Freund     2          85     0.6831967  0.9967213  0.000
  Freund     2          86     0.6946721  0.9967213  0.000
  Freund     2          87     0.6946721  0.9967213  0.000
  Freund     2          88     0.6946721  0.9967213  0.000
  Freund     2          89     0.6942623  0.9967213  0.000
  Freund     2          90     0.7004098  0.9967213  0.000
  Freund     2          91     0.7049180  0.9967213  0.000
  Freund     2          92     0.7147541  1.0000000  0.000
  Freund     2          93     0.7049180  1.0000000  0.000
  Freund     2          94     0.6819672  1.0000000  0.000
  Freund     2          95     0.6754098  0.9967213  0.000
  Freund     2          96     0.6754098  0.9967213  0.000
  Freund     2          97     0.6733607  0.9967213  0.000
  Freund     2          98     0.6782787  0.9967213  0.000
  Freund     2          99     0.6663934  0.9967213  0.000
  Freund     2         100     0.6786885  0.9967213  0.000
  Freund     3          20     0.8172131  0.9934426  0.000
  Freund     3          21     0.8262295  0.9934426  0.000
  Freund     3          22     0.8069672  0.9901639  0.000
  Freund     3          23     0.7819672  0.9901639  0.000
  Freund     3          24     0.7770492  0.9901639  0.000
  Freund     3          25     0.7803279  0.9868852  0.000
  Freund     3          26     0.7561475  0.9901639  0.000
  Freund     3          27     0.7200820  0.9901639  0.000
  Freund     3          28     0.7200820  0.9901639  0.000
  Freund     3          29     0.6930328  0.9967213  0.000
  Freund     3          30     0.6852459  0.9934426  0.000
  Freund     3          31     0.7200820  0.9934426  0.000
  Freund     3          32     0.7004098  0.9868852  0.000
  Freund     3          33     0.7098361  0.9868852  0.000
  Freund     3          34     0.7086066  0.9868852  0.000
  Freund     3          35     0.7598361  0.9868852  0.000
  Freund     3          36     0.7573770  0.9868852  0.000
  Freund     3          37     0.8147541  0.9901639  0.000
  Freund     3          38     0.7741803  0.9934426  0.000
  Freund     3          39     0.7836066  0.9901639  0.000
  Freund     3          40     0.7729508  0.9901639  0.000
  Freund     3          41     0.7840164  0.9901639  0.000
  Freund     3          42     0.7827869  0.9901639  0.000
  Freund     3          43     0.7754098  0.9901639  0.000
  Freund     3          44     0.7635246  0.9901639  0.000
  Freund     3          45     0.7696721  0.9901639  0.000
  Freund     3          46     0.7696721  0.9934426  0.000
  Freund     3          47     0.7725410  0.9901639  0.000
  Freund     3          48     0.7639344  0.9868852  0.000
  Freund     3          49     0.7618852  0.9901639  0.000
  Freund     3          50     0.7573770  0.9934426  0.000
  Freund     3          51     0.7327869  0.9901639  0.000
  Freund     3          52     0.7319672  0.9901639  0.000
  Freund     3          53     0.7336066  0.9901639  0.000
  Freund     3          54     0.7200820  0.9967213  0.000
  Freund     3          55     0.7327869  0.9901639  0.000
  Freund     3          56     0.7397541  0.9934426  0.000
  Freund     3          57     0.7229508  0.9934426  0.000
  Freund     3          58     0.7237705  0.9934426  0.000
  Freund     3          59     0.7491803  0.9934426  0.000
  Freund     3          60     0.7442623  0.9934426  0.000
  Freund     3          61     0.7385246  0.9934426  0.000
  Freund     3          62     0.7295082  0.9934426  0.000
  Freund     3          63     0.7200820  0.9934426  0.000
  Freund     3          64     0.7090164  0.9934426  0.000
  Freund     3          65     0.7196721  0.9934426  0.000
  Freund     3          66     0.7188525  0.9934426  0.000
  Freund     3          67     0.7286885  0.9934426  0.000
  Freund     3          68     0.7090164  0.9967213  0.000
  Freund     3          69     0.7200820  0.9967213  0.000
  Freund     3          70     0.7204918  0.9967213  0.000
  Freund     3          71     0.7200820  0.9967213  0.000
  Freund     3          72     0.7118852  0.9967213  0.000
  Freund     3          73     0.7229508  0.9934426  0.000
  Freund     3          74     0.7278689  0.9934426  0.000
  Freund     3          75     0.7250000  0.9934426  0.000
  Freund     3          76     0.7188525  0.9967213  0.000
  Freund     3          77     0.7196721  0.9934426  0.000
  Freund     3          78     0.7135246  0.9934426  0.000
  Freund     3          79     0.7118852  0.9934426  0.000
  Freund     3          80     0.7065574  0.9934426  0.000
  Freund     3          81     0.7073770  0.9934426  0.000
  Freund     3          82     0.7127049  0.9901639  0.000
  Freund     3          83     0.7032787  0.9934426  0.000
  Freund     3          84     0.7016393  0.9901639  0.000
  Freund     3          85     0.6959016  0.9934426  0.000
  Freund     3          86     0.7045082  0.9934426  0.000
  Freund     3          87     0.7213115  0.9934426  0.000
  Freund     3          88     0.7192623  0.9934426  0.000
  Freund     3          89     0.7155738  0.9934426  0.000
  Freund     3          90     0.7344262  0.9934426  0.000
  Freund     3          91     0.7393443  0.9967213  0.000
  Freund     3          92     0.7327869  0.9934426  0.000
  Freund     3          93     0.7336066  0.9967213  0.000
  Freund     3          94     0.7536885  0.9934426  0.000
  Freund     3          95     0.7598361  0.9934426  0.000
  Freund     3          96     0.7569672  0.9934426  0.000
  Freund     3          97     0.7479508  0.9901639  0.000
  Freund     3          98     0.7422131  0.9934426  0.000
  Freund     3          99     0.7549180  0.9934426  0.000
  Freund     3         100     0.7491803  0.9934426  0.000
  Freund     4          20     0.5987705  0.9868852  0.125
  Freund     4          21     0.5963115  0.9901639  0.125
  Freund     4          22     0.6028689  0.9901639  0.125
  Freund     4          23     0.5524590  0.9901639  0.000
  Freund     4          24     0.5868852  0.9901639  0.000
  Freund     4          25     0.5655738  0.9901639  0.000
  Freund     4          26     0.5516393  0.9868852  0.000
  Freund     4          27     0.5500000  0.9868852  0.000
  Freund     4          28     0.5336066  0.9868852  0.000
  Freund     4          29     0.5393443  0.9901639  0.000
  Freund     4          30     0.5256148  0.9868852  0.000
  Freund     4          31     0.5202869  0.9901639  0.000
  Freund     4          32     0.5137295  0.9901639  0.000
  Freund     4          33     0.4805328  0.9901639  0.000
  Freund     4          34     0.4911885  0.9901639  0.000
  Freund     4          35     0.4965164  0.9901639  0.000
  Freund     4          36     0.5375000  0.9901639  0.000
  Freund     4          37     0.5239754  0.9901639  0.000
  Freund     4          38     0.5145492  0.9901639  0.000
  Freund     4          39     0.5088115  0.9901639  0.000
  Freund     4          40     0.5473361  0.9901639  0.000
  Freund     4          41     0.5784836  0.9901639  0.000
  Freund     4          42     0.5891393  0.9901639  0.000
  Freund     4          43     0.5862705  0.9901639  0.000
  Freund     4          44     0.5946721  0.9901639  0.000
  Freund     4          45     0.6135246  0.9934426  0.000
  Freund     4          46     0.6241803  0.9901639  0.000
  Freund     4          47     0.6180328  0.9901639  0.000
  Freund     4          48     0.6135246  0.9901639  0.000
  Freund     4          49     0.6036885  0.9934426  0.000
  Freund     4          50     0.6049180  0.9934426  0.000
  Freund     4          51     0.5950820  0.9934426  0.000
  Freund     4          52     0.5967213  0.9901639  0.000
  Freund     4          53     0.6163934  0.9934426  0.000
  Freund     4          54     0.6131148  0.9901639  0.000
  Freund     4          55     0.6081967  0.9934426  0.000
  Freund     4          56     0.6102459  0.9934426  0.000
  Freund     4          57     0.6184426  0.9934426  0.000
  Freund     4          58     0.6106557  0.9934426  0.000
  Freund     4          59     0.6196721  0.9934426  0.000
  Freund     4          60     0.6290984  0.9901639  0.000
  Freund     4          61     0.6356557  0.9934426  0.000
  Freund     4          62     0.6385246  0.9934426  0.000
  Freund     4          63     0.6336066  0.9901639  0.000
  Freund     4          64     0.6348361  0.9868852  0.000
  Freund     4          65     0.6344262  0.9868852  0.000
  Freund     4          66     0.6413934  0.9868852  0.000
  Freund     4          67     0.6647541  0.9868852  0.000
  Freund     4          68     0.6655738  0.9868852  0.000
  Freund     4          69     0.6688525  0.9868852  0.000
  Freund     4          70     0.6811475  0.9934426  0.000
  Freund     4          71     0.6991803  0.9868852  0.000
  Freund     4          72     0.7012295  0.9868852  0.000
  Freund     4          73     0.7032787  0.9868852  0.000
  Freund     4          74     0.7016393  0.9868852  0.000
  Freund     4          75     0.7045082  0.9868852  0.000
  Freund     4          76     0.7020492  0.9868852  0.000
  Freund     4          77     0.6844262  0.9868852  0.000
  Freund     4          78     0.6918033  0.9868852  0.000
  Freund     4          79     0.6926230  0.9901639  0.000
  Freund     4          80     0.6991803  0.9901639  0.000
  Freund     4          81     0.7118852  0.9901639  0.000
  Freund     4          82     0.7151639  0.9901639  0.000
  Freund     4          83     0.7024590  0.9901639  0.000
  Freund     4          84     0.6930328  0.9901639  0.000
  Freund     4          85     0.7012295  0.9901639  0.000
  Freund     4          86     0.6975410  0.9901639  0.000
  Freund     4          87     0.6975410  0.9901639  0.000
  Freund     4          88     0.6971311  0.9901639  0.000
  Freund     4          89     0.6856557  0.9934426  0.000
  Freund     4          90     0.6815574  0.9901639  0.000
  Freund     4          91     0.6774590  0.9934426  0.000
  Freund     4          92     0.6696721  0.9934426  0.000
  Freund     4          93     0.6790984  0.9934426  0.000
  Freund     4          94     0.6901639  0.9934426  0.000
  Freund     4          95     0.6852459  0.9934426  0.000
  Freund     4          96     0.6823770  0.9934426  0.000
  Freund     4          97     0.6840164  0.9934426  0.000
  Freund     4          98     0.6819672  0.9934426  0.000
  Freund     4          99     0.6881148  0.9934426  0.000
  Freund     4         100     0.6827869  0.9934426  0.000
  Zhu        2          20     0.6838115  0.9967213  0.000
  Zhu        2          21     0.6838115  1.0000000  0.000
  Zhu        2          22     0.6977459  1.0000000  0.000
  Zhu        2          23     0.6899590  1.0000000  0.000
  Zhu        2          24     0.6899590  1.0000000  0.000
  Zhu        2          25     0.6842213  1.0000000  0.000
  Zhu        2          26     0.6793033  1.0000000  0.000
  Zhu        2          27     0.6651639  1.0000000  0.000
  Zhu        2          28     0.6637295  1.0000000  0.000
  Zhu        2          29     0.6522541  1.0000000  0.000
  Zhu        2          30     0.6424180  0.9967213  0.000
  Zhu        2          31     0.6407787  1.0000000  0.000
  Zhu        2          32     0.6526639  1.0000000  0.000
  Zhu        2          33     0.6538934  0.9967213  0.000
  Zhu        2          34     0.6538934  1.0000000  0.000
  Zhu        2          35     0.6518443  0.9967213  0.000
  Zhu        2          36     0.6547131  1.0000000  0.000
  Zhu        2          37     0.6522541  0.9967213  0.000
  Zhu        2          38     0.6645492  0.9934426  0.000
  Zhu        2          39     0.6645492  0.9934426  0.000
  Zhu        2          40     0.6645492  0.9934426  0.000
  Zhu        2          41     0.6825820  0.9934426  0.000
  Zhu        2          42     0.6850410  0.9934426  0.000
  Zhu        2          43     0.6555328  0.9934426  0.000
  Zhu        2          44     0.6555328  0.9934426  0.000
  Zhu        2          45     0.6723361  0.9934426  0.000
  Zhu        2          46     0.6682377  0.9934426  0.000
  Zhu        2          47     0.6727459  0.9934426  0.000
  Zhu        2          48     0.6657787  0.9934426  0.000
  Zhu        2          49     0.6797131  0.9967213  0.000
  Zhu        2          50     0.6633197  0.9934426  0.000
  Zhu        2          51     0.6629098  0.9934426  0.000
  Zhu        2          52     0.6198770  0.9967213  0.000
  Zhu        2          53     0.6346311  0.9967213  0.000
  Zhu        2          54     0.6370902  0.9967213  0.000
  Zhu        2          55     0.6108607  0.9967213  0.000
  Zhu        2          56     0.6096311  0.9967213  0.000
  Zhu        2          57     0.6211066  0.9967213  0.000
  Zhu        2          58     0.6444672  0.9967213  0.000
  Zhu        2          59     0.6444672  0.9967213  0.000
  Zhu        2          60     0.6750000  0.9967213  0.000
  Zhu        2          61     0.6750000  0.9967213  0.000
  Zhu        2          62     0.6577869  0.9967213  0.000
  Zhu        2          63     0.6471311  0.9967213  0.000
  Zhu        2          64     0.6805328  0.9967213  0.000
  Zhu        2          65     0.7002049  0.9967213  0.000
  Zhu        2          66     0.6600410  0.9967213  0.000
  Zhu        2          67     0.6670082  0.9967213  0.000
  Zhu        2          68     0.6780738  0.9967213  0.000
  Zhu        2          69     0.6784836  0.9967213  0.000
  Zhu        2          70     0.6768443  0.9967213  0.000
  Zhu        2          71     0.6715164  0.9967213  0.000
  Zhu        2          72     0.6719262  0.9967213  0.000
  Zhu        2          73     0.6903689  0.9967213  0.000
  Zhu        2          74     0.6838115  0.9967213  0.000
  Zhu        2          75     0.6838115  0.9967213  0.000
  Zhu        2          76     0.6838115  0.9967213  0.000
  Zhu        2          77     0.6747951  0.9967213  0.000
  Zhu        2          78     0.6747951  0.9967213  0.000
  Zhu        2          79     0.6801230  0.9967213  0.000
  Zhu        2          80     0.6801230  0.9967213  0.000
  Zhu        2          81     0.6772541  0.9967213  0.000
  Zhu        2          82     0.6772541  1.0000000  0.000
  Zhu        2          83     0.6719262  0.9967213  0.000
  Zhu        2          84     0.6719262  1.0000000  0.000
  Zhu        2          85     0.6686475  0.9967213  0.000
  Zhu        2          86     0.6702869  0.9967213  0.000
  Zhu        2          87     0.6731557  0.9967213  0.000
  Zhu        2          88     0.6731557  1.0000000  0.000
  Zhu        2          89     0.6633197  1.0000000  0.000
  Zhu        2          90     0.6489754  1.0000000  0.000
  Zhu        2          91     0.6756148  0.9967213  0.000
  Zhu        2          92     0.6616803  0.9967213  0.000
  Zhu        2          93     0.6608607  0.9967213  0.000
  Zhu        2          94     0.6424180  0.9967213  0.000
  Zhu        2          95     0.6538934  0.9967213  0.000
  Zhu        2          96     0.6670082  0.9967213  0.000
  Zhu        2          97     0.6649590  0.9967213  0.000
  Zhu        2          98     0.6616803  0.9967213  0.000
  Zhu        2          99     0.6653689  0.9967213  0.000
  Zhu        2         100     0.6321721  0.9967213  0.000
  Zhu        3          20     0.5063525  0.9934426  0.000
  Zhu        3          21     0.5034836  0.9967213  0.000
  Zhu        3          22     0.4805328  0.9934426  0.000
  Zhu        3          23     0.5112705  0.9967213  0.000
  Zhu        3          24     0.5100410  0.9967213  0.000
  Zhu        3          25     0.5264344  0.9967213  0.000
  Zhu        3          26     0.5055328  1.0000000  0.000
  Zhu        3          27     0.5319672  1.0000000  0.000
  Zhu        3          28     0.5217213  1.0000000  0.000
  Zhu        3          29     0.4959016  0.9934426  0.000
  Zhu        3          30     0.4893443  0.9967213  0.000
  Zhu        3          31     0.4868852  0.9967213  0.000
  Zhu        3          32     0.5024590  1.0000000  0.000
  Zhu        3          33     0.4889344  1.0000000  0.000
  Zhu        3          34     0.4803279  1.0000000  0.000
  Zhu        3          35     0.5020492  1.0000000  0.000
  Zhu        3          36     0.5139344  1.0000000  0.000
  Zhu        3          37     0.4987705  1.0000000  0.000
  Zhu        3          38     0.5311475  1.0000000  0.000
  Zhu        3          39     0.5254098  1.0000000  0.000
  Zhu        3          40     0.5258197  1.0000000  0.000
  Zhu        3          41     0.5254098  1.0000000  0.000
  Zhu        3          42     0.5090164  1.0000000  0.000
  Zhu        3          43     0.5209016  1.0000000  0.000
  Zhu        3          44     0.5176230  1.0000000  0.000
  Zhu        3          45     0.5184426  1.0000000  0.000
  Zhu        3          46     0.5364754  1.0000000  0.000
  Zhu        3          47     0.5348361  1.0000000  0.000
  Zhu        3          48     0.5254098  1.0000000  0.000
  Zhu        3          49     0.5303279  1.0000000  0.000
  Zhu        3          50     0.5356557  1.0000000  0.000
  Zhu        3          51     0.5352459  1.0000000  0.000
  Zhu        3          52     0.5491803  1.0000000  0.000
  Zhu        3          53     0.5479508  1.0000000  0.000
  Zhu        3          54     0.5454918  1.0000000  0.000
  Zhu        3          55     0.5454918  1.0000000  0.000
  Zhu        3          56     0.5430328  1.0000000  0.000
  Zhu        3          57     0.5286885  1.0000000  0.000
  Zhu        3          58     0.5487705  1.0000000  0.000
  Zhu        3          59     0.5352459  1.0000000  0.000
  Zhu        3          60     0.5413934  1.0000000  0.000
  Zhu        3          61     0.5516393  0.9967213  0.000
  Zhu        3          62     0.5565574  0.9967213  0.000
  Zhu        3          63     0.5836066  0.9967213  0.000
  Zhu        3          64     0.5831967  0.9967213  0.000
  Zhu        3          65     0.5897541  0.9967213  0.000
  Zhu        3          66     0.5909836  0.9967213  0.000
  Zhu        3          67     0.5950820  1.0000000  0.000
  Zhu        3          68     0.5823770  0.9967213  0.000
  Zhu        3          69     0.6163934  0.9967213  0.000
  Zhu        3          70     0.6069672  0.9967213  0.000
  Zhu        3          71     0.6065574  0.9967213  0.000
  Zhu        3          72     0.5721311  0.9967213  0.000
  Zhu        3          73     0.5721311  0.9934426  0.000
  Zhu        3          74     0.5635246  0.9934426  0.000
  Zhu        3          75     0.5635246  0.9934426  0.000
  Zhu        3          76     0.5762295  0.9967213  0.000
  Zhu        3          77     0.5840164  0.9967213  0.000
  Zhu        3          78     0.5709016  0.9967213  0.000
  Zhu        3          79     0.5647541  0.9967213  0.000
  Zhu        3          80     0.5704918  0.9967213  0.000
  Zhu        3          81     0.5737705  0.9967213  0.000
  Zhu        3          82     0.6229508  0.9967213  0.000
  Zhu        3          83     0.6229508  0.9967213  0.000
  Zhu        3          84     0.6245902  0.9967213  0.000
  Zhu        3          85     0.6299180  0.9967213  0.000
  Zhu        3          86     0.6491803  0.9967213  0.000
  Zhu        3          87     0.6532787  0.9967213  0.000
  Zhu        3          88     0.6532787  0.9967213  0.000
  Zhu        3          89     0.6442623  0.9967213  0.000
  Zhu        3          90     0.6495902  0.9967213  0.000
  Zhu        3          91     0.6508197  0.9967213  0.000
  Zhu        3          92     0.6426230  1.0000000  0.000
  Zhu        3          93     0.6319672  1.0000000  0.000
  Zhu        3          94     0.6278689  1.0000000  0.000
  Zhu        3          95     0.6204918  0.9967213  0.000
  Zhu        3          96     0.6274590  0.9967213  0.000
  Zhu        3          97     0.6319672  0.9967213  0.000
  Zhu        3          98     0.6327869  0.9967213  0.000
  Zhu        3          99     0.6172131  0.9967213  0.000
  Zhu        3         100     0.6172131  0.9967213  0.000
  Zhu        4          20     0.6866803  0.9967213  0.000
  Zhu        4          21     0.7364754  0.9967213  0.000
  Zhu        4          22     0.7467213  0.9967213  0.000
  Zhu        4          23     0.7276639  0.9934426  0.000
  Zhu        4          24     0.7329918  0.9901639  0.000
  Zhu        4          25     0.7293033  0.9934426  0.000
  Zhu        4          26     0.7051230  0.9967213  0.000
  Zhu        4          27     0.7047131  0.9967213  0.000
  Zhu        4          28     0.6944672  0.9967213  0.000
  Zhu        4          29     0.7047131  0.9934426  0.125
  Zhu        4          30     0.7071721  0.9934426  0.125
  Zhu        4          31     0.6575820  0.9934426  0.125
  Zhu        4          32     0.6661885  0.9901639  0.125
  Zhu        4          33     0.6653689  0.9934426  0.125
  Zhu        4          34     0.6506148  0.9934426  0.125
  Zhu        4          35     0.6530738  0.9934426  0.125
  Zhu        4          36     0.6682377  0.9901639  0.125
  Zhu        4          37     0.6526639  0.9934426  0.125
  Zhu        4          38     0.6485656  0.9901639  0.125
  Zhu        4          39     0.6569672  0.9934426  0.125
  Zhu        4          40     0.6557377  0.9934426  0.000
  Zhu        4          41     0.6737705  0.9934426  0.125
  Zhu        4          42     0.6774590  0.9901639  0.125
  Zhu        4          43     0.6815574  0.9901639  0.125
  Zhu        4          44     0.6750000  0.9901639  0.125
  Zhu        4          45     0.6733607  0.9901639  0.125
  Zhu        4          46     0.6659836  0.9901639  0.125
  Zhu        4          47     0.6569672  0.9901639  0.125
  Zhu        4          48     0.6684426  0.9901639  0.000
  Zhu        4          49     0.6622951  0.9901639  0.125
  Zhu        4          50     0.6602459  0.9901639  0.125
  Zhu        4          51     0.6663934  0.9901639  0.125
  Zhu        4          52     0.6717213  0.9901639  0.000
  Zhu        4          53     0.6909836  0.9901639  0.125
  Zhu        4          54     0.6995902  0.9901639  0.125
  Zhu        4          55     0.6942623  0.9901639  0.000
  Zhu        4          56     0.7180328  0.9901639  0.125
  Zhu        4          57     0.7241803  0.9901639  0.125
  Zhu        4          58     0.7303279  0.9901639  0.125
  Zhu        4          59     0.7418033  0.9901639  0.125
  Zhu        4          60     0.7413934  0.9901639  0.125
  Zhu        4          61     0.7327869  0.9901639  0.125
  Zhu        4          62     0.7245902  0.9901639  0.125
  Zhu        4          63     0.7213115  0.9901639  0.125
  Zhu        4          64     0.7049180  0.9901639  0.125
  Zhu        4          65     0.7286885  0.9901639  0.125
  Zhu        4          66     0.7299180  0.9901639  0.125
  Zhu        4          67     0.7155738  0.9901639  0.125
  Zhu        4          68     0.7061475  0.9901639  0.125
  Zhu        4          69     0.7098361  0.9901639  0.125
  Zhu        4          70     0.7090164  0.9901639  0.125
  Zhu        4          71     0.7118852  0.9901639  0.125
  Zhu        4          72     0.7163934  0.9901639  0.125
  Zhu        4          73     0.7151639  0.9901639  0.125
  Zhu        4          74     0.7229508  0.9901639  0.125
  Zhu        4          75     0.7168033  0.9901639  0.125
  Zhu        4          76     0.7172131  0.9901639  0.125
  Zhu        4          77     0.7090164  0.9901639  0.125
  Zhu        4          78     0.7188525  0.9901639  0.125
  Zhu        4          79     0.7180328  0.9934426  0.125
  Zhu        4          80     0.7233607  0.9901639  0.125
  Zhu        4          81     0.7245902  0.9901639  0.000
  Zhu        4          82     0.7209016  0.9901639  0.000
  Zhu        4          83     0.7196721  0.9901639  0.000
  Zhu        4          84     0.7057377  0.9901639  0.000
  Zhu        4          85     0.7024590  0.9901639  0.125
  Zhu        4          86     0.7000000  0.9901639  0.125
  Zhu        4          87     0.6897541  0.9901639  0.125
  Zhu        4          88     0.6860656  0.9901639  0.125
  Zhu        4          89     0.6889344  0.9901639  0.125
  Zhu        4          90     0.6959016  0.9901639  0.125
  Zhu        4          91     0.6959016  0.9901639  0.125
  Zhu        4          92     0.7081967  0.9901639  0.125
  Zhu        4          93     0.7069672  0.9901639  0.125
  Zhu        4          94     0.7061475  0.9901639  0.125
  Zhu        4          95     0.7004098  0.9901639  0.125
  Zhu        4          96     0.6905738  0.9901639  0.125
  Zhu        4          97     0.6930328  0.9901639  0.125
  Zhu        4          98     0.7004098  0.9901639  0.125
  Zhu        4          99     0.6942623  0.9901639  0.125
  Zhu        4         100     0.6905738  0.9901639  0.125

ROC was used to select the optimal model using the largest value.
The final values used for the model were mfinal = 21, maxdepth = 3
 and coeflearn = Freund.

    \end{Verbatim}

    
    \begin{verbatim}
Bootstrapped (1 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   No  Yes
       No  96.8  2.6
       Yes  0.6  0.0
                            
 Accuracy (average) : 0.9681

    \end{verbatim}

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_78_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Confusion Matrix for adaboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} caretPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}ada\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{raw\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}caretPredictedClass\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  356   9
       Yes   4   2
                                          
               Accuracy : 0.965           
                 95% CI : (0.9408, 0.9812)
    No Information Rate : 0.9704          
    P-Value [Acc > NIR] : 0.7841          
                                          
                  Kappa : 0.2189          
 Mcnemar's Test P-Value : 0.2673          
                                          
            Sensitivity : 0.9889          
            Specificity : 0.1818          
         Pos Pred Value : 0.9753          
         Neg Pred Value : 0.3333          
             Prevalence : 0.9704          
         Detection Rate : 0.9596          
   Detection Prevalence : 0.9838          
      Balanced Accuracy : 0.5854          
                                          
       'Positive' Class : No              
                                          
    \end{verbatim}

    
    ROC plot for adaboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} ada\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}ada\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         ada\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}ada\PYZus{}pred\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         ada\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}ada\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}ada\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for adaboost\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}ada\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.7848485


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_82_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Visulaizing the rules coming out of ada boost. We can loop and print all
the trees which was built using boosting. For simplicity, we are
printing just one of the trees

To retrieve the understand any model specific attribute, we have to call
the \textbf{\$finalmodel} of the train object created using caret
package. This is a generic way to use functions which are model
specific. Here \textbf{get\_tree()} is a function of
\textbf{fastadaboost} package which cannot be used unless the the object
returned is not of adaboost class.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{c+c1}{\PYZsh{}listTreesAda(ada\PYZus{}model\PYZdl{}finalModel,3) \PYZsh{}this is a function with rattle package}
         \PY{c+c1}{\PYZsh{}get\PYZus{}tree(ada\PYZus{}model\PYZdl{}finalModel,2)}
\end{Verbatim}


    \hypertarget{boosting-with-adaboost-upsample}{%
\subsubsection{Boosting with adaboost
(upsample)}\label{boosting-with-adaboost-upsample}}

The below code chunk sets some of the control parameters for adaboost

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} objControl \PY{o}{\PYZlt{}\PYZhy{}} trainControl\PY{p}{(}method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{boot\PYZsq{}}\PY{p}{,} number \PY{o}{=} \PY{l+m}{1}\PY{p}{,}
                                    returnResamp\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{all\PYZsq{}}\PY{p}{,}
                                    summaryFunction \PY{o}{=} twoClassSummary\PY{p}{,}
                                    savePredictions \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    classProbs \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    sampling \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{up\PYZdq{}}\PY{p}{)}\PY{c+c1}{\PYZsh{}, p = 0.70) \PYZsh{}in case method = \PYZsh{}\PYZdq{}LGOCV\PYZdq{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} search\PYZus{}grid \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{expand.grid}\PY{p}{(}mfinal \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{20}\PY{o}{:}\PY{l+m}{100}\PY{p}{)}\PY{p}{,} maxdepth \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{2}\PY{o}{:}\PY{l+m}{4}\PY{p}{)}\PY{p}{,}
                             coeflearn \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Breiman\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{Freund\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{Zhu\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    After setting the control paramters, the model is run

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} num\PYZus{}cores \PY{o}{\PYZlt{}\PYZhy{}} makeCluster\PY{p}{(}detectCores\PY{p}{(}\PY{p}{)}\PY{l+m}{\PYZhy{}5}\PY{p}{)}
         registerDoParallel\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         tic\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Adaptive Boosting with UP Sample\PYZdq{}}\PY{p}{)}
         
         \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         ada\PYZus{}up\PYZus{}model \PY{o}{\PYZlt{}\PYZhy{}} train\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{,}
                           method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{AdaBoost.M1\PYZsq{}}\PY{p}{,}
                           trControl\PY{o}{=}objControl\PY{p}{,}
                           tuneGrid \PY{o}{=} search\PYZus{}grid\PY{p}{,}
                           metric \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{ROC\PYZdq{}}\PY{p}{)}
         stopCluster\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         toc\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Adaptive Boosting with UP Sample: 116.933 sec elapsed

    \end{Verbatim}

    Confusion Matrix for adaboost on train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{c+c1}{\PYZsh{}ada\PYZus{}up\PYZus{}model\PYZdl{}finalModel \PYZsh{}ada\PYZus{}up\PYZus{}model\PYZdl{}results}
         \PY{k+kp}{print}\PY{p}{(}ada\PYZus{}up\PYZus{}model\PY{p}{)}
         confusionMatrix.train\PY{p}{(}ada\PYZus{}up\PYZus{}model\PY{p}{)}
         plot\PY{p}{(}varImp\PY{p}{(}ada\PYZus{}up\PYZus{}model\PY{p}{)}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable importance from Adaboost with Up Sample\PYZdq{}}\PY{p}{,} col \PY{o}{=} \PY{l+m}{2}\PY{p}{,} lwd \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
AdaBoost.M1 

868 samples
  5 predictor
  2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Bootstrapped (1 reps) 
Summary of sample sizes: 868 
Addtional sampling using up-sampling

Resampling results across tuning parameters:

  coeflearn  maxdepth  mfinal  ROC        Sens       Spec 
  Breiman    2          20     0.7114754  0.9245902  0.000
  Breiman    2          21     0.7135246  0.9278689  0.125
  Breiman    2          22     0.7135246  0.9344262  0.000
  Breiman    2          23     0.6938525  0.9311475  0.000
  Breiman    2          24     0.6938525  0.9311475  0.000
  Breiman    2          25     0.6821721  0.9344262  0.000
  Breiman    2          26     0.6903689  0.9409836  0.000
  Breiman    2          27     0.6934426  0.9409836  0.000
  Breiman    2          28     0.6926230  0.9344262  0.000
  Breiman    2          29     0.6926230  0.9475410  0.000
  Breiman    2          30     0.6868852  0.9442623  0.000
  Breiman    2          31     0.6868852  0.9475410  0.000
  Breiman    2          32     0.6889344  0.9442623  0.000
  Breiman    2          33     0.7040984  0.9409836  0.000
  Breiman    2          34     0.6844262  0.9409836  0.000
  Breiman    2          35     0.6795082  0.9475410  0.000
  Breiman    2          36     0.6795082  0.9475410  0.000
  Breiman    2          37     0.6856557  0.9508197  0.000
  Breiman    2          38     0.6950820  0.9475410  0.000
  Breiman    2          39     0.6950820  0.9475410  0.000
  Breiman    2          40     0.7073770  0.9475410  0.000
  Breiman    2          41     0.7065574  0.9442623  0.000
  Breiman    2          42     0.7020492  0.9442623  0.000
  Breiman    2          43     0.7020492  0.9442623  0.000
  Breiman    2          44     0.7147541  0.9508197  0.000
  Breiman    2          45     0.7094262  0.9475410  0.000
  Breiman    2          46     0.7057377  0.9442623  0.000
  Breiman    2          47     0.7020492  0.9475410  0.000
  Breiman    2          48     0.7090164  0.9475410  0.000
  Breiman    2          49     0.7090164  0.9508197  0.000
  Breiman    2          50     0.7090164  0.9540984  0.000
  Breiman    2          51     0.6875000  0.9540984  0.000
  Breiman    2          52     0.6948770  0.9508197  0.000
  Breiman    2          53     0.7129098  0.9540984  0.000
  Breiman    2          54     0.7227459  0.9540984  0.000
  Breiman    2          55     0.7252049  0.9606557  0.000
  Breiman    2          56     0.7096311  0.9573770  0.000
  Breiman    2          57     0.7137295  0.9606557  0.000
  Breiman    2          58     0.7129098  0.9606557  0.000
  Breiman    2          59     0.7137295  0.9606557  0.000
  Breiman    2          60     0.7141393  0.9573770  0.000
  Breiman    2          61     0.7141393  0.9639344  0.000
  Breiman    2          62     0.7174180  0.9606557  0.000
  Breiman    2          63     0.7174180  0.9639344  0.000
  Breiman    2          64     0.7092213  0.9606557  0.000
  Breiman    2          65     0.7004098  0.9606557  0.000
  Breiman    2          66     0.7258197  0.9606557  0.000
  Breiman    2          67     0.7356557  0.9606557  0.000
  Breiman    2          68     0.7307377  0.9573770  0.000
  Breiman    2          69     0.7295082  0.9573770  0.000
  Breiman    2          70     0.7254098  0.9540984  0.000
  Breiman    2          71     0.7254098  0.9573770  0.000
  Breiman    2          72     0.7245902  0.9573770  0.000
  Breiman    2          73     0.7245902  0.9606557  0.000
  Breiman    2          74     0.7254098  0.9639344  0.000
  Breiman    2          75     0.7299180  0.9639344  0.000
  Breiman    2          76     0.7352459  0.9639344  0.000
  Breiman    2          77     0.7340164  0.9672131  0.000
  Breiman    2          78     0.7413934  0.9704918  0.000
  Breiman    2          79     0.7413934  0.9639344  0.000
  Breiman    2          80     0.7413934  0.9704918  0.000
  Breiman    2          81     0.7413934  0.9704918  0.000
  Breiman    2          82     0.7397541  0.9704918  0.000
  Breiman    2          83     0.7381148  0.9672131  0.000
  Breiman    2          84     0.7430328  0.9704918  0.000
  Breiman    2          85     0.7545082  0.9737705  0.000
  Breiman    2          86     0.7495902  0.9737705  0.000
  Breiman    2          87     0.7532787  0.9737705  0.000
  Breiman    2          88     0.7495902  0.9704918  0.000
  Breiman    2          89     0.7495902  0.9704918  0.000
  Breiman    2          90     0.7463115  0.9737705  0.000
  Breiman    2          91     0.7336066  0.9672131  0.000
  Breiman    2          92     0.7331967  0.9672131  0.000
  Breiman    2          93     0.7331967  0.9737705  0.000
  Breiman    2          94     0.7295082  0.9672131  0.000
  Breiman    2          95     0.7295082  0.9672131  0.000
  Breiman    2          96     0.7360656  0.9672131  0.000
  Breiman    2          97     0.7360656  0.9704918  0.000
  Breiman    2          98     0.7434426  0.9672131  0.000
  Breiman    2          99     0.7372951  0.9672131  0.000
  Breiman    2         100     0.7430328  0.9672131  0.000
  Breiman    3          20     0.7313525  0.9475410  0.000
  Breiman    3          21     0.7245902  0.9540984  0.000
  Breiman    3          22     0.6766393  0.9573770  0.000
  Breiman    3          23     0.6795082  0.9639344  0.000
  Breiman    3          24     0.6741803  0.9573770  0.000
  Breiman    3          25     0.6827869  0.9606557  0.000
  Breiman    3          26     0.6959016  0.9704918  0.000
  Breiman    3          27     0.7069672  0.9606557  0.000
  Breiman    3          28     0.6959016  0.9704918  0.000
  Breiman    3          29     0.6959016  0.9672131  0.000
  Breiman    3          30     0.6836066  0.9704918  0.000
  Breiman    3          31     0.6573770  0.9672131  0.000
  Breiman    3          32     0.6676230  0.9672131  0.000
  Breiman    3          33     0.6879098  0.9672131  0.000
  Breiman    3          34     0.7047131  0.9639344  0.000
  Breiman    3          35     0.7112705  0.9704918  0.000
  Breiman    3          36     0.7116803  0.9672131  0.000
  Breiman    3          37     0.7252049  0.9704918  0.000
  Breiman    3          38     0.7223361  0.9737705  0.000
  Breiman    3          39     0.7258197  0.9770492  0.000
  Breiman    3          40     0.7254098  0.9770492  0.000
  Breiman    3          41     0.7200820  0.9737705  0.000
  Breiman    3          42     0.7135246  0.9737705  0.000
  Breiman    3          43     0.7024590  0.9803279  0.000
  Breiman    3          44     0.7155738  0.9770492  0.000
  Breiman    3          45     0.7127049  0.9770492  0.000
  Breiman    3          46     0.7094262  0.9770492  0.000
  Breiman    3          47     0.7061475  0.9803279  0.000
  Breiman    3          48     0.7172131  0.9803279  0.000
  Breiman    3          49     0.7028689  0.9836066  0.000
  Breiman    3          50     0.7016393  0.9836066  0.000
  Breiman    3          51     0.6963115  0.9836066  0.000
  Breiman    3          52     0.6950820  0.9836066  0.000
  Breiman    3          53     0.6971311  0.9803279  0.000
  Breiman    3          54     0.6959016  0.9836066  0.000
  Breiman    3          55     0.6983607  0.9836066  0.000
  Breiman    3          56     0.6897541  0.9836066  0.000
  Breiman    3          57     0.6868852  0.9803279  0.000
  Breiman    3          58     0.6864754  0.9770492  0.000
  Breiman    3          59     0.6860656  0.9770492  0.000
  Breiman    3          60     0.6680328  0.9803279  0.000
  Breiman    3          61     0.6622951  0.9803279  0.000
  Breiman    3          62     0.6741803  0.9803279  0.000
  Breiman    3          63     0.6799180  0.9836066  0.000
  Breiman    3          64     0.6877049  0.9836066  0.000
  Breiman    3          65     0.6868852  0.9836066  0.000
  Breiman    3          66     0.6909836  0.9836066  0.000
  Breiman    3          67     0.6913934  0.9836066  0.000
  Breiman    3          68     0.6913934  0.9836066  0.000
  Breiman    3          69     0.6831967  0.9836066  0.000
  Breiman    3          70     0.6913934  0.9836066  0.000
  Breiman    3          71     0.6897541  0.9836066  0.000
  Breiman    3          72     0.6893443  0.9836066  0.000
  Breiman    3          73     0.6844262  0.9836066  0.000
  Breiman    3          74     0.6717213  0.9836066  0.000
  Breiman    3          75     0.6737705  0.9836066  0.000
  Breiman    3          76     0.6897541  0.9836066  0.000
  Breiman    3          77     0.6901639  0.9868852  0.000
  Breiman    3          78     0.6959016  0.9901639  0.000
  Breiman    3          79     0.6926230  0.9868852  0.000
  Breiman    3          80     0.6926230  0.9901639  0.000
  Breiman    3          81     0.6881148  0.9901639  0.000
  Breiman    3          82     0.6827869  0.9868852  0.000
  Breiman    3          83     0.6807377  0.9868852  0.000
  Breiman    3          84     0.6807377  0.9868852  0.000
  Breiman    3          85     0.6881148  0.9901639  0.000
  Breiman    3          86     0.6872951  0.9901639  0.000
  Breiman    3          87     0.6852459  0.9901639  0.000
  Breiman    3          88     0.6840164  0.9868852  0.000
  Breiman    3          89     0.6840164  0.9868852  0.000
  Breiman    3          90     0.6889344  0.9868852  0.000
  Breiman    3          91     0.6950820  0.9868852  0.000
  Breiman    3          92     0.6930328  0.9868852  0.000
  Breiman    3          93     0.6885246  0.9868852  0.000
  Breiman    3          94     0.6655738  0.9868852  0.000
  Breiman    3          95     0.6577869  0.9868852  0.000
  Breiman    3          96     0.6586066  0.9868852  0.000
  Breiman    3          97     0.6586066  0.9868852  0.000
  Breiman    3          98     0.6557377  0.9868852  0.000
  Breiman    3          99     0.6651639  0.9868852  0.000
  Breiman    3         100     0.6647541  0.9868852  0.000
  Breiman    4          20     0.7075820  0.9868852  0.000
  Breiman    4          21     0.7057377  0.9901639  0.000
  Breiman    4          22     0.6741803  0.9901639  0.000
  Breiman    4          23     0.6733607  0.9934426  0.000
  Breiman    4          24     0.6696721  0.9901639  0.000
  Breiman    4          25     0.6762295  0.9868852  0.000
  Breiman    4          26     0.6733607  0.9901639  0.000
  Breiman    4          27     0.6504098  0.9934426  0.000
  Breiman    4          28     0.6846311  0.9934426  0.000
  Breiman    4          29     0.6735656  0.9901639  0.000
  Breiman    4          30     0.6649590  0.9967213  0.000
  Breiman    4          31     0.6579918  0.9934426  0.000
  Breiman    4          32     0.6784836  0.9967213  0.000
  Breiman    4          33     0.6817623  0.9967213  0.000
  Breiman    4          34     0.6866803  0.9967213  0.000
  Breiman    4          35     0.6645492  0.9934426  0.000
  Breiman    4          36     0.6686475  0.9967213  0.000
  Breiman    4          37     0.6645492  0.9967213  0.000
  Breiman    4          38     0.6784836  0.9967213  0.000
  Breiman    4          39     0.6735656  0.9934426  0.000
  Breiman    4          40     0.6801230  0.9967213  0.000
  Breiman    4          41     0.6805328  0.9934426  0.000
  Breiman    4          42     0.6686475  0.9901639  0.000
  Breiman    4          43     0.6571721  0.9934426  0.000
  Breiman    4          44     0.6571721  0.9934426  0.000
  Breiman    4          45     0.6518443  0.9967213  0.000
  Breiman    4          46     0.6440574  0.9967213  0.000
  Breiman    4          47     0.6493852  0.9967213  0.000
  Breiman    4          48     0.6543033  0.9967213  0.000
  Breiman    4          49     0.6522541  0.9901639  0.000
  Breiman    4          50     0.6485656  0.9934426  0.000
  Breiman    4          51     0.6502049  0.9934426  0.000
  Breiman    4          52     0.6592213  0.9934426  0.000
  Breiman    4          53     0.6678279  0.9934426  0.000
  Breiman    4          54     0.6838115  0.9934426  0.000
  Breiman    4          55     0.6665984  0.9934426  0.000
  Breiman    4          56     0.6670082  0.9934426  0.000
  Breiman    4          57     0.6415984  0.9934426  0.000
  Breiman    4          58     0.6411885  0.9934426  0.000
  Breiman    4          59     0.6297131  0.9934426  0.000
  Breiman    4          60     0.6309426  0.9934426  0.000
  Breiman    4          61     0.6375000  0.9934426  0.000
  Breiman    4          62     0.6448770  0.9967213  0.000
  Breiman    4          63     0.6555328  0.9901639  0.000
  Breiman    4          64     0.6657787  0.9901639  0.000
  Breiman    4          65     0.6612705  0.9901639  0.000
  Breiman    4          66     0.6665984  0.9901639  0.000
  Breiman    4          67     0.6575820  0.9901639  0.000
  Breiman    4          68     0.6620902  0.9901639  0.000
  Breiman    4          69     0.6620902  0.9901639  0.000
  Breiman    4          70     0.6772541  0.9901639  0.000
  Breiman    4          71     0.6686475  0.9901639  0.000
  Breiman    4          72     0.6682377  0.9901639  0.000
  Breiman    4          73     0.6661885  0.9901639  0.000
  Breiman    4          74     0.6420082  0.9901639  0.000
  Breiman    4          75     0.6432377  0.9901639  0.000
  Breiman    4          76     0.6272541  0.9901639  0.000
  Breiman    4          77     0.6293033  0.9901639  0.000
  Breiman    4          78     0.6411885  0.9901639  0.000
  Breiman    4          79     0.6370902  0.9901639  0.000
  Breiman    4          80     0.6375000  0.9901639  0.000
  Breiman    4          81     0.6559426  0.9901639  0.000
  Breiman    4          82     0.6547131  0.9901639  0.000
  Breiman    4          83     0.6735656  0.9901639  0.000
  Breiman    4          84     0.6706967  0.9901639  0.000
  Breiman    4          85     0.6600410  0.9901639  0.000
  Breiman    4          86     0.6715164  0.9901639  0.000
  Breiman    4          87     0.6690574  0.9901639  0.000
  Breiman    4          88     0.6661885  0.9901639  0.000
  Breiman    4          89     0.6686475  0.9901639  0.000
  Breiman    4          90     0.6686475  0.9901639  0.000
  Breiman    4          91     0.6711066  0.9901639  0.000
  Breiman    4          92     0.6727459  0.9901639  0.000
  Breiman    4          93     0.6797131  0.9901639  0.000
  Breiman    4          94     0.6911885  0.9901639  0.000
  Breiman    4          95     0.6965164  0.9901639  0.000
  Breiman    4          96     0.7014344  0.9901639  0.000
  Breiman    4          97     0.6965164  0.9901639  0.000
  Breiman    4          98     0.6969262  0.9901639  0.000
  Breiman    4          99     0.6961066  0.9901639  0.000
  Breiman    4         100     0.6739754  0.9901639  0.000
  Freund     2          20     0.7979508  0.9344262  0.125
  Freund     2          21     0.7993852  0.9409836  0.125
  Freund     2          22     0.7989754  0.9475410  0.000
  Freund     2          23     0.7924180  0.9508197  0.000
  Freund     2          24     0.7858607  0.9573770  0.000
  Freund     2          25     0.7797131  0.9606557  0.000
  Freund     2          26     0.7838115  0.9508197  0.000
  Freund     2          27     0.7829918  0.9606557  0.000
  Freund     2          28     0.7747951  0.9606557  0.000
  Freund     2          29     0.7584016  0.9508197  0.000
  Freund     2          30     0.7446721  0.9475410  0.000
  Freund     2          31     0.7413934  0.9508197  0.000
  Freund     2          32     0.7508197  0.9540984  0.000
  Freund     2          33     0.7508197  0.9606557  0.000
  Freund     2          34     0.7680328  0.9639344  0.000
  Freund     2          35     0.7741803  0.9704918  0.000
  Freund     2          36     0.7799180  0.9639344  0.000
  Freund     2          37     0.7831967  0.9672131  0.000
  Freund     2          38     0.7635246  0.9573770  0.000
  Freund     2          39     0.7586066  0.9672131  0.000
  Freund     2          40     0.7610656  0.9704918  0.000
  Freund     2          41     0.7622951  0.9672131  0.000
  Freund     2          42     0.7581967  0.9672131  0.000
  Freund     2          43     0.7651639  0.9704918  0.125
  Freund     2          44     0.7672131  0.9672131  0.125
  Freund     2          45     0.7598361  0.9672131  0.125
  Freund     2          46     0.7487705  0.9737705  0.125
  Freund     2          47     0.7471311  0.9737705  0.125
  Freund     2          48     0.7471311  0.9737705  0.125
  Freund     2          49     0.7483607  0.9737705  0.125
  Freund     2          50     0.7340164  0.9737705  0.125
  Freund     2          51     0.7340164  0.9737705  0.000
  Freund     2          52     0.7434426  0.9704918  0.000
  Freund     2          53     0.7454918  0.9704918  0.000
  Freund     2          54     0.7454918  0.9704918  0.000
  Freund     2          55     0.7581967  0.9737705  0.125
  Freund     2          56     0.7540984  0.9737705  0.000
  Freund     2          57     0.7536885  0.9737705  0.000
  Freund     2          58     0.7536885  0.9737705  0.000
  Freund     2          59     0.7442623  0.9737705  0.000
  Freund     2          60     0.7442623  0.9737705  0.000
  Freund     2          61     0.7471311  0.9737705  0.000
  Freund     2          62     0.7581967  0.9704918  0.000
  Freund     2          63     0.7581967  0.9770492  0.000
  Freund     2          64     0.7512295  0.9737705  0.000
  Freund     2          65     0.7512295  0.9770492  0.000
  Freund     2          66     0.7500000  0.9803279  0.000
  Freund     2          67     0.7430328  0.9803279  0.000
  Freund     2          68     0.7475410  0.9803279  0.000
  Freund     2          69     0.7668033  0.9836066  0.000
  Freund     2          70     0.7618852  0.9803279  0.000
  Freund     2          71     0.7618852  0.9836066  0.000
  Freund     2          72     0.7610656  0.9803279  0.000
  Freund     2          73     0.7758197  0.9803279  0.000
  Freund     2          74     0.7602459  0.9803279  0.000
  Freund     2          75     0.7725410  0.9803279  0.000
  Freund     2          76     0.7754098  0.9770492  0.000
  Freund     2          77     0.7754098  0.9803279  0.000
  Freund     2          78     0.7745902  0.9770492  0.000
  Freund     2          79     0.7737705  0.9803279  0.000
  Freund     2          80     0.7487705  0.9770492  0.000
  Freund     2          81     0.7602459  0.9803279  0.000
  Freund     2          82     0.7627049  0.9770492  0.000
  Freund     2          83     0.7741803  0.9737705  0.000
  Freund     2          84     0.7741803  0.9803279  0.000
  Freund     2          85     0.7565574  0.9770492  0.000
  Freund     2          86     0.7557377  0.9803279  0.000
  Freund     2          87     0.7459016  0.9803279  0.000
  Freund     2          88     0.7618852  0.9803279  0.000
  Freund     2          89     0.7606557  0.9803279  0.000
  Freund     2          90     0.7471311  0.9803279  0.000
  Freund     2          91     0.7372951  0.9803279  0.125
  Freund     2          92     0.7528689  0.9803279  0.125
  Freund     2          93     0.7356557  0.9803279  0.125
  Freund     2          94     0.7266393  0.9803279  0.000
  Freund     2          95     0.7270492  0.9803279  0.000
  Freund     2          96     0.7266393  0.9803279  0.000
  Freund     2          97     0.7385246  0.9803279  0.000
  Freund     2          98     0.7213115  0.9836066  0.125
  Freund     2          99     0.7143443  0.9803279  0.125
  Freund     2         100     0.7143443  0.9836066  0.125
  Freund     3          20     0.6618852  0.9934426  0.125
  Freund     3          21     0.6606557  0.9901639  0.000
  Freund     3          22     0.7028689  0.9934426  0.250
  Freund     3          23     0.7061475  0.9934426  0.000
  Freund     3          24     0.7049180  0.9934426  0.000
  Freund     3          25     0.6926230  0.9967213  0.000
  Freund     3          26     0.7098361  0.9967213  0.000
  Freund     3          27     0.7120902  0.9934426  0.000
  Freund     3          28     0.7034836  0.9934426  0.125
  Freund     3          29     0.7010246  0.9934426  0.000
  Freund     3          30     0.6850410  0.9967213  0.125
  Freund     3          31     0.6887295  0.9934426  0.125
  Freund     3          32     0.6866803  0.9934426  0.000
  Freund     3          33     0.6973361  0.9901639  0.125
  Freund     3          34     0.6903689  0.9934426  0.000
  Freund     3          35     0.6879098  0.9901639  0.125
  Freund     3          36     0.6993852  0.9967213  0.125
  Freund     3          37     0.7063525  0.9967213  0.125
  Freund     3          38     0.7178279  0.9967213  0.000
  Freund     3          39     0.7137295  0.9967213  0.125
  Freund     3          40     0.7604508  0.9934426  0.000
  Freund     3          41     0.7276639  0.9967213  0.000
  Freund     3          42     0.6936475  0.9967213  0.000
  Freund     3          43     0.6838115  0.9967213  0.000
  Freund     3          44     0.6825820  0.9967213  0.000
  Freund     3          45     0.6899590  0.9967213  0.000
  Freund     3          46     0.6997951  0.9967213  0.000
  Freund     3          47     0.6993852  0.9967213  0.000
  Freund     3          48     0.6940574  0.9967213  0.000
  Freund     3          49     0.6920082  0.9967213  0.000
  Freund     3          50     0.6940574  0.9967213  0.000
  Freund     3          51     0.6969262  0.9967213  0.000
  Freund     3          52     0.7034836  0.9967213  0.000
  Freund     3          53     0.7030738  0.9967213  0.000
  Freund     3          54     0.7149590  0.9967213  0.000
  Freund     3          55     0.7272541  0.9967213  0.000
  Freund     3          56     0.7252049  0.9967213  0.000
  Freund     3          57     0.7108607  0.9967213  0.000
  Freund     3          58     0.7051230  0.9967213  0.000
  Freund     3          59     0.6956967  0.9967213  0.000
  Freund     3          60     0.6776639  0.9934426  0.000
  Freund     3          61     0.6866803  0.9934426  0.000
  Freund     3          62     0.6838115  0.9967213  0.000
  Freund     3          63     0.6825820  0.9967213  0.000
  Freund     3          64     0.6752049  0.9934426  0.000
  Freund     3          65     0.6719262  0.9967213  0.000
  Freund     3          66     0.6686475  0.9967213  0.000
  Freund     3          67     0.6727459  0.9967213  0.000
  Freund     3          68     0.6797131  0.9967213  0.000
  Freund     3          69     0.6776639  0.9967213  0.000
  Freund     3          70     0.6866803  0.9967213  0.000
  Freund     3          71     0.6752049  0.9967213  0.000
  Freund     3          72     0.6690574  0.9967213  0.000
  Freund     3          73     0.6538934  0.9967213  0.000
  Freund     3          74     0.6428279  0.9967213  0.000
  Freund     3          75     0.6448770  0.9967213  0.000
  Freund     3          76     0.6428279  0.9967213  0.000
  Freund     3          77     0.6465164  0.9967213  0.000
  Freund     3          78     0.6465164  0.9967213  0.000
  Freund     3          79     0.6440574  0.9967213  0.000
  Freund     3          80     0.6395492  0.9967213  0.000
  Freund     3          81     0.6366803  0.9934426  0.000
  Freund     3          82     0.6043033  0.9934426  0.000
  Freund     3          83     0.6071721  0.9934426  0.000
  Freund     3          84     0.6256148  0.9934426  0.000
  Freund     3          85     0.6354508  0.9934426  0.000
  Freund     3          86     0.6301230  0.9934426  0.000
  Freund     3          87     0.6366803  0.9934426  0.000
  Freund     3          88     0.6391393  0.9934426  0.000
  Freund     3          89     0.6387295  0.9934426  0.000
  Freund     3          90     0.6428279  0.9934426  0.000
  Freund     3          91     0.6415984  0.9934426  0.000
  Freund     3          92     0.6432377  0.9934426  0.000
  Freund     3          93     0.6432377  0.9934426  0.000
  Freund     3          94     0.6489754  0.9934426  0.000
  Freund     3          95     0.6694672  0.9934426  0.000
  Freund     3          96     0.6600410  0.9934426  0.000
  Freund     3          97     0.6760246  0.9934426  0.000
  Freund     3          98     0.6747951  0.9967213  0.000
  Freund     3          99     0.6747951  0.9967213  0.000
  Freund     3         100     0.6809426  0.9934426  0.000
  Freund     4          20     0.5286885  1.0000000  0.000
  Freund     4          21     0.5241803  0.9967213  0.000
  Freund     4          22     0.5497951  0.9967213  0.000
  Freund     4          23     0.5756148  0.9967213  0.125
  Freund     4          24     0.5698770  1.0000000  0.000
  Freund     4          25     0.5868852  1.0000000  0.125
  Freund     4          26     0.6198770  1.0000000  0.125
  Freund     4          27     0.6268443  1.0000000  0.125
  Freund     4          28     0.6391393  1.0000000  0.125
  Freund     4          29     0.6502049  1.0000000  0.125
  Freund     4          30     0.6530738  0.9967213  0.000
  Freund     4          31     0.6469262  1.0000000  0.125
  Freund     4          32     0.6106557  0.9967213  0.000
  Freund     4          33     0.6106557  1.0000000  0.125
  Freund     4          34     0.6397541  0.9967213  0.000
  Freund     4          35     0.6471311  1.0000000  0.000
  Freund     4          36     0.6586066  1.0000000  0.000
  Freund     4          37     0.6155738  0.9967213  0.000
  Freund     4          38     0.6081967  0.9967213  0.000
  Freund     4          39     0.6098361  0.9967213  0.000
  Freund     4          40     0.6028689  0.9967213  0.000
  Freund     4          41     0.6135246  0.9967213  0.000
  Freund     4          42     0.6110656  0.9967213  0.000
  Freund     4          43     0.6221311  0.9967213  0.000
  Freund     4          44     0.6286885  0.9934426  0.000
  Freund     4          45     0.6102459  0.9934426  0.000
  Freund     4          46     0.6237705  0.9934426  0.000
  Freund     4          47     0.6549180  0.9934426  0.000
  Freund     4          48     0.6545082  0.9934426  0.000
  Freund     4          49     0.6741803  0.9901639  0.000
  Freund     4          50     0.6635246  0.9901639  0.000
  Freund     4          51     0.6393443  0.9934426  0.000
  Freund     4          52     0.6467213  0.9901639  0.000
  Freund     4          53     0.6290984  0.9934426  0.000
  Freund     4          54     0.6274590  0.9934426  0.000
  Freund     4          55     0.6184426  0.9934426  0.000
  Freund     4          56     0.6319672  0.9967213  0.000
  Freund     4          57     0.6393443  0.9934426  0.000
  Freund     4          58     0.6418033  1.0000000  0.000
  Freund     4          59     0.6397541  0.9934426  0.000
  Freund     4          60     0.6877049  1.0000000  0.000
  Freund     4          61     0.7073770  0.9967213  0.000
  Freund     4          62     0.7114754  0.9967213  0.125
  Freund     4          63     0.7094262  0.9967213  0.000
  Freund     4          64     0.7053279  0.9967213  0.000
  Freund     4          65     0.6704918  0.9934426  0.000
  Freund     4          66     0.6663934  0.9934426  0.000
  Freund     4          67     0.6573770  0.9967213  0.000
  Freund     4          68     0.6983607  0.9967213  0.000
  Freund     4          69     0.6803279  0.9967213  0.000
  Freund     4          70     0.7000000  0.9934426  0.125
  Freund     4          71     0.6979508  0.9967213  0.125
  Freund     4          72     0.7024590  0.9934426  0.125
  Freund     4          73     0.7184426  1.0000000  0.125
  Freund     4          74     0.7110656  0.9934426  0.125
  Freund     4          75     0.6963115  1.0000000  0.000
  Freund     4          76     0.7036885  1.0000000  0.125
  Freund     4          77     0.7315574  1.0000000  0.000
  Freund     4          78     0.7209016  1.0000000  0.000
  Freund     4          79     0.7065574  1.0000000  0.000
  Freund     4          80     0.7213115  0.9967213  0.000
  Freund     4          81     0.7204918  1.0000000  0.000
  Freund     4          82     0.7168033  1.0000000  0.000
  Freund     4          83     0.7237705  1.0000000  0.000
  Freund     4          84     0.7209016  0.9967213  0.000
  Freund     4          85     0.7139344  1.0000000  0.000
  Freund     4          86     0.7192623  1.0000000  0.000
  Freund     4          87     0.7217213  1.0000000  0.000
  Freund     4          88     0.7237705  1.0000000  0.000
  Freund     4          89     0.7147541  0.9967213  0.000
  Freund     4          90     0.7196721  0.9967213  0.000
  Freund     4          91     0.7196721  0.9967213  0.000
  Freund     4          92     0.7168033  1.0000000  0.000
  Freund     4          93     0.7184426  1.0000000  0.000
  Freund     4          94     0.7192623  0.9967213  0.000
  Freund     4          95     0.7135246  0.9967213  0.000
  Freund     4          96     0.7270492  0.9967213  0.000
  Freund     4          97     0.7262295  0.9967213  0.000
  Freund     4          98     0.7250000  0.9967213  0.000
  Freund     4          99     0.7262295  1.0000000  0.000
  Freund     4         100     0.7245902  0.9967213  0.000
  Zhu        2          20     0.6668033  0.9508197  0.000
  Zhu        2          21     0.6586066  0.9573770  0.000
  Zhu        2          22     0.6602459  0.9606557  0.000
  Zhu        2          23     0.6565574  0.9639344  0.000
  Zhu        2          24     0.6323770  0.9639344  0.000
  Zhu        2          25     0.6672131  0.9540984  0.000
  Zhu        2          26     0.6877049  0.9639344  0.000
  Zhu        2          27     0.7397541  0.9573770  0.000
  Zhu        2          28     0.7172131  0.9704918  0.000
  Zhu        2          29     0.6971311  0.9704918  0.000
  Zhu        2          30     0.7254098  0.9737705  0.000
  Zhu        2          31     0.7254098  0.9704918  0.000
  Zhu        2          32     0.6959016  0.9573770  0.000
  Zhu        2          33     0.6909836  0.9606557  0.000
  Zhu        2          34     0.6827869  0.9639344  0.000
  Zhu        2          35     0.6536885  0.9606557  0.000
  Zhu        2          36     0.6500000  0.9639344  0.000
  Zhu        2          37     0.6520492  0.9639344  0.000
  Zhu        2          38     0.6356557  0.9639344  0.000
  Zhu        2          39     0.6413934  0.9606557  0.000
  Zhu        2          40     0.6413934  0.9672131  0.000
  Zhu        2          41     0.6270492  0.9606557  0.000
  Zhu        2          42     0.6163934  0.9606557  0.000
  Zhu        2          43     0.6348361  0.9704918  0.000
  Zhu        2          44     0.6258197  0.9639344  0.000
  Zhu        2          45     0.6213115  0.9639344  0.000
  Zhu        2          46     0.6348361  0.9704918  0.000
  Zhu        2          47     0.6348361  0.9704918  0.000
  Zhu        2          48     0.6155738  0.9672131  0.000
  Zhu        2          49     0.6504098  0.9737705  0.000
  Zhu        2          50     0.6413934  0.9606557  0.000
  Zhu        2          51     0.6413934  0.9737705  0.000
  Zhu        2          52     0.6426230  0.9672131  0.000
  Zhu        2          53     0.6581967  0.9737705  0.000
  Zhu        2          54     0.6389344  0.9770492  0.000
  Zhu        2          55     0.6495902  0.9672131  0.000
  Zhu        2          56     0.6565574  0.9770492  0.000
  Zhu        2          57     0.6676230  0.9770492  0.000
  Zhu        2          58     0.6655738  0.9803279  0.000
  Zhu        2          59     0.6471311  0.9803279  0.000
  Zhu        2          60     0.6463115  0.9803279  0.000
  Zhu        2          61     0.6446721  0.9737705  0.000
  Zhu        2          62     0.6442623  0.9737705  0.000
  Zhu        2          63     0.6454918  0.9737705  0.000
  Zhu        2          64     0.6438525  0.9737705  0.000
  Zhu        2          65     0.6405738  0.9770492  0.000
  Zhu        2          66     0.6405738  0.9770492  0.000
  Zhu        2          67     0.6405738  0.9737705  0.000
  Zhu        2          68     0.6405738  0.9770492  0.000
  Zhu        2          69     0.6372951  0.9737705  0.000
  Zhu        2          70     0.6413934  0.9770492  0.000
  Zhu        2          71     0.6471311  0.9770492  0.000
  Zhu        2          72     0.6471311  0.9770492  0.000
  Zhu        2          73     0.6590164  0.9770492  0.000
  Zhu        2          74     0.6622951  0.9770492  0.000
  Zhu        2          75     0.6622951  0.9770492  0.000
  Zhu        2          76     0.6659836  0.9737705  0.000
  Zhu        2          77     0.6659836  0.9803279  0.000
  Zhu        2          78     0.6659836  0.9737705  0.000
  Zhu        2          79     0.6614754  0.9803279  0.000
  Zhu        2          80     0.6709016  0.9770492  0.000
  Zhu        2          81     0.6704918  0.9803279  0.000
  Zhu        2          82     0.6762295  0.9770492  0.000
  Zhu        2          83     0.6754098  0.9770492  0.000
  Zhu        2          84     0.6750000  0.9803279  0.000
  Zhu        2          85     0.6729508  0.9836066  0.000
  Zhu        2          86     0.6819672  0.9803279  0.000
  Zhu        2          87     0.6733607  0.9836066  0.000
  Zhu        2          88     0.6918033  0.9803279  0.000
  Zhu        2          89     0.6967213  0.9836066  0.000
  Zhu        2          90     0.6971311  0.9836066  0.000
  Zhu        2          91     0.7102459  0.9836066  0.000
  Zhu        2          92     0.7086066  0.9836066  0.000
  Zhu        2          93     0.6987705  0.9836066  0.000
  Zhu        2          94     0.6950820  0.9836066  0.000
  Zhu        2          95     0.6782787  0.9836066  0.000
  Zhu        2          96     0.6782787  0.9836066  0.000
  Zhu        2          97     0.6651639  0.9836066  0.000
  Zhu        2          98     0.6672131  0.9836066  0.000
  Zhu        2          99     0.6647541  0.9836066  0.000
  Zhu        2         100     0.6647541  0.9836066  0.000
  Zhu        3          20     0.6370902  0.9868852  0.000
  Zhu        3          21     0.6290984  0.9868852  0.000
  Zhu        3          22     0.6684426  0.9836066  0.000
  Zhu        3          23     0.6565574  0.9836066  0.000
  Zhu        3          24     0.6815574  0.9868852  0.000
  Zhu        3          25     0.6680328  0.9901639  0.000
  Zhu        3          26     0.6655738  0.9934426  0.000
  Zhu        3          27     0.6254098  0.9868852  0.000
  Zhu        3          28     0.6245902  0.9868852  0.000
  Zhu        3          29     0.5909836  0.9836066  0.000
  Zhu        3          30     0.6204918  0.9868852  0.000
  Zhu        3          31     0.6032787  0.9868852  0.000
  Zhu        3          32     0.6258197  0.9868852  0.000
  Zhu        3          33     0.6598361  0.9803279  0.000
  Zhu        3          34     0.6573770  0.9901639  0.000
  Zhu        3          35     0.6561475  0.9934426  0.000
  Zhu        3          36     0.6639344  0.9901639  0.000
  Zhu        3          37     0.7053279  0.9868852  0.000
  Zhu        3          38     0.7159836  0.9901639  0.000
  Zhu        3          39     0.7139344  0.9868852  0.000
  Zhu        3          40     0.7122951  0.9868852  0.000
  Zhu        3          41     0.7127049  0.9868852  0.000
  Zhu        3          42     0.7098361  0.9901639  0.000
  Zhu        3          43     0.7061475  0.9901639  0.000
  Zhu        3          44     0.7061475  0.9901639  0.000
  Zhu        3          45     0.6905738  0.9901639  0.000
  Zhu        3          46     0.7077869  0.9901639  0.000
  Zhu        3          47     0.6987705  0.9901639  0.000
  Zhu        3          48     0.6946721  0.9901639  0.000
  Zhu        3          49     0.6487705  0.9934426  0.000
  Zhu        3          50     0.6487705  0.9934426  0.000
  Zhu        3          51     0.6672131  0.9934426  0.000
  Zhu        3          52     0.6622951  0.9868852  0.000
  Zhu        3          53     0.6594262  0.9868852  0.000
  Zhu        3          54     0.6905738  0.9868852  0.000
  Zhu        3          55     0.6954918  0.9868852  0.125
  Zhu        3          56     0.6827869  0.9868852  0.000
  Zhu        3          57     0.6655738  0.9868852  0.125
  Zhu        3          58     0.6745902  0.9901639  0.000
  Zhu        3          59     0.6647541  0.9868852  0.000
  Zhu        3          60     0.6639344  0.9901639  0.000
  Zhu        3          61     0.6614754  0.9868852  0.000
  Zhu        3          62     0.6610656  0.9901639  0.000
  Zhu        3          63     0.6790984  0.9901639  0.000
  Zhu        3          64     0.6750000  0.9901639  0.000
  Zhu        3          65     0.6930328  0.9934426  0.000
  Zhu        3          66     0.6618852  0.9868852  0.000
  Zhu        3          67     0.6627049  0.9901639  0.000
  Zhu        3          68     0.6536885  0.9901639  0.000
  Zhu        3          69     0.6577869  0.9901639  0.000
  Zhu        3          70     0.6590164  0.9868852  0.000
  Zhu        3          71     0.6553279  0.9901639  0.000
  Zhu        3          72     0.6409836  0.9868852  0.000
  Zhu        3          73     0.6590164  0.9868852  0.000
  Zhu        3          74     0.6610656  0.9868852  0.000
  Zhu        3          75     0.6586066  0.9868852  0.000
  Zhu        3          76     0.6372951  0.9868852  0.000
  Zhu        3          77     0.6463115  0.9901639  0.000
  Zhu        3          78     0.6463115  0.9901639  0.000
  Zhu        3          79     0.6565574  0.9868852  0.000
  Zhu        3          80     0.6606557  0.9901639  0.000
  Zhu        3          81     0.6594262  0.9934426  0.000
  Zhu        3          82     0.6340164  0.9934426  0.000
  Zhu        3          83     0.6336066  0.9934426  0.000
  Zhu        3          84     0.6467213  0.9934426  0.000
  Zhu        3          85     0.6434426  0.9967213  0.000
  Zhu        3          86     0.6545082  0.9934426  0.000
  Zhu        3          87     0.6434426  0.9901639  0.000
  Zhu        3          88     0.6434426  0.9934426  0.000
  Zhu        3          89     0.6545082  0.9934426  0.000
  Zhu        3          90     0.6471311  0.9934426  0.000
  Zhu        3          91     0.6467213  0.9934426  0.000
  Zhu        3          92     0.6483607  0.9934426  0.000
  Zhu        3          93     0.6540984  0.9934426  0.000
  Zhu        3          94     0.6397541  0.9934426  0.000
  Zhu        3          95     0.6680328  0.9934426  0.000
  Zhu        3          96     0.6836066  0.9934426  0.000
  Zhu        3          97     0.6823770  0.9934426  0.000
  Zhu        3          98     0.6823770  0.9967213  0.000
  Zhu        3          99     0.6631148  0.9934426  0.000
  Zhu        3         100     0.6631148  0.9967213  0.000
  Zhu        4          20     0.7850410  0.9868852  0.125
  Zhu        4          21     0.7864754  0.9868852  0.125
  Zhu        4          22     0.7659836  0.9868852  0.125
  Zhu        4          23     0.7618852  0.9868852  0.000
  Zhu        4          24     0.7286885  0.9868852  0.000
  Zhu        4          25     0.7155738  0.9901639  0.000
  Zhu        4          26     0.6922131  0.9901639  0.000
  Zhu        4          27     0.6823770  0.9901639  0.000
  Zhu        4          28     0.6827869  0.9901639  0.000
  Zhu        4          29     0.6717213  0.9901639  0.000
  Zhu        4          30     0.6745902  0.9901639  0.000
  Zhu        4          31     0.6971311  0.9901639  0.000
  Zhu        4          32     0.6967213  0.9901639  0.000
  Zhu        4          33     0.6610656  0.9934426  0.000
  Zhu        4          34     0.6897541  0.9901639  0.000
  Zhu        4          35     0.6856557  0.9901639  0.000
  Zhu        4          36     0.6922131  0.9901639  0.000
  Zhu        4          37     0.6938525  0.9934426  0.000
  Zhu        4          38     0.6909836  0.9901639  0.000
  Zhu        4          39     0.6905738  0.9934426  0.000
  Zhu        4          40     0.7028689  0.9934426  0.000
  Zhu        4          41     0.6967213  0.9967213  0.000
  Zhu        4          42     0.6967213  0.9934426  0.000
  Zhu        4          43     0.7139344  0.9967213  0.000
  Zhu        4          44     0.7282787  0.9967213  0.000
  Zhu        4          45     0.7258197  0.9967213  0.000
  Zhu        4          46     0.7262295  0.9967213  0.000
  Zhu        4          47     0.7221311  0.9967213  0.000
  Zhu        4          48     0.7180328  0.9934426  0.000
  Zhu        4          49     0.7049180  0.9934426  0.000
  Zhu        4          50     0.7040984  0.9934426  0.000
  Zhu        4          51     0.7446721  0.9934426  0.000
  Zhu        4          52     0.7491803  0.9934426  0.000
  Zhu        4          53     0.7340164  0.9934426  0.000
  Zhu        4          54     0.7323770  0.9934426  0.000
  Zhu        4          55     0.7286885  0.9934426  0.000
  Zhu        4          56     0.7331967  0.9934426  0.000
  Zhu        4          57     0.7450820  0.9934426  0.000
  Zhu        4          58     0.7241803  0.9934426  0.000
  Zhu        4          59     0.7258197  0.9934426  0.000
  Zhu        4          60     0.7405738  0.9934426  0.000
  Zhu        4          61     0.7315574  0.9934426  0.000
  Zhu        4          62     0.7278689  0.9934426  0.000
  Zhu        4          63     0.7217213  0.9934426  0.000
  Zhu        4          64     0.7196721  0.9934426  0.000
  Zhu        4          65     0.7250000  0.9934426  0.000
  Zhu        4          66     0.7127049  0.9934426  0.000
  Zhu        4          67     0.7098361  0.9934426  0.000
  Zhu        4          68     0.7143443  0.9934426  0.000
  Zhu        4          69     0.7336066  0.9934426  0.000
  Zhu        4          70     0.7237705  0.9934426  0.000
  Zhu        4          71     0.7233607  0.9967213  0.000
  Zhu        4          72     0.7241803  0.9934426  0.000
  Zhu        4          73     0.7204918  0.9967213  0.000
  Zhu        4          74     0.7135246  0.9967213  0.000
  Zhu        4          75     0.7094262  0.9967213  0.000
  Zhu        4          76     0.7094262  0.9967213  0.000
  Zhu        4          77     0.7008197  0.9967213  0.000
  Zhu        4          78     0.6983607  0.9967213  0.000
  Zhu        4          79     0.6901639  0.9967213  0.000
  Zhu        4          80     0.7057377  0.9967213  0.000
  Zhu        4          81     0.7061475  0.9967213  0.000
  Zhu        4          82     0.6987705  0.9967213  0.000
  Zhu        4          83     0.6926230  0.9967213  0.000
  Zhu        4          84     0.6983607  0.9967213  0.000
  Zhu        4          85     0.6934426  0.9967213  0.000
  Zhu        4          86     0.7049180  0.9967213  0.000
  Zhu        4          87     0.6950820  0.9967213  0.000
  Zhu        4          88     0.7053279  0.9967213  0.000
  Zhu        4          89     0.7024590  0.9967213  0.000
  Zhu        4          90     0.6946721  0.9967213  0.000
  Zhu        4          91     0.6946721  0.9967213  0.000
  Zhu        4          92     0.6926230  0.9967213  0.000
  Zhu        4          93     0.6905738  0.9967213  0.000
  Zhu        4          94     0.6852459  0.9967213  0.000
  Zhu        4          95     0.6844262  0.9967213  0.000
  Zhu        4          96     0.6913934  0.9967213  0.000
  Zhu        4          97     0.6901639  0.9967213  0.000
  Zhu        4          98     0.6856557  0.9967213  0.000
  Zhu        4          99     0.6938525  0.9967213  0.000
  Zhu        4         100     0.6905738  0.9967213  0.000

ROC was used to select the optimal model using the largest value.
The final values used for the model were mfinal = 21, maxdepth = 2
 and coeflearn = Freund.

    \end{Verbatim}

    
    \begin{verbatim}
Bootstrapped (1 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   No  Yes
       No  91.7  2.2
       Yes  5.8  0.3
                            
 Accuracy (average) : 0.9201

    \end{verbatim}

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_91_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Confusion Matrix for adaboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} caretPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}ada\PYZus{}up\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{raw\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}caretPredictedClass\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  343   7
       Yes  17   4
                                          
               Accuracy : 0.9353          
                 95% CI : (0.9053, 0.9581)
    No Information Rate : 0.9704          
    P-Value [Acc > NIR] : 0.99985         
                                          
                  Kappa : 0.2196          
 Mcnemar's Test P-Value : 0.06619         
                                          
            Sensitivity : 0.9528          
            Specificity : 0.3636          
         Pos Pred Value : 0.9800          
         Neg Pred Value : 0.1905          
             Prevalence : 0.9704          
         Detection Rate : 0.9245          
   Detection Prevalence : 0.9434          
      Balanced Accuracy : 0.6582          
                                          
       'Positive' Class : No              
                                          
    \end{verbatim}

    
    ROC plot for adaboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} ada\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}ada\PYZus{}up\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         ada\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}ada\PYZus{}pred\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         ada\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}ada\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}ada\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for adaboost with upsample\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}ada\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.7568182


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_95_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{boosting-with-adaboost-down-sample}{%
\subsubsection{Boosting with adaboost (down
sample)}\label{boosting-with-adaboost-down-sample}}

The below code chunk sets some of the control parameters for adaboost

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} objControl \PY{o}{\PYZlt{}\PYZhy{}} trainControl\PY{p}{(}method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{boot\PYZsq{}}\PY{p}{,} number \PY{o}{=} \PY{l+m}{1}\PY{p}{,}
                                    returnResamp\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{all\PYZsq{}}\PY{p}{,}
                                    summaryFunction \PY{o}{=} twoClassSummary\PY{p}{,}
                                    savePredictions \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    classProbs \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    sampling \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{down\PYZdq{}}\PY{p}{)}\PY{c+c1}{\PYZsh{}, p = 0.70) \PYZsh{}in case method = \PYZsh{}\PYZdq{}LGO\PYZdq{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} search\PYZus{}grid \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{expand.grid}\PY{p}{(}mfinal \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{20}\PY{o}{:}\PY{l+m}{100}\PY{p}{)}\PY{p}{,} maxdepth \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{2}\PY{o}{:}\PY{l+m}{4}\PY{p}{)}\PY{p}{,}
                             coeflearn \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Breiman\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{Freund\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{Zhu\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    After setting the control paramters, the model is run

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} num\PYZus{}cores \PY{o}{\PYZlt{}\PYZhy{}} makeCluster\PY{p}{(}detectCores\PY{p}{(}\PY{p}{)}\PY{l+m}{\PYZhy{}5}\PY{p}{)}
         registerDoParallel\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         tic\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Adaptive Boosting with Down Sample\PYZdq{}}\PY{p}{)}
         
         \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         ada\PYZus{}down.model \PY{o}{\PYZlt{}\PYZhy{}} train\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{,}
                           method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{AdaBoost.M1\PYZsq{}}\PY{p}{,}
                           trControl\PY{o}{=}objControl\PY{p}{,}
                           tuneGrid \PY{o}{=} search\PYZus{}grid\PY{p}{,}
                           metric \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{ROC\PYZdq{}}\PY{p}{)}
         stopCluster\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         toc\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Adaptive Boosting with Down Sample: 137.311 sec elapsed

    \end{Verbatim}

    Confusion Matrix for adaboost on train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{c+c1}{\PYZsh{}ada\PYZus{}down.model\PYZdl{}finalModel \PYZsh{}ada\PYZus{}down.model\PYZdl{}results}
         \PY{k+kp}{print}\PY{p}{(}ada\PYZus{}down.model\PY{p}{)}
         confusionMatrix.train\PY{p}{(}ada\PYZus{}down.model\PY{p}{)}
         plot\PY{p}{(}varImp\PY{p}{(}ada\PYZus{}down.model\PY{p}{)}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable importance from Adaboost with down sample\PYZdq{}}\PY{p}{,} col \PY{o}{=} \PY{l+m}{2}\PY{p}{,} lwd \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
AdaBoost.M1 

868 samples
  5 predictor
  2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Bootstrapped (1 reps) 
Summary of sample sizes: 868 
Addtional sampling using down-sampling

Resampling results across tuning parameters:

  coeflearn  maxdepth  mfinal  ROC        Sens       Spec 
  Breiman    2          20     0.6885246  0.7770492  0.375
  Breiman    2          21     0.6721311  0.7737705  0.375
  Breiman    2          22     0.6817623  0.7836066  0.375
  Breiman    2          23     0.6764344  0.7836066  0.375
  Breiman    2          24     0.6362705  0.7540984  0.375
  Breiman    2          25     0.6350410  0.7836066  0.250
  Breiman    2          26     0.6399590  0.7803279  0.250
  Breiman    2          27     0.6301230  0.7803279  0.250
  Breiman    2          28     0.6354508  0.7868852  0.250
  Breiman    2          29     0.6481557  0.8032787  0.250
  Breiman    2          30     0.6625000  0.7967213  0.250
  Breiman    2          31     0.6760246  0.8000000  0.250
  Breiman    2          32     0.6782787  0.8295082  0.250
  Breiman    2          33     0.6704918  0.7868852  0.250
  Breiman    2          34     0.6536885  0.8000000  0.250
  Breiman    2          35     0.6602459  0.8131148  0.250
  Breiman    2          36     0.6635246  0.8163934  0.250
  Breiman    2          37     0.6827869  0.7901639  0.250
  Breiman    2          38     0.6848361  0.7934426  0.250
  Breiman    2          39     0.6549180  0.7737705  0.250
  Breiman    2          40     0.6807377  0.7967213  0.250
  Breiman    2          41     0.6823770  0.7803279  0.250
  Breiman    2          42     0.6819672  0.7737705  0.250
  Breiman    2          43     0.6918033  0.7868852  0.250
  Breiman    2          44     0.6860656  0.7967213  0.250
  Breiman    2          45     0.6795082  0.8000000  0.250
  Breiman    2          46     0.6795082  0.7901639  0.250
  Breiman    2          47     0.6983607  0.7770492  0.250
  Breiman    2          48     0.7053279  0.7704918  0.250
  Breiman    2          49     0.7098361  0.7770492  0.250
  Breiman    2          50     0.7106557  0.7836066  0.250
  Breiman    2          51     0.6889344  0.7836066  0.250
  Breiman    2          52     0.6868852  0.7803279  0.250
  Breiman    2          53     0.6766393  0.7868852  0.250
  Breiman    2          54     0.6803279  0.8000000  0.250
  Breiman    2          55     0.6795082  0.7836066  0.250
  Breiman    2          56     0.6795082  0.7803279  0.250
  Breiman    2          57     0.6938525  0.7868852  0.250
  Breiman    2          58     0.6840164  0.7901639  0.250
  Breiman    2          59     0.7000000  0.7704918  0.250
  Breiman    2          60     0.6897541  0.7672131  0.250
  Breiman    2          61     0.6840164  0.7704918  0.250
  Breiman    2          62     0.6918033  0.7672131  0.250
  Breiman    2          63     0.6950820  0.7737705  0.250
  Breiman    2          64     0.6938525  0.7868852  0.250
  Breiman    2          65     0.7131148  0.7836066  0.250
  Breiman    2          66     0.7073770  0.7836066  0.250
  Breiman    2          67     0.7086066  0.7672131  0.250
  Breiman    2          68     0.7024590  0.7672131  0.250
  Breiman    2          69     0.7012295  0.7639344  0.250
  Breiman    2          70     0.6971311  0.7737705  0.250
  Breiman    2          71     0.6913934  0.7704918  0.250
  Breiman    2          72     0.6930328  0.7868852  0.250
  Breiman    2          73     0.6975410  0.7672131  0.250
  Breiman    2          74     0.7061475  0.7934426  0.375
  Breiman    2          75     0.7053279  0.7901639  0.375
  Breiman    2          76     0.6983607  0.8000000  0.375
  Breiman    2          77     0.7086066  0.8229508  0.375
  Breiman    2          78     0.7094262  0.8295082  0.375
  Breiman    2          79     0.7110656  0.8196721  0.375
  Breiman    2          80     0.7020492  0.7934426  0.375
  Breiman    2          81     0.7090164  0.7901639  0.375
  Breiman    2          82     0.6959016  0.7868852  0.375
  Breiman    2          83     0.7049180  0.7737705  0.375
  Breiman    2          84     0.7090164  0.7770492  0.375
  Breiman    2          85     0.7086066  0.7803279  0.375
  Breiman    2          86     0.7295082  0.7934426  0.375
  Breiman    2          87     0.7196721  0.7934426  0.375
  Breiman    2          88     0.7049180  0.7967213  0.375
  Breiman    2          89     0.7065574  0.7967213  0.375
  Breiman    2          90     0.7131148  0.7934426  0.375
  Breiman    2          91     0.7098361  0.7901639  0.375
  Breiman    2          92     0.7012295  0.7868852  0.375
  Breiman    2          93     0.7040984  0.7836066  0.375
  Breiman    2          94     0.7036885  0.7868852  0.375
  Breiman    2          95     0.6995902  0.7836066  0.250
  Breiman    2          96     0.6979508  0.7803279  0.250
  Breiman    2          97     0.6959016  0.7836066  0.250
  Breiman    2          98     0.6954918  0.7868852  0.250
  Breiman    2          99     0.7024590  0.7868852  0.250
  Breiman    2         100     0.6987705  0.7901639  0.375
  Breiman    3          20     0.6106557  0.7770492  0.125
  Breiman    3          21     0.6061475  0.7803279  0.125
  Breiman    3          22     0.6000000  0.7770492  0.250
  Breiman    3          23     0.6491803  0.7770492  0.250
  Breiman    3          24     0.6446721  0.7901639  0.250
  Breiman    3          25     0.6299180  0.7868852  0.250
  Breiman    3          26     0.6475410  0.7934426  0.250
  Breiman    3          27     0.6553279  0.7770492  0.250
  Breiman    3          28     0.6635246  0.7803279  0.375
  Breiman    3          29     0.6545082  0.7967213  0.375
  Breiman    3          30     0.6540984  0.7934426  0.375
  Breiman    3          31     0.6745902  0.8032787  0.375
  Breiman    3          32     0.6811475  0.7836066  0.375
  Breiman    3          33     0.6627049  0.7803279  0.375
  Breiman    3          34     0.6397541  0.7770492  0.375
  Breiman    3          35     0.6770492  0.7836066  0.375
  Breiman    3          36     0.6823770  0.7770492  0.375
  Breiman    3          37     0.6704918  0.7737705  0.250
  Breiman    3          38     0.6602459  0.7737705  0.250
  Breiman    3          39     0.6594262  0.7803279  0.250
  Breiman    3          40     0.6590164  0.7803279  0.250
  Breiman    3          41     0.6610656  0.7967213  0.250
  Breiman    3          42     0.6479508  0.7934426  0.250
  Breiman    3          43     0.6672131  0.7836066  0.250
  Breiman    3          44     0.6668033  0.7901639  0.250
  Breiman    3          45     0.6737705  0.7901639  0.250
  Breiman    3          46     0.6938525  0.8000000  0.250
  Breiman    3          47     0.6995902  0.7967213  0.375
  Breiman    3          48     0.6860656  0.8032787  0.250
  Breiman    3          49     0.6823770  0.7901639  0.375
  Breiman    3          50     0.6692623  0.7901639  0.250
  Breiman    3          51     0.6688525  0.7901639  0.250
  Breiman    3          52     0.6672131  0.7868852  0.250
  Breiman    3          53     0.6668033  0.7934426  0.250
  Breiman    3          54     0.6766393  0.7934426  0.250
  Breiman    3          55     0.6823770  0.8000000  0.250
  Breiman    3          56     0.6770492  0.8032787  0.250
  Breiman    3          57     0.6786885  0.7901639  0.250
  Breiman    3          58     0.6782787  0.8000000  0.125
  Breiman    3          59     0.6754098  0.7868852  0.125
  Breiman    3          60     0.6659836  0.7836066  0.375
  Breiman    3          61     0.6590164  0.7803279  0.375
  Breiman    3          62     0.6479508  0.7639344  0.250
  Breiman    3          63     0.6565574  0.7803279  0.250
  Breiman    3          64     0.6565574  0.7803279  0.250
  Breiman    3          65     0.6512295  0.7770492  0.250
  Breiman    3          66     0.6549180  0.7639344  0.375
  Breiman    3          67     0.6418033  0.7606557  0.250
  Breiman    3          68     0.6475410  0.7639344  0.250
  Breiman    3          69     0.6348361  0.7803279  0.250
  Breiman    3          70     0.6254098  0.7639344  0.250
  Breiman    3          71     0.6262295  0.7770492  0.250
  Breiman    3          72     0.6344262  0.7770492  0.250
  Breiman    3          73     0.6418033  0.7737705  0.250
  Breiman    3          74     0.6504098  0.7770492  0.250
  Breiman    3          75     0.6495902  0.7901639  0.250
  Breiman    3          76     0.6463115  0.7737705  0.250
  Breiman    3          77     0.6360656  0.7672131  0.250
  Breiman    3          78     0.6377049  0.7639344  0.250
  Breiman    3          79     0.6344262  0.7737705  0.250
  Breiman    3          80     0.6327869  0.7704918  0.250
  Breiman    3          81     0.6315574  0.7737705  0.250
  Breiman    3          82     0.6340164  0.7737705  0.250
  Breiman    3          83     0.6307377  0.7836066  0.250
  Breiman    3          84     0.6315574  0.7836066  0.250
  Breiman    3          85     0.6323770  0.7868852  0.250
  Breiman    3          86     0.6319672  0.7868852  0.250
  Breiman    3          87     0.6200820  0.7901639  0.250
  Breiman    3          88     0.6254098  0.7901639  0.250
  Breiman    3          89     0.6233607  0.7836066  0.250
  Breiman    3          90     0.6196721  0.7737705  0.250
  Breiman    3          91     0.6250000  0.7737705  0.250
  Breiman    3          92     0.6168033  0.7704918  0.250
  Breiman    3          93     0.6135246  0.7836066  0.250
  Breiman    3          94     0.6086066  0.7737705  0.250
  Breiman    3          95     0.6040984  0.7737705  0.250
  Breiman    3          96     0.6172131  0.7704918  0.250
  Breiman    3          97     0.6127049  0.7704918  0.250
  Breiman    3          98     0.6081967  0.7737705  0.250
  Breiman    3          99     0.6340164  0.7737705  0.250
  Breiman    3         100     0.6336066  0.7836066  0.250
  Breiman    4          20     0.4651639  0.7344262  0.250
  Breiman    4          21     0.4934426  0.7442623  0.125
  Breiman    4          22     0.4948770  0.7475410  0.125
  Breiman    4          23     0.4801230  0.7508197  0.125
  Breiman    4          24     0.5084016  0.7344262  0.125
  Breiman    4          25     0.5252049  0.7606557  0.125
  Breiman    4          26     0.5186475  0.7606557  0.125
  Breiman    4          27     0.5317623  0.7803279  0.125
  Breiman    4          28     0.5540984  0.7573770  0.250
  Breiman    4          29     0.5524590  0.7803279  0.125
  Breiman    4          30     0.5573770  0.7606557  0.250
  Breiman    4          31     0.5762295  0.7704918  0.250
  Breiman    4          32     0.5889344  0.7639344  0.250
  Breiman    4          33     0.5782787  0.7475410  0.125
  Breiman    4          34     0.5807377  0.7606557  0.125
  Breiman    4          35     0.5950820  0.7573770  0.125
  Breiman    4          36     0.6045082  0.7475410  0.250
  Breiman    4          37     0.6024590  0.7540984  0.125
  Breiman    4          38     0.5971311  0.7508197  0.125
  Breiman    4          39     0.6000000  0.7639344  0.125
  Breiman    4          40     0.6020492  0.7672131  0.375
  Breiman    4          41     0.6024590  0.7639344  0.125
  Breiman    4          42     0.6004098  0.7737705  0.250
  Breiman    4          43     0.6040984  0.7639344  0.250
  Breiman    4          44     0.6028689  0.7639344  0.250
  Breiman    4          45     0.5959016  0.7606557  0.125
  Breiman    4          46     0.5954918  0.7737705  0.375
  Breiman    4          47     0.6020492  0.7704918  0.125
  Breiman    4          48     0.6032787  0.7672131  0.125
  Breiman    4          49     0.5983607  0.7704918  0.125
  Breiman    4          50     0.6008197  0.7704918  0.375
  Breiman    4          51     0.6000000  0.7704918  0.375
  Breiman    4          52     0.5963115  0.7737705  0.375
  Breiman    4          53     0.5922131  0.7737705  0.250
  Breiman    4          54     0.6004098  0.7770492  0.375
  Breiman    4          55     0.6073770  0.7770492  0.250
  Breiman    4          56     0.6151639  0.7672131  0.375
  Breiman    4          57     0.6245902  0.7704918  0.250
  Breiman    4          58     0.6250000  0.7704918  0.250
  Breiman    4          59     0.6254098  0.7639344  0.250
  Breiman    4          60     0.6221311  0.7672131  0.250
  Breiman    4          61     0.6196721  0.7737705  0.250
  Breiman    4          62     0.6127049  0.7803279  0.250
  Breiman    4          63     0.6135246  0.7770492  0.250
  Breiman    4          64     0.6155738  0.7737705  0.250
  Breiman    4          65     0.6094262  0.7770492  0.250
  Breiman    4          66     0.6180328  0.7803279  0.250
  Breiman    4          67     0.6225410  0.7737705  0.375
  Breiman    4          68     0.6188525  0.7704918  0.375
  Breiman    4          69     0.6225410  0.7704918  0.375
  Breiman    4          70     0.6237705  0.7836066  0.375
  Breiman    4          71     0.6295082  0.7737705  0.375
  Breiman    4          72     0.6307377  0.7836066  0.250
  Breiman    4          73     0.6258197  0.7737705  0.375
  Breiman    4          74     0.6139344  0.7803279  0.250
  Breiman    4          75     0.6147541  0.7737705  0.250
  Breiman    4          76     0.6155738  0.7770492  0.250
  Breiman    4          77     0.6127049  0.7704918  0.250
  Breiman    4          78     0.6188525  0.7803279  0.250
  Breiman    4          79     0.6163934  0.7836066  0.250
  Breiman    4          80     0.6217213  0.7803279  0.250
  Breiman    4          81     0.6176230  0.7770492  0.250
  Breiman    4          82     0.6204918  0.7770492  0.250
  Breiman    4          83     0.6204918  0.7770492  0.250
  Breiman    4          84     0.6196721  0.7770492  0.250
  Breiman    4          85     0.6168033  0.7770492  0.250
  Breiman    4          86     0.6180328  0.7770492  0.250
  Breiman    4          87     0.6209016  0.7868852  0.250
  Breiman    4          88     0.6155738  0.7770492  0.250
  Breiman    4          89     0.6159836  0.7836066  0.250
  Breiman    4          90     0.6168033  0.7836066  0.250
  Breiman    4          91     0.6176230  0.7803279  0.250
  Breiman    4          92     0.6143443  0.7803279  0.250
  Breiman    4          93     0.6118852  0.7836066  0.250
  Breiman    4          94     0.6086066  0.7803279  0.250
  Breiman    4          95     0.6045082  0.7737705  0.125
  Breiman    4          96     0.6049180  0.7737705  0.250
  Breiman    4          97     0.6094262  0.7770492  0.250
  Breiman    4          98     0.6110656  0.7770492  0.250
  Breiman    4          99     0.6122951  0.7836066  0.250
  Breiman    4         100     0.6147541  0.7868852  0.250
  Freund     2          20     0.5590164  0.6754098  0.375
  Freund     2          21     0.5520492  0.7377049  0.250
  Freund     2          22     0.5418033  0.7081967  0.375
  Freund     2          23     0.5401639  0.7278689  0.375
  Freund     2          24     0.5577869  0.7180328  0.375
  Freund     2          25     0.5344262  0.7016393  0.375
  Freund     2          26     0.5139344  0.7114754  0.375
  Freund     2          27     0.5073770  0.7016393  0.250
  Freund     2          28     0.5069672  0.7180328  0.250
  Freund     2          29     0.4987705  0.6918033  0.375
  Freund     2          30     0.5159836  0.7016393  0.250
  Freund     2          31     0.5073770  0.7016393  0.250
  Freund     2          32     0.4795082  0.6950820  0.250
  Freund     2          33     0.4881148  0.7049180  0.250
  Freund     2          34     0.5040984  0.7147541  0.250
  Freund     2          35     0.5020492  0.7278689  0.250
  Freund     2          36     0.5114754  0.7377049  0.250
  Freund     2          37     0.5118852  0.7245902  0.250
  Freund     2          38     0.5163934  0.7213115  0.250
  Freund     2          39     0.5397541  0.7180328  0.375
  Freund     2          40     0.5233607  0.7081967  0.375
  Freund     2          41     0.5295082  0.7213115  0.250
  Freund     2          42     0.5336066  0.7147541  0.375
  Freund     2          43     0.5381148  0.7180328  0.375
  Freund     2          44     0.5217213  0.7049180  0.375
  Freund     2          45     0.5065574  0.7049180  0.375
  Freund     2          46     0.5209016  0.7114754  0.375
  Freund     2          47     0.5016393  0.6983607  0.375
  Freund     2          48     0.5557377  0.7114754  0.250
  Freund     2          49     0.5614754  0.7049180  0.250
  Freund     2          50     0.5639344  0.7049180  0.250
  Freund     2          51     0.5573770  0.7081967  0.375
  Freund     2          52     0.5627049  0.7016393  0.375
  Freund     2          53     0.5815574  0.6950820  0.375
  Freund     2          54     0.5770492  0.7180328  0.375
  Freund     2          55     0.5586066  0.7081967  0.250
  Freund     2          56     0.5655738  0.6983607  0.375
  Freund     2          57     0.5692623  0.7114754  0.250
  Freund     2          58     0.5790984  0.7147541  0.250
  Freund     2          59     0.5778689  0.7344262  0.250
  Freund     2          60     0.5741803  0.7245902  0.250
  Freund     2          61     0.5745902  0.7245902  0.250
  Freund     2          62     0.5942623  0.7147541  0.250
  Freund     2          63     0.6065574  0.7278689  0.250
  Freund     2          64     0.5881148  0.7311475  0.250
  Freund     2          65     0.5913934  0.7278689  0.250
  Freund     2          66     0.5877049  0.7147541  0.250
  Freund     2          67     0.5864754  0.7245902  0.250
  Freund     2          68     0.5856557  0.7311475  0.250
  Freund     2          69     0.5815574  0.7377049  0.250
  Freund     2          70     0.5889344  0.7344262  0.250
  Freund     2          71     0.5827869  0.7278689  0.250
  Freund     2          72     0.5668033  0.7147541  0.250
  Freund     2          73     0.5733607  0.7245902  0.250
  Freund     2          74     0.5762295  0.7180328  0.250
  Freund     2          75     0.5782787  0.7278689  0.250
  Freund     2          76     0.5901639  0.7377049  0.250
  Freund     2          77     0.5991803  0.7344262  0.250
  Freund     2          78     0.5905738  0.7278689  0.250
  Freund     2          79     0.5983607  0.7344262  0.375
  Freund     2          80     0.5954918  0.7278689  0.375
  Freund     2          81     0.5938525  0.7180328  0.250
  Freund     2          82     0.5918033  0.7409836  0.250
  Freund     2          83     0.5946721  0.7377049  0.250
  Freund     2          84     0.5934426  0.7409836  0.250
  Freund     2          85     0.5922131  0.7311475  0.375
  Freund     2          86     0.5946721  0.7213115  0.375
  Freund     2          87     0.5893443  0.7213115  0.375
  Freund     2          88     0.5844262  0.7147541  0.375
  Freund     2          89     0.5844262  0.7180328  0.250
  Freund     2          90     0.5840164  0.7180328  0.375
  Freund     2          91     0.5758197  0.7344262  0.250
  Freund     2          92     0.5700820  0.7278689  0.375
  Freund     2          93     0.5692623  0.7245902  0.375
  Freund     2          94     0.5737705  0.7245902  0.375
  Freund     2          95     0.5782787  0.7245902  0.375
  Freund     2          96     0.5774590  0.7311475  0.375
  Freund     2          97     0.5696721  0.7245902  0.375
  Freund     2          98     0.5639344  0.7245902  0.250
  Freund     2          99     0.5618852  0.7311475  0.250
  Freund     2         100     0.5692623  0.7245902  0.250
  Freund     3          20     0.6290984  0.7737705  0.250
  Freund     3          21     0.6219262  0.7672131  0.500
  Freund     3          22     0.6323770  0.7803279  0.500
  Freund     3          23     0.6217213  0.7704918  0.500
  Freund     3          24     0.6295082  0.7737705  0.500
  Freund     3          25     0.6315574  0.7868852  0.375
  Freund     3          26     0.6405738  0.7803279  0.375
  Freund     3          27     0.6397541  0.7803279  0.375
  Freund     3          28     0.6196721  0.7803279  0.375
  Freund     3          29     0.6163934  0.7934426  0.375
  Freund     3          30     0.6155738  0.7737705  0.375
  Freund     3          31     0.5864754  0.7737705  0.375
  Freund     3          32     0.5901639  0.7737705  0.375
  Freund     3          33     0.5950820  0.7901639  0.375
  Freund     3          34     0.6114754  0.8000000  0.375
  Freund     3          35     0.6151639  0.8032787  0.375
  Freund     3          36     0.6229508  0.7934426  0.375
  Freund     3          37     0.6065574  0.7934426  0.375
  Freund     3          38     0.6032787  0.8131148  0.250
  Freund     3          39     0.5971311  0.8098361  0.250
  Freund     3          40     0.6151639  0.7868852  0.250
  Freund     3          41     0.6135246  0.7868852  0.375
  Freund     3          42     0.6098361  0.7868852  0.375
  Freund     3          43     0.6135246  0.7836066  0.375
  Freund     3          44     0.6200820  0.7803279  0.250
  Freund     3          45     0.6127049  0.7704918  0.375
  Freund     3          46     0.6143443  0.7803279  0.250
  Freund     3          47     0.6053279  0.7737705  0.250
  Freund     3          48     0.5950820  0.7770492  0.250
  Freund     3          49     0.6016393  0.7704918  0.250
  Freund     3          50     0.5942623  0.7672131  0.250
  Freund     3          51     0.5926230  0.7770492  0.375
  Freund     3          52     0.5938525  0.7803279  0.375
  Freund     3          53     0.5893443  0.7770492  0.375
  Freund     3          54     0.5877049  0.7836066  0.375
  Freund     3          55     0.5881148  0.7770492  0.375
  Freund     3          56     0.5868852  0.7770492  0.375
  Freund     3          57     0.5762295  0.7737705  0.375
  Freund     3          58     0.5627049  0.7803279  0.375
  Freund     3          59     0.5577869  0.7770492  0.250
  Freund     3          60     0.5602459  0.7868852  0.375
  Freund     3          61     0.5561475  0.7934426  0.250
  Freund     3          62     0.5659836  0.7934426  0.250
  Freund     3          63     0.5643443  0.7934426  0.250
  Freund     3          64     0.5618852  0.7901639  0.250
  Freund     3          65     0.5598361  0.7967213  0.250
  Freund     3          66     0.5627049  0.7934426  0.250
  Freund     3          67     0.5704918  0.7934426  0.250
  Freund     3          68     0.5700820  0.7967213  0.250
  Freund     3          69     0.5668033  0.7967213  0.250
  Freund     3          70     0.5606557  0.7967213  0.250
  Freund     3          71     0.5651639  0.7934426  0.250
  Freund     3          72     0.5651639  0.8000000  0.250
  Freund     3          73     0.5569672  0.7934426  0.250
  Freund     3          74     0.5540984  0.8000000  0.250
  Freund     3          75     0.5668033  0.7934426  0.250
  Freund     3          76     0.5721311  0.7901639  0.250
  Freund     3          77     0.5631148  0.7934426  0.250
  Freund     3          78     0.5528689  0.7836066  0.250
  Freund     3          79     0.5442623  0.7934426  0.250
  Freund     3          80     0.5639344  0.7967213  0.250
  Freund     3          81     0.5577869  0.8000000  0.250
  Freund     3          82     0.5549180  0.7967213  0.250
  Freund     3          83     0.5565574  0.8000000  0.250
  Freund     3          84     0.5602459  0.8032787  0.250
  Freund     3          85     0.5598361  0.7934426  0.250
  Freund     3          86     0.5581967  0.7901639  0.250
  Freund     3          87     0.5524590  0.7901639  0.250
  Freund     3          88     0.5500000  0.7868852  0.250
  Freund     3          89     0.5495902  0.7901639  0.250
  Freund     3          90     0.5405738  0.7836066  0.250
  Freund     3          91     0.5569672  0.7868852  0.250
  Freund     3          92     0.5594262  0.7868852  0.250
  Freund     3          93     0.5598361  0.7868852  0.250
  Freund     3          94     0.5606557  0.7803279  0.250
  Freund     3          95     0.5688525  0.7836066  0.250
  Freund     3          96     0.5717213  0.7836066  0.250
  Freund     3          97     0.5717213  0.7868852  0.250
  Freund     3          98     0.5561475  0.7868852  0.250
  Freund     3          99     0.5602459  0.7836066  0.250
  Freund     3         100     0.5577869  0.7836066  0.250
  Freund     4          20     0.6397541  0.7704918  0.625
  Freund     4          21     0.5975410  0.7508197  0.375
  Freund     4          22     0.5811475  0.7409836  0.500
  Freund     4          23     0.5893443  0.7540984  0.375
  Freund     4          24     0.5885246  0.7573770  0.375
  Freund     4          25     0.6098361  0.7639344  0.375
  Freund     4          26     0.5893443  0.7639344  0.375
  Freund     4          27     0.6204918  0.7606557  0.500
  Freund     4          28     0.6381148  0.7573770  0.375
  Freund     4          29     0.6229508  0.7672131  0.375
  Freund     4          30     0.6098361  0.7672131  0.375
  Freund     4          31     0.6151639  0.7606557  0.375
  Freund     4          32     0.6188525  0.7442623  0.375
  Freund     4          33     0.6204918  0.7540984  0.375
  Freund     4          34     0.5991803  0.7475410  0.375
  Freund     4          35     0.5971311  0.7409836  0.375
  Freund     4          36     0.6122951  0.7639344  0.375
  Freund     4          37     0.6045082  0.7573770  0.375
  Freund     4          38     0.5995902  0.7508197  0.500
  Freund     4          39     0.5954918  0.7540984  0.375
  Freund     4          40     0.6057377  0.7508197  0.500
  Freund     4          41     0.6122951  0.7606557  0.375
  Freund     4          42     0.6077869  0.7508197  0.500
  Freund     4          43     0.6098361  0.7409836  0.500
  Freund     4          44     0.6045082  0.7573770  0.500
  Freund     4          45     0.6000000  0.7442623  0.500
  Freund     4          46     0.5987705  0.7442623  0.500
  Freund     4          47     0.5864754  0.7409836  0.375
  Freund     4          48     0.5831967  0.7377049  0.500
  Freund     4          49     0.5811475  0.7442623  0.500
  Freund     4          50     0.5913934  0.7573770  0.375
  Freund     4          51     0.5758197  0.7344262  0.375
  Freund     4          52     0.5889344  0.7377049  0.375
  Freund     4          53     0.5930328  0.7442623  0.500
  Freund     4          54     0.5848361  0.7409836  0.375
  Freund     4          55     0.5897541  0.7573770  0.375
  Freund     4          56     0.5930328  0.7344262  0.375
  Freund     4          57     0.5872951  0.7311475  0.375
  Freund     4          58     0.5905738  0.7377049  0.375
  Freund     4          59     0.5889344  0.7475410  0.375
  Freund     4          60     0.5872951  0.7606557  0.375
  Freund     4          61     0.5909836  0.7409836  0.375
  Freund     4          62     0.5770492  0.7442623  0.375
  Freund     4          63     0.5872951  0.7344262  0.375
  Freund     4          64     0.5967213  0.7377049  0.375
  Freund     4          65     0.5856557  0.7409836  0.375
  Freund     4          66     0.5774590  0.7409836  0.375
  Freund     4          67     0.5668033  0.7377049  0.375
  Freund     4          68     0.5700820  0.7442623  0.375
  Freund     4          69     0.5709016  0.7606557  0.375
  Freund     4          70     0.5799180  0.7475410  0.375
  Freund     4          71     0.5803279  0.7377049  0.375
  Freund     4          72     0.5774590  0.7409836  0.375
  Freund     4          73     0.5700820  0.7475410  0.375
  Freund     4          74     0.5557377  0.7442623  0.375
  Freund     4          75     0.5647541  0.7377049  0.375
  Freund     4          76     0.5688525  0.7475410  0.375
  Freund     4          77     0.5827869  0.7442623  0.375
  Freund     4          78     0.5823770  0.7508197  0.375
  Freund     4          79     0.5786885  0.7442623  0.375
  Freund     4          80     0.5811475  0.7344262  0.375
  Freund     4          81     0.5913934  0.7442623  0.375
  Freund     4          82     0.5905738  0.7442623  0.375
  Freund     4          83     0.5926230  0.7475410  0.375
  Freund     4          84     0.5942623  0.7508197  0.375
  Freund     4          85     0.5950820  0.7540984  0.375
  Freund     4          86     0.5905738  0.7475410  0.375
  Freund     4          87     0.5901639  0.7508197  0.375
  Freund     4          88     0.5905738  0.7475410  0.375
  Freund     4          89     0.5954918  0.7508197  0.375
  Freund     4          90     0.5963115  0.7540984  0.375
  Freund     4          91     0.5918033  0.7508197  0.375
  Freund     4          92     0.5918033  0.7540984  0.375
  Freund     4          93     0.5860656  0.7442623  0.375
  Freund     4          94     0.5840164  0.7475410  0.375
  Freund     4          95     0.5868852  0.7442623  0.375
  Freund     4          96     0.5872951  0.7442623  0.375
  Freund     4          97     0.5868852  0.7442623  0.375
  Freund     4          98     0.5856557  0.7475410  0.375
  Freund     4          99     0.5942623  0.7508197  0.375
  Freund     4         100     0.5930328  0.7606557  0.375
  Zhu        2          20     0.5663934  0.7770492  0.250
  Zhu        2          21     0.5508197  0.7672131  0.125
  Zhu        2          22     0.5520492  0.7868852  0.375
  Zhu        2          23     0.5377049  0.7836066  0.375
  Zhu        2          24     0.5459016  0.7967213  0.375
  Zhu        2          25     0.5782787  0.8163934  0.375
  Zhu        2          26     0.5684426  0.8065574  0.250
  Zhu        2          27     0.5315574  0.7967213  0.250
  Zhu        2          28     0.5459016  0.7672131  0.375
  Zhu        2          29     0.5393443  0.7901639  0.250
  Zhu        2          30     0.5360656  0.8065574  0.375
  Zhu        2          31     0.4979508  0.7934426  0.125
  Zhu        2          32     0.5168033  0.7803279  0.250
  Zhu        2          33     0.5430328  0.7868852  0.250
  Zhu        2          34     0.5327869  0.7868852  0.250
  Zhu        2          35     0.5520492  0.8000000  0.250
  Zhu        2          36     0.5524590  0.8163934  0.250
  Zhu        2          37     0.5475410  0.8163934  0.250
  Zhu        2          38     0.5877049  0.8262295  0.250
  Zhu        2          39     0.5680328  0.8065574  0.250
  Zhu        2          40     0.6118852  0.8295082  0.250
  Zhu        2          41     0.6118852  0.7934426  0.250
  Zhu        2          42     0.6127049  0.8163934  0.250
  Zhu        2          43     0.6061475  0.8032787  0.250
  Zhu        2          44     0.6061475  0.8032787  0.250
  Zhu        2          45     0.5893443  0.8032787  0.250
  Zhu        2          46     0.5852459  0.8032787  0.500
  Zhu        2          47     0.5717213  0.8000000  0.125
  Zhu        2          48     0.5959016  0.7934426  0.375
  Zhu        2          49     0.5979508  0.7901639  0.250
  Zhu        2          50     0.5942623  0.8032787  0.250
  Zhu        2          51     0.6040984  0.7868852  0.375
  Zhu        2          52     0.6000000  0.8098361  0.250
  Zhu        2          53     0.6213115  0.8098361  0.375
  Zhu        2          54     0.5942623  0.8065574  0.375
  Zhu        2          55     0.5926230  0.8032787  0.375
  Zhu        2          56     0.6008197  0.8032787  0.375
  Zhu        2          57     0.5979508  0.8131148  0.250
  Zhu        2          58     0.5868852  0.8131148  0.375
  Zhu        2          59     0.5868852  0.8131148  0.375
  Zhu        2          60     0.5807377  0.8163934  0.250
  Zhu        2          61     0.5790984  0.8032787  0.375
  Zhu        2          62     0.5672131  0.7967213  0.375
  Zhu        2          63     0.5655738  0.8131148  0.250
  Zhu        2          64     0.5762295  0.8032787  0.375
  Zhu        2          65     0.5680328  0.8131148  0.250
  Zhu        2          66     0.5889344  0.8065574  0.375
  Zhu        2          67     0.5852459  0.8032787  0.250
  Zhu        2          68     0.5840164  0.8000000  0.375
  Zhu        2          69     0.6069672  0.8032787  0.375
  Zhu        2          70     0.5954918  0.8065574  0.375
  Zhu        2          71     0.6024590  0.7967213  0.375
  Zhu        2          72     0.5872951  0.8065574  0.375
  Zhu        2          73     0.6090164  0.8032787  0.375
  Zhu        2          74     0.5786885  0.8065574  0.375
  Zhu        2          75     0.5831967  0.8032787  0.375
  Zhu        2          76     0.5803279  0.8131148  0.375
  Zhu        2          77     0.5733607  0.8163934  0.375
  Zhu        2          78     0.5504098  0.8065574  0.250
  Zhu        2          79     0.5360656  0.7967213  0.250
  Zhu        2          80     0.5536885  0.8098361  0.250
  Zhu        2          81     0.5663934  0.7967213  0.375
  Zhu        2          82     0.5618852  0.8000000  0.250
  Zhu        2          83     0.5602459  0.7901639  0.375
  Zhu        2          84     0.5430328  0.7967213  0.250
  Zhu        2          85     0.5602459  0.7901639  0.375
  Zhu        2          86     0.5557377  0.7901639  0.375
  Zhu        2          87     0.5549180  0.8000000  0.375
  Zhu        2          88     0.5487705  0.7901639  0.375
  Zhu        2          89     0.5540984  0.8000000  0.375
  Zhu        2          90     0.5696721  0.8032787  0.375
  Zhu        2          91     0.5704918  0.7967213  0.375
  Zhu        2          92     0.5688525  0.7967213  0.375
  Zhu        2          93     0.5713115  0.7934426  0.375
  Zhu        2          94     0.5737705  0.8065574  0.375
  Zhu        2          95     0.5688525  0.8000000  0.375
  Zhu        2          96     0.5487705  0.7901639  0.375
  Zhu        2          97     0.5512295  0.7803279  0.250
  Zhu        2          98     0.5504098  0.7836066  0.250
  Zhu        2          99     0.5598361  0.7803279  0.250
  Zhu        2         100     0.5631148  0.7967213  0.375
  Zhu        3          20     0.5512295  0.6983607  0.375
  Zhu        3          21     0.5729508  0.7311475  0.375
  Zhu        3          22     0.5704918  0.7377049  0.250
  Zhu        3          23     0.5450820  0.7508197  0.250
  Zhu        3          24     0.5823770  0.7278689  0.250
  Zhu        3          25     0.5848361  0.7475410  0.250
  Zhu        3          26     0.6024590  0.7704918  0.500
  Zhu        3          27     0.5864754  0.7409836  0.500
  Zhu        3          28     0.6258197  0.7508197  0.500
  Zhu        3          29     0.6315574  0.7606557  0.500
  Zhu        3          30     0.6348361  0.7704918  0.500
  Zhu        3          31     0.6356557  0.7770492  0.500
  Zhu        3          32     0.6352459  0.7770492  0.500
  Zhu        3          33     0.6389344  0.7901639  0.500
  Zhu        3          34     0.6159836  0.7737705  0.500
  Zhu        3          35     0.6192623  0.7704918  0.500
  Zhu        3          36     0.6139344  0.7704918  0.375
  Zhu        3          37     0.6299180  0.7770492  0.500
  Zhu        3          38     0.6360656  0.7737705  0.500
  Zhu        3          39     0.6467213  0.7704918  0.500
  Zhu        3          40     0.6315574  0.7672131  0.500
  Zhu        3          41     0.6217213  0.7573770  0.500
  Zhu        3          42     0.6274590  0.7573770  0.500
  Zhu        3          43     0.6340164  0.7606557  0.500
  Zhu        3          44     0.6172131  0.7606557  0.500
  Zhu        3          45     0.6172131  0.7606557  0.500
  Zhu        3          46     0.6122951  0.7475410  0.500
  Zhu        3          47     0.6184426  0.7672131  0.500
  Zhu        3          48     0.6258197  0.7639344  0.500
  Zhu        3          49     0.6204918  0.7639344  0.500
  Zhu        3          50     0.6061475  0.7508197  0.500
  Zhu        3          51     0.6372951  0.7639344  0.625
  Zhu        3          52     0.6381148  0.7737705  0.500
  Zhu        3          53     0.6360656  0.7606557  0.625
  Zhu        3          54     0.6450820  0.7639344  0.625
  Zhu        3          55     0.6479508  0.7606557  0.625
  Zhu        3          56     0.6356557  0.7606557  0.500
  Zhu        3          57     0.6221311  0.7540984  0.500
  Zhu        3          58     0.6184426  0.7475410  0.375
  Zhu        3          59     0.6254098  0.7508197  0.625
  Zhu        3          60     0.6331967  0.7573770  0.625
  Zhu        3          61     0.6372951  0.7672131  0.625
  Zhu        3          62     0.6344262  0.7540984  0.625
  Zhu        3          63     0.6397541  0.7606557  0.625
  Zhu        3          64     0.6430328  0.7606557  0.500
  Zhu        3          65     0.6418033  0.7442623  0.625
  Zhu        3          66     0.6331967  0.7540984  0.500
  Zhu        3          67     0.6336066  0.7409836  0.625
  Zhu        3          68     0.6331967  0.7442623  0.625
  Zhu        3          69     0.6196721  0.7442623  0.500
  Zhu        3          70     0.6155738  0.7475410  0.500
  Zhu        3          71     0.6188525  0.7508197  0.500
  Zhu        3          72     0.6209016  0.7475410  0.500
  Zhu        3          73     0.6143443  0.7540984  0.500
  Zhu        3          74     0.6090164  0.7540984  0.500
  Zhu        3          75     0.6102459  0.7508197  0.500
  Zhu        3          76     0.6069672  0.7540984  0.500
  Zhu        3          77     0.6036885  0.7409836  0.500
  Zhu        3          78     0.6090164  0.7540984  0.375
  Zhu        3          79     0.6168033  0.7508197  0.625
  Zhu        3          80     0.6159836  0.7606557  0.375
  Zhu        3          81     0.6245902  0.7573770  0.625
  Zhu        3          82     0.6262295  0.7540984  0.500
  Zhu        3          83     0.6241803  0.7573770  0.625
  Zhu        3          84     0.6344262  0.7606557  0.625
  Zhu        3          85     0.6303279  0.7639344  0.500
  Zhu        3          86     0.6245902  0.7475410  0.625
  Zhu        3          87     0.6299180  0.7606557  0.500
  Zhu        3          88     0.6262295  0.7606557  0.625
  Zhu        3          89     0.6290984  0.7639344  0.500
  Zhu        3          90     0.6188525  0.7606557  0.500
  Zhu        3          91     0.6229508  0.7475410  0.500
  Zhu        3          92     0.6176230  0.7475410  0.500
  Zhu        3          93     0.6196721  0.7409836  0.500
  Zhu        3          94     0.6176230  0.7475410  0.500
  Zhu        3          95     0.6266393  0.7540984  0.625
  Zhu        3          96     0.6217213  0.7475410  0.625
  Zhu        3          97     0.6250000  0.7540984  0.625
  Zhu        3          98     0.6204918  0.7639344  0.500
  Zhu        3          99     0.6209016  0.7573770  0.500
  Zhu        3         100     0.6168033  0.7573770  0.375
  Zhu        4          20     0.6704918  0.7508197  0.375
  Zhu        4          21     0.6356557  0.7573770  0.375
  Zhu        4          22     0.6299180  0.7672131  0.375
  Zhu        4          23     0.6233607  0.7803279  0.375
  Zhu        4          24     0.6245902  0.7836066  0.375
  Zhu        4          25     0.6213115  0.8065574  0.375
  Zhu        4          26     0.6204918  0.7934426  0.375
  Zhu        4          27     0.5934426  0.7934426  0.375
  Zhu        4          28     0.6012295  0.7803279  0.500
  Zhu        4          29     0.5991803  0.7704918  0.375
  Zhu        4          30     0.5946721  0.7606557  0.375
  Zhu        4          31     0.6016393  0.7836066  0.375
  Zhu        4          32     0.6077869  0.7901639  0.375
  Zhu        4          33     0.5995902  0.7868852  0.375
  Zhu        4          34     0.6000000  0.7934426  0.375
  Zhu        4          35     0.6131148  0.8000000  0.375
  Zhu        4          36     0.6073770  0.8000000  0.375
  Zhu        4          37     0.6020492  0.8032787  0.375
  Zhu        4          38     0.6008197  0.7934426  0.375
  Zhu        4          39     0.5971311  0.7934426  0.375
  Zhu        4          40     0.5856557  0.7868852  0.375
  Zhu        4          41     0.5852459  0.7934426  0.375
  Zhu        4          42     0.5897541  0.7868852  0.375
  Zhu        4          43     0.6036885  0.7901639  0.375
  Zhu        4          44     0.6081967  0.7836066  0.375
  Zhu        4          45     0.6151639  0.7934426  0.250
  Zhu        4          46     0.6094262  0.7934426  0.375
  Zhu        4          47     0.6081967  0.8000000  0.375
  Zhu        4          48     0.6036885  0.7934426  0.375
  Zhu        4          49     0.6020492  0.7967213  0.250
  Zhu        4          50     0.6102459  0.7934426  0.375
  Zhu        4          51     0.6081967  0.7967213  0.375
  Zhu        4          52     0.6110656  0.8032787  0.375
  Zhu        4          53     0.6053279  0.8000000  0.375
  Zhu        4          54     0.6016393  0.8032787  0.375
  Zhu        4          55     0.6122951  0.7967213  0.375
  Zhu        4          56     0.6127049  0.8032787  0.375
  Zhu        4          57     0.6196721  0.8032787  0.375
  Zhu        4          58     0.6040984  0.8065574  0.375
  Zhu        4          59     0.5934426  0.8098361  0.375
  Zhu        4          60     0.5918033  0.8000000  0.375
  Zhu        4          61     0.5868852  0.8098361  0.375
  Zhu        4          62     0.6127049  0.8098361  0.375
  Zhu        4          63     0.5934426  0.8098361  0.375
  Zhu        4          64     0.5872951  0.8098361  0.250
  Zhu        4          65     0.6045082  0.7967213  0.375
  Zhu        4          66     0.6020492  0.8131148  0.375
  Zhu        4          67     0.6053279  0.8163934  0.375
  Zhu        4          68     0.6147541  0.8196721  0.250
  Zhu        4          69     0.6057377  0.8229508  0.375
  Zhu        4          70     0.6032787  0.8229508  0.375
  Zhu        4          71     0.6069672  0.8229508  0.375
  Zhu        4          72     0.5975410  0.8196721  0.375
  Zhu        4          73     0.5922131  0.8196721  0.375
  Zhu        4          74     0.6024590  0.8163934  0.250
  Zhu        4          75     0.5975410  0.8065574  0.375
  Zhu        4          76     0.6098361  0.8098361  0.375
  Zhu        4          77     0.6225410  0.8131148  0.375
  Zhu        4          78     0.6241803  0.8229508  0.375
  Zhu        4          79     0.6213115  0.8229508  0.375
  Zhu        4          80     0.6188525  0.8131148  0.375
  Zhu        4          81     0.6163934  0.8196721  0.375
  Zhu        4          82     0.6237705  0.8131148  0.375
  Zhu        4          83     0.6266393  0.8131148  0.375
  Zhu        4          84     0.6266393  0.8065574  0.375
  Zhu        4          85     0.6336066  0.8032787  0.375
  Zhu        4          86     0.6204918  0.8032787  0.375
  Zhu        4          87     0.6196721  0.8032787  0.375
  Zhu        4          88     0.6204918  0.8032787  0.375
  Zhu        4          89     0.6286885  0.8065574  0.375
  Zhu        4          90     0.6221311  0.8065574  0.375
  Zhu        4          91     0.6237705  0.8098361  0.375
  Zhu        4          92     0.6282787  0.8098361  0.375
  Zhu        4          93     0.6344262  0.8196721  0.375
  Zhu        4          94     0.6274590  0.8196721  0.375
  Zhu        4          95     0.6290984  0.8196721  0.375
  Zhu        4          96     0.6319672  0.8098361  0.375
  Zhu        4          97     0.6295082  0.8065574  0.375
  Zhu        4          98     0.6299180  0.8131148  0.250
  Zhu        4          99     0.6381148  0.8098361  0.375
  Zhu        4         100     0.6368852  0.8229508  0.250

ROC was used to select the optimal model using the largest value.
The final values used for the model were mfinal = 86, maxdepth = 2
 and coeflearn = Breiman.

    \end{Verbatim}

    
    \begin{verbatim}
Bootstrapped (1 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   No  Yes
       No  77.3  1.6
       Yes 20.1  1.0
                            
 Accuracy (average) : 0.7827

    \end{verbatim}

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_102_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Confusion Matrix for adaboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} caretPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}ada\PYZus{}down.model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{raw\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}caretPredictedClass\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  261   2
       Yes  99   9
                                          
               Accuracy : 0.7278          
                 95% CI : (0.6794, 0.7724)
    No Information Rate : 0.9704          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.103           
 Mcnemar's Test P-Value : <2e-16          
                                          
            Sensitivity : 0.72500         
            Specificity : 0.81818         
         Pos Pred Value : 0.99240         
         Neg Pred Value : 0.08333         
             Prevalence : 0.97035         
         Detection Rate : 0.70350         
   Detection Prevalence : 0.70889         
      Balanced Accuracy : 0.77159         
                                          
       'Positive' Class : No              
                                          
    \end{verbatim}

    
    ROC plot for adaboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} ada\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}ada\PYZus{}down.model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         ada\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}ada\PYZus{}pred\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         ada\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}ada\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}ada\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for adaboost with down sample\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}ada\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.8616162


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_106_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{boosting-with-adaboost-smote}{%
\subsubsection{Boosting with adaboost
(SMOTE)}\label{boosting-with-adaboost-smote}}

The below code chunk sets some of the control parameters for adaboost

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}57}]:} objControl \PY{o}{\PYZlt{}\PYZhy{}} trainControl\PY{p}{(}method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{boot\PYZsq{}}\PY{p}{,} number \PY{o}{=} \PY{l+m}{1}\PY{p}{,}
                                    returnResamp\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{all\PYZsq{}}\PY{p}{,}
                                    summaryFunction \PY{o}{=} twoClassSummary\PY{p}{,}
                                    savePredictions \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    classProbs \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    sampling \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{smote\PYZdq{}}\PY{p}{)}\PY{c+c1}{\PYZsh{}, p = 0.70) \PYZsh{}in case method = \PYZsh{}\PYZdq{}LGO\PYZdq{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}58}]:} search\PYZus{}grid \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{expand.grid}\PY{p}{(}mfinal \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{20}\PY{o}{:}\PY{l+m}{100}\PY{p}{)}\PY{p}{,} maxdepth \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{2}\PY{o}{:}\PY{l+m}{4}\PY{p}{)}\PY{p}{,}
                             coeflearn \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Breiman\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{Freund\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{Zhu\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    After setting the control paramters, the model is run

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}59}]:} num\PYZus{}cores \PY{o}{\PYZlt{}\PYZhy{}} makeCluster\PY{p}{(}detectCores\PY{p}{(}\PY{p}{)}\PY{l+m}{\PYZhy{}5}\PY{p}{)}
         registerDoParallel\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         tic\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Adaptive Boosting with SMOTE\PYZdq{}}\PY{p}{)}
         
         \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         ada\PYZus{}smote\PYZus{}model \PY{o}{\PYZlt{}\PYZhy{}} train\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{,}
                           method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{AdaBoost.M1\PYZsq{}}\PY{p}{,}
                           trControl\PY{o}{=}objControl\PY{p}{,}
                           tuneGrid \PY{o}{=} search\PYZus{}grid\PY{p}{,}
                           metric \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{ROC\PYZdq{}}\PY{p}{)}
         stopCluster\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         toc\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Adaptive Boosting with SMOTE: 123.87 sec elapsed

    \end{Verbatim}

    Confusion Matrix for adaboost on train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{c+c1}{\PYZsh{}ada\PYZus{}smote\PYZus{}model\PYZdl{}finalModel \PYZsh{}ada\PYZus{}smote\PYZus{}model\PYZdl{}results}
         \PY{k+kp}{print}\PY{p}{(}ada\PYZus{}smote\PYZus{}model\PY{p}{)}
         confusionMatrix.train\PY{p}{(}ada\PYZus{}smote\PYZus{}model\PY{p}{)}
         plot\PY{p}{(}varImp\PY{p}{(}ada\PYZus{}smote\PYZus{}model\PY{p}{)}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable importance from Adaboost with SMOTE\PYZdq{}}\PY{p}{,} col \PY{o}{=} \PY{l+m}{2}\PY{p}{,} lwd \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
AdaBoost.M1 

868 samples
  5 predictor
  2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Bootstrapped (1 reps) 
Summary of sample sizes: 868 
Addtional sampling using SMOTE

Resampling results across tuning parameters:

  coeflearn  maxdepth  mfinal  ROC        Sens       Spec 
  Breiman    2          20     0.6805328  0.8262295  0.250
  Breiman    2          21     0.6838115  0.8590164  0.125
  Breiman    2          22     0.6547131  0.8590164  0.125
  Breiman    2          23     0.6555328  0.8622951  0.000
  Breiman    2          24     0.6629098  0.8721311  0.000
  Breiman    2          25     0.6612705  0.8852459  0.000
  Breiman    2          26     0.6657787  0.8754098  0.000
  Breiman    2          27     0.6764344  0.8459016  0.125
  Breiman    2          28     0.6784836  0.8622951  0.000
  Breiman    2          29     0.6522541  0.8491803  0.125
  Breiman    2          30     0.6534836  0.8688525  0.000
  Breiman    2          31     0.6532787  0.8688525  0.125
  Breiman    2          32     0.6491803  0.8688525  0.000
  Breiman    2          33     0.6252049  0.8622951  0.125
  Breiman    2          34     0.6362705  0.8459016  0.125
  Breiman    2          35     0.6317623  0.8459016  0.125
  Breiman    2          36     0.6247951  0.8590164  0.125
  Breiman    2          37     0.6235656  0.8622951  0.125
  Breiman    2          38     0.6227459  0.8590164  0.125
  Breiman    2          39     0.6319672  0.8622951  0.125
  Breiman    2          40     0.6319672  0.8688525  0.125
  Breiman    2          41     0.6299180  0.8786885  0.125
  Breiman    2          42     0.6319672  0.8655738  0.125
  Breiman    2          43     0.6225410  0.8557377  0.125
  Breiman    2          44     0.6118852  0.8622951  0.125
  Breiman    2          45     0.6200820  0.8557377  0.125
  Breiman    2          46     0.6196721  0.8622951  0.125
  Breiman    2          47     0.6217213  0.8688525  0.125
  Breiman    2          48     0.6422131  0.8688525  0.125
  Breiman    2          49     0.6229508  0.8622951  0.125
  Breiman    2          50     0.6102459  0.8557377  0.125
  Breiman    2          51     0.6106557  0.8688525  0.125
  Breiman    2          52     0.5918033  0.8590164  0.125
  Breiman    2          53     0.5823770  0.8459016  0.125
  Breiman    2          54     0.5823770  0.8590164  0.000
  Breiman    2          55     0.5913934  0.8557377  0.125
  Breiman    2          56     0.5963115  0.8590164  0.125
  Breiman    2          57     0.5975410  0.8459016  0.125
  Breiman    2          58     0.5967213  0.8590164  0.125
  Breiman    2          59     0.5975410  0.8622951  0.125
  Breiman    2          60     0.5934426  0.8655738  0.125
  Breiman    2          61     0.6159836  0.8688525  0.000
  Breiman    2          62     0.5950820  0.8655738  0.125
  Breiman    2          63     0.5987705  0.8590164  0.125
  Breiman    2          64     0.5987705  0.8557377  0.125
  Breiman    2          65     0.5987705  0.8590164  0.125
  Breiman    2          66     0.5950820  0.8590164  0.000
  Breiman    2          67     0.5893443  0.8590164  0.000
  Breiman    2          68     0.5905738  0.8622951  0.000
  Breiman    2          69     0.5819672  0.8524590  0.125
  Breiman    2          70     0.5790984  0.8491803  0.000
  Breiman    2          71     0.5856557  0.8491803  0.000
  Breiman    2          72     0.5950820  0.8524590  0.125
  Breiman    2          73     0.5918033  0.8655738  0.000
  Breiman    2          74     0.6053279  0.8491803  0.125
  Breiman    2          75     0.6069672  0.8655738  0.000
  Breiman    2          76     0.6061475  0.8622951  0.000
  Breiman    2          77     0.6065574  0.8524590  0.125
  Breiman    2          78     0.6057377  0.8622951  0.000
  Breiman    2          79     0.6110656  0.8524590  0.125
  Breiman    2          80     0.6008197  0.8459016  0.125
  Breiman    2          81     0.6028689  0.8491803  0.125
  Breiman    2          82     0.6147541  0.8426230  0.125
  Breiman    2          83     0.6155738  0.8459016  0.125
  Breiman    2          84     0.6151639  0.8557377  0.000
  Breiman    2          85     0.6262295  0.8491803  0.000
  Breiman    2          86     0.6237705  0.8426230  0.000
  Breiman    2          87     0.6237705  0.8426230  0.125
  Breiman    2          88     0.6069672  0.8393443  0.125
  Breiman    2          89     0.6069672  0.8393443  0.125
  Breiman    2          90     0.6069672  0.8622951  0.000
  Breiman    2          91     0.6241803  0.8491803  0.125
  Breiman    2          92     0.6372951  0.8590164  0.000
  Breiman    2          93     0.6364754  0.8655738  0.000
  Breiman    2          94     0.6290984  0.8688525  0.000
  Breiman    2          95     0.6213115  0.8622951  0.000
  Breiman    2          96     0.6315574  0.8590164  0.000
  Breiman    2          97     0.6344262  0.8491803  0.000
  Breiman    2          98     0.6340164  0.8655738  0.000
  Breiman    2          99     0.6217213  0.8590164  0.000
  Breiman    2         100     0.6184426  0.8590164  0.125
  Breiman    3          20     0.6381148  0.8590164  0.125
  Breiman    3          21     0.6266393  0.8688525  0.125
  Breiman    3          22     0.6245902  0.8721311  0.250
  Breiman    3          23     0.6209016  0.8885246  0.125
  Breiman    3          24     0.6069672  0.8852459  0.125
  Breiman    3          25     0.5959016  0.8819672  0.125
  Breiman    3          26     0.6086066  0.8950820  0.125
  Breiman    3          27     0.6040984  0.8819672  0.125
  Breiman    3          28     0.6081967  0.8754098  0.125
  Breiman    3          29     0.6135246  0.8786885  0.125
  Breiman    3          30     0.6094262  0.8885246  0.125
  Breiman    3          31     0.5852459  0.8819672  0.125
  Breiman    3          32     0.6081967  0.8819672  0.125
  Breiman    3          33     0.6204918  0.8754098  0.125
  Breiman    3          34     0.6204918  0.8721311  0.125
  Breiman    3          35     0.6336066  0.8590164  0.125
  Breiman    3          36     0.6254098  0.8721311  0.125
  Breiman    3          37     0.6192623  0.8721311  0.125
  Breiman    3          38     0.6163934  0.8721311  0.125
  Breiman    3          39     0.6077869  0.8622951  0.125
  Breiman    3          40     0.6028689  0.8622951  0.125
  Breiman    3          41     0.6008197  0.8590164  0.125
  Breiman    3          42     0.6008197  0.8688525  0.125
  Breiman    3          43     0.6008197  0.8721311  0.125
  Breiman    3          44     0.5979508  0.8688525  0.125
  Breiman    3          45     0.5983607  0.8754098  0.125
  Breiman    3          46     0.5926230  0.8622951  0.125
  Breiman    3          47     0.5926230  0.8655738  0.125
  Breiman    3          48     0.5987705  0.8819672  0.125
  Breiman    3          49     0.5811475  0.8655738  0.125
  Breiman    3          50     0.5725410  0.8655738  0.125
  Breiman    3          51     0.5872951  0.8622951  0.125
  Breiman    3          52     0.6004098  0.8655738  0.125
  Breiman    3          53     0.6036885  0.8622951  0.125
  Breiman    3          54     0.6012295  0.8655738  0.125
  Breiman    3          55     0.6053279  0.8590164  0.125
  Breiman    3          56     0.5946721  0.8688525  0.125
  Breiman    3          57     0.5938525  0.8655738  0.125
  Breiman    3          58     0.6155738  0.8655738  0.125
  Breiman    3          59     0.6028689  0.8590164  0.125
  Breiman    3          60     0.6139344  0.8688525  0.125
  Breiman    3          61     0.6286885  0.8655738  0.125
  Breiman    3          62     0.6282787  0.8754098  0.125
  Breiman    3          63     0.6295082  0.8754098  0.125
  Breiman    3          64     0.6336066  0.8655738  0.125
  Breiman    3          65     0.6151639  0.8819672  0.125
  Breiman    3          66     0.6102459  0.8852459  0.125
  Breiman    3          67     0.6061475  0.8852459  0.125
  Breiman    3          68     0.6098361  0.8786885  0.125
  Breiman    3          69     0.6139344  0.8885246  0.125
  Breiman    3          70     0.6274590  0.8885246  0.125
  Breiman    3          71     0.6315574  0.8885246  0.125
  Breiman    3          72     0.6245902  0.8721311  0.125
  Breiman    3          73     0.6385246  0.8721311  0.125
  Breiman    3          74     0.6327869  0.8688525  0.125
  Breiman    3          75     0.6413934  0.8754098  0.125
  Breiman    3          76     0.6348361  0.8688525  0.125
  Breiman    3          77     0.6315574  0.8688525  0.125
  Breiman    3          78     0.6311475  0.8721311  0.125
  Breiman    3          79     0.6397541  0.8786885  0.125
  Breiman    3          80     0.6389344  0.8754098  0.125
  Breiman    3          81     0.6397541  0.8754098  0.125
  Breiman    3          82     0.6430328  0.8721311  0.125
  Breiman    3          83     0.6364754  0.8688525  0.125
  Breiman    3          84     0.6356557  0.8688525  0.125
  Breiman    3          85     0.6356557  0.8721311  0.125
  Breiman    3          86     0.6315574  0.8754098  0.125
  Breiman    3          87     0.6315574  0.8786885  0.125
  Breiman    3          88     0.6377049  0.8819672  0.125
  Breiman    3          89     0.6344262  0.8885246  0.125
  Breiman    3          90     0.6405738  0.8819672  0.125
  Breiman    3          91     0.6397541  0.8885246  0.125
  Breiman    3          92     0.6430328  0.8819672  0.125
  Breiman    3          93     0.6430328  0.8786885  0.125
  Breiman    3          94     0.6401639  0.8852459  0.125
  Breiman    3          95     0.6495902  0.8950820  0.125
  Breiman    3          96     0.6528689  0.8918033  0.125
  Breiman    3          97     0.6397541  0.8885246  0.125
  Breiman    3          98     0.6389344  0.8819672  0.125
  Breiman    3          99     0.6409836  0.8721311  0.125
  Breiman    3         100     0.6446721  0.8852459  0.125
  Breiman    4          20     0.6682377  0.8852459  0.125
  Breiman    4          21     0.6680328  0.8819672  0.250
  Breiman    4          22     0.6725410  0.8819672  0.125
  Breiman    4          23     0.6881148  0.8622951  0.250
  Breiman    4          24     0.6963115  0.8786885  0.125
  Breiman    4          25     0.6934426  0.8918033  0.250
  Breiman    4          26     0.6987705  0.8918033  0.125
  Breiman    4          27     0.7057377  0.9114754  0.375
  Breiman    4          28     0.7086066  0.8950820  0.375
  Breiman    4          29     0.7176230  0.9016393  0.375
  Breiman    4          30     0.7229508  0.9049180  0.375
  Breiman    4          31     0.7237705  0.9114754  0.375
  Breiman    4          32     0.7229508  0.9114754  0.375
  Breiman    4          33     0.7106557  0.9016393  0.375
  Breiman    4          34     0.7237705  0.8885246  0.375
  Breiman    4          35     0.7098361  0.8918033  0.375
  Breiman    4          36     0.7057377  0.9081967  0.250
  Breiman    4          37     0.6934426  0.8983607  0.250
  Breiman    4          38     0.6913934  0.8918033  0.250
  Breiman    4          39     0.6909836  0.8983607  0.250
  Breiman    4          40     0.6950820  0.8950820  0.250
  Breiman    4          41     0.6885246  0.8918033  0.250
  Breiman    4          42     0.6868852  0.9016393  0.375
  Breiman    4          43     0.6774590  0.9049180  0.375
  Breiman    4          44     0.6762295  0.9114754  0.375
  Breiman    4          45     0.6737705  0.9081967  0.375
  Breiman    4          46     0.6745902  0.9016393  0.375
  Breiman    4          47     0.6823770  0.9016393  0.375
  Breiman    4          48     0.6782787  0.9049180  0.250
  Breiman    4          49     0.6766393  0.9049180  0.250
  Breiman    4          50     0.6655738  0.9049180  0.250
  Breiman    4          51     0.6721311  0.8983607  0.250
  Breiman    4          52     0.6610656  0.9049180  0.250
  Breiman    4          53     0.6565574  0.9081967  0.375
  Breiman    4          54     0.6475410  0.8950820  0.375
  Breiman    4          55     0.6553279  0.8983607  0.375
  Breiman    4          56     0.6508197  0.8950820  0.375
  Breiman    4          57     0.6434426  0.8918033  0.375
  Breiman    4          58     0.6397541  0.8950820  0.375
  Breiman    4          59     0.6540984  0.8918033  0.250
  Breiman    4          60     0.6549180  0.8918033  0.375
  Breiman    4          61     0.6557377  0.8950820  0.250
  Breiman    4          62     0.6553279  0.8983607  0.250
  Breiman    4          63     0.6704918  0.9016393  0.250
  Breiman    4          64     0.6762295  0.9016393  0.250
  Breiman    4          65     0.6676230  0.9049180  0.250
  Breiman    4          66     0.6545082  0.8983607  0.250
  Breiman    4          67     0.6553279  0.9016393  0.250
  Breiman    4          68     0.6438525  0.8983607  0.250
  Breiman    4          69     0.6508197  0.8950820  0.250
  Breiman    4          70     0.6508197  0.9016393  0.250
  Breiman    4          71     0.6418033  0.8950820  0.125
  Breiman    4          72     0.6516393  0.8983607  0.125
  Breiman    4          73     0.6471311  0.8983607  0.125
  Breiman    4          74     0.6495902  0.8950820  0.250
  Breiman    4          75     0.6545082  0.9016393  0.375
  Breiman    4          76     0.6450820  0.9016393  0.375
  Breiman    4          77     0.6344262  0.8983607  0.375
  Breiman    4          78     0.6463115  0.9016393  0.250
  Breiman    4          79     0.6426230  0.9016393  0.250
  Breiman    4          80     0.6397541  0.9049180  0.250
  Breiman    4          81     0.6438525  0.8983607  0.250
  Breiman    4          82     0.6401639  0.9016393  0.250
  Breiman    4          83     0.6327869  0.8918033  0.250
  Breiman    4          84     0.6471311  0.8983607  0.250
  Breiman    4          85     0.6483607  0.8950820  0.250
  Breiman    4          86     0.6475410  0.9016393  0.250
  Breiman    4          87     0.6536885  0.8950820  0.125
  Breiman    4          88     0.6360656  0.9016393  0.125
  Breiman    4          89     0.6344262  0.8983607  0.125
  Breiman    4          90     0.6344262  0.9016393  0.125
  Breiman    4          91     0.6315574  0.8983607  0.125
  Breiman    4          92     0.6262295  0.9049180  0.125
  Breiman    4          93     0.6155738  0.8983607  0.125
  Breiman    4          94     0.6245902  0.8983607  0.125
  Breiman    4          95     0.6135246  0.8983607  0.125
  Breiman    4          96     0.6221311  0.8983607  0.125
  Breiman    4          97     0.6241803  0.9016393  0.125
  Breiman    4          98     0.6184426  0.9016393  0.125
  Breiman    4          99     0.6217213  0.9016393  0.125
  Breiman    4         100     0.6352459  0.8983607  0.125
  Freund     2          20     0.6084016  0.8360656  0.125
  Freund     2          21     0.6297131  0.8327869  0.125
  Freund     2          22     0.6385246  0.8327869  0.125
  Freund     2          23     0.6688525  0.8393443  0.125
  Freund     2          24     0.6520492  0.8557377  0.125
  Freund     2          25     0.6356557  0.8360656  0.125
  Freund     2          26     0.5954918  0.8360656  0.250
  Freund     2          27     0.5954918  0.8459016  0.125
  Freund     2          28     0.5983607  0.8524590  0.125
  Freund     2          29     0.5827869  0.8327869  0.125
  Freund     2          30     0.5852459  0.8295082  0.125
  Freund     2          31     0.5766393  0.8459016  0.125
  Freund     2          32     0.5721311  0.8327869  0.125
  Freund     2          33     0.5725410  0.8459016  0.125
  Freund     2          34     0.5696721  0.8459016  0.125
  Freund     2          35     0.5618852  0.8393443  0.125
  Freund     2          36     0.5668033  0.8295082  0.250
  Freund     2          37     0.6237705  0.8491803  0.125
  Freund     2          38     0.6377049  0.8393443  0.250
  Freund     2          39     0.6368852  0.8393443  0.125
  Freund     2          40     0.6438525  0.8327869  0.250
  Freund     2          41     0.6463115  0.8426230  0.125
  Freund     2          42     0.6422131  0.8262295  0.250
  Freund     2          43     0.6418033  0.8295082  0.250
  Freund     2          44     0.6446721  0.8295082  0.250
  Freund     2          45     0.6438525  0.8327869  0.250
  Freund     2          46     0.6098361  0.8327869  0.250
  Freund     2          47     0.6581967  0.8327869  0.250
  Freund     2          48     0.6434426  0.8393443  0.250
  Freund     2          49     0.6418033  0.8426230  0.250
  Freund     2          50     0.6430328  0.8393443  0.250
  Freund     2          51     0.6512295  0.8360656  0.250
  Freund     2          52     0.6389344  0.8295082  0.250
  Freund     2          53     0.6393443  0.8360656  0.250
  Freund     2          54     0.6610656  0.8459016  0.125
  Freund     2          55     0.6659836  0.8393443  0.125
  Freund     2          56     0.6745902  0.8393443  0.125
  Freund     2          57     0.6668033  0.8360656  0.250
  Freund     2          58     0.6586066  0.8360656  0.250
  Freund     2          59     0.6536885  0.8360656  0.250
  Freund     2          60     0.6618852  0.8327869  0.250
  Freund     2          61     0.6647541  0.8360656  0.250
  Freund     2          62     0.6684426  0.8426230  0.125
  Freund     2          63     0.6606557  0.8459016  0.250
  Freund     2          64     0.6450820  0.8393443  0.125
  Freund     2          65     0.6536885  0.8393443  0.250
  Freund     2          66     0.6553279  0.8426230  0.125
  Freund     2          67     0.6504098  0.8622951  0.125
  Freund     2          68     0.6553279  0.8524590  0.125
  Freund     2          69     0.6413934  0.8459016  0.250
  Freund     2          70     0.6446721  0.8426230  0.250
  Freund     2          71     0.6479508  0.8524590  0.250
  Freund     2          72     0.6483607  0.8360656  0.250
  Freund     2          73     0.6512295  0.8557377  0.250
  Freund     2          74     0.6385246  0.8491803  0.125
  Freund     2          75     0.6389344  0.8426230  0.250
  Freund     2          76     0.6389344  0.8590164  0.125
  Freund     2          77     0.6467213  0.8524590  0.250
  Freund     2          78     0.6467213  0.8459016  0.250
  Freund     2          79     0.6545082  0.8557377  0.125
  Freund     2          80     0.6573770  0.8557377  0.375
  Freund     2          81     0.6545082  0.8524590  0.375
  Freund     2          82     0.6528689  0.8557377  0.125
  Freund     2          83     0.6491803  0.8524590  0.250
  Freund     2          84     0.6438525  0.8491803  0.125
  Freund     2          85     0.6278689  0.8557377  0.250
  Freund     2          86     0.6311475  0.8557377  0.250
  Freund     2          87     0.6401639  0.8524590  0.250
  Freund     2          88     0.6393443  0.8557377  0.250
  Freund     2          89     0.6483607  0.8524590  0.250
  Freund     2          90     0.6487705  0.8426230  0.250
  Freund     2          91     0.6454918  0.8459016  0.250
  Freund     2          92     0.6491803  0.8393443  0.250
  Freund     2          93     0.6516393  0.8459016  0.250
  Freund     2          94     0.6446721  0.8491803  0.250
  Freund     2          95     0.6278689  0.8491803  0.250
  Freund     2          96     0.6274590  0.8557377  0.250
  Freund     2          97     0.6393443  0.8459016  0.250
  Freund     2          98     0.6389344  0.8491803  0.250
  Freund     2          99     0.6352459  0.8524590  0.250
  Freund     2         100     0.6368852  0.8557377  0.250
  Freund     3          20     0.7442623  0.8819672  0.250
  Freund     3          21     0.7395492  0.8590164  0.250
  Freund     3          22     0.7479508  0.8688525  0.375
  Freund     3          23     0.7668033  0.8655738  0.375
  Freund     3          24     0.7565574  0.8721311  0.250
  Freund     3          25     0.7471311  0.8688525  0.250
  Freund     3          26     0.7241803  0.8590164  0.250
  Freund     3          27     0.7213115  0.8819672  0.125
  Freund     3          28     0.7161885  0.8786885  0.125
  Freund     3          29     0.7247951  0.8655738  0.125
  Freund     3          30     0.7360656  0.8819672  0.125
  Freund     3          31     0.7237705  0.8721311  0.125
  Freund     3          32     0.6852459  0.8655738  0.125
  Freund     3          33     0.6610656  0.8721311  0.125
  Freund     3          34     0.6479508  0.8655738  0.125
  Freund     3          35     0.6524590  0.8721311  0.125
  Freund     3          36     0.6524590  0.8655738  0.125
  Freund     3          37     0.6565574  0.8786885  0.125
  Freund     3          38     0.6393443  0.8754098  0.125
  Freund     3          39     0.6442623  0.8786885  0.125
  Freund     3          40     0.6639344  0.8557377  0.125
  Freund     3          41     0.6680328  0.8557377  0.125
  Freund     3          42     0.6807377  0.8655738  0.125
  Freund     3          43     0.6745902  0.8655738  0.125
  Freund     3          44     0.6786885  0.8622951  0.125
  Freund     3          45     0.6709016  0.8491803  0.250
  Freund     3          46     0.7127049  0.8622951  0.250
  Freund     3          47     0.7122951  0.8688525  0.250
  Freund     3          48     0.7094262  0.8819672  0.250
  Freund     3          49     0.7258197  0.8786885  0.250
  Freund     3          50     0.6954918  0.8721311  0.125
  Freund     3          51     0.6950820  0.8688525  0.250
  Freund     3          52     0.6959016  0.8754098  0.125
  Freund     3          53     0.7118852  0.8786885  0.250
  Freund     3          54     0.7143443  0.8721311  0.250
  Freund     3          55     0.7077869  0.8786885  0.250
  Freund     3          56     0.7176230  0.8721311  0.250
  Freund     3          57     0.7200820  0.8786885  0.250
  Freund     3          58     0.7135246  0.8786885  0.375
  Freund     3          59     0.6983607  0.8754098  0.250
  Freund     3          60     0.6922131  0.8655738  0.250
  Freund     3          61     0.7016393  0.8655738  0.250
  Freund     3          62     0.7036885  0.8754098  0.250
  Freund     3          63     0.6872951  0.8688525  0.250
  Freund     3          64     0.6795082  0.8688525  0.250
  Freund     3          65     0.6729508  0.8557377  0.250
  Freund     3          66     0.6831967  0.8622951  0.250
  Freund     3          67     0.6729508  0.8524590  0.250
  Freund     3          68     0.6663934  0.8557377  0.250
  Freund     3          69     0.6520492  0.8524590  0.250
  Freund     3          70     0.6520492  0.8688525  0.250
  Freund     3          71     0.6504098  0.8524590  0.250
  Freund     3          72     0.6450820  0.8590164  0.250
  Freund     3          73     0.6385246  0.8524590  0.250
  Freund     3          74     0.6430328  0.8524590  0.250
  Freund     3          75     0.6442623  0.8655738  0.250
  Freund     3          76     0.6118852  0.8557377  0.250
  Freund     3          77     0.6061475  0.8622951  0.250
  Freund     3          78     0.6094262  0.8557377  0.250
  Freund     3          79     0.6065574  0.8590164  0.250
  Freund     3          80     0.6143443  0.8622951  0.250
  Freund     3          81     0.6077869  0.8557377  0.250
  Freund     3          82     0.6081967  0.8655738  0.250
  Freund     3          83     0.6106557  0.8622951  0.250
  Freund     3          84     0.6098361  0.8655738  0.250
  Freund     3          85     0.6172131  0.8590164  0.250
  Freund     3          86     0.6196721  0.8524590  0.250
  Freund     3          87     0.6188525  0.8590164  0.250
  Freund     3          88     0.6168033  0.8655738  0.250
  Freund     3          89     0.6176230  0.8655738  0.250
  Freund     3          90     0.6180328  0.8590164  0.250
  Freund     3          91     0.6225410  0.8557377  0.250
  Freund     3          92     0.6204918  0.8622951  0.250
  Freund     3          93     0.6217213  0.8590164  0.250
  Freund     3          94     0.6139344  0.8557377  0.250
  Freund     3          95     0.6139344  0.8590164  0.250
  Freund     3          96     0.6163934  0.8557377  0.250
  Freund     3          97     0.6237705  0.8524590  0.250
  Freund     3          98     0.6254098  0.8557377  0.250
  Freund     3          99     0.6250000  0.8491803  0.250
  Freund     3         100     0.6237705  0.8590164  0.250
  Freund     4          20     0.6028689  0.8524590  0.125
  Freund     4          21     0.6024590  0.8655738  0.125
  Freund     4          22     0.6131148  0.8819672  0.125
  Freund     4          23     0.6200820  0.8655738  0.125
  Freund     4          24     0.6131148  0.8622951  0.125
  Freund     4          25     0.6114754  0.8819672  0.125
  Freund     4          26     0.5909836  0.8852459  0.125
  Freund     4          27     0.5901639  0.8819672  0.125
  Freund     4          28     0.5782787  0.8622951  0.125
  Freund     4          29     0.5807377  0.8754098  0.125
  Freund     4          30     0.6118852  0.8754098  0.125
  Freund     4          31     0.5860656  0.8754098  0.125
  Freund     4          32     0.6045082  0.8918033  0.125
  Freund     4          33     0.5766393  0.8721311  0.125
  Freund     4          34     0.5860656  0.8754098  0.125
  Freund     4          35     0.5918033  0.8786885  0.125
  Freund     4          36     0.5811475  0.8918033  0.125
  Freund     4          37     0.5758197  0.8754098  0.125
  Freund     4          38     0.5778689  0.8852459  0.125
  Freund     4          39     0.5659836  0.8754098  0.125
  Freund     4          40     0.5799180  0.8786885  0.125
  Freund     4          41     0.6016393  0.8721311  0.125
  Freund     4          42     0.5971311  0.8819672  0.125
  Freund     4          43     0.5950820  0.8721311  0.125
  Freund     4          44     0.6012295  0.8721311  0.125
  Freund     4          45     0.6237705  0.8754098  0.125
  Freund     4          46     0.6327869  0.8819672  0.125
  Freund     4          47     0.6237705  0.8786885  0.125
  Freund     4          48     0.6254098  0.8721311  0.125
  Freund     4          49     0.6139344  0.8786885  0.125
  Freund     4          50     0.5938525  0.8852459  0.125
  Freund     4          51     0.5877049  0.8918033  0.125
  Freund     4          52     0.5655738  0.8983607  0.125
  Freund     4          53     0.5762295  0.8885246  0.125
  Freund     4          54     0.5864754  0.8918033  0.125
  Freund     4          55     0.5954918  0.8819672  0.125
  Freund     4          56     0.6131148  0.8983607  0.125
  Freund     4          57     0.6118852  0.8754098  0.125
  Freund     4          58     0.5922131  0.8983607  0.125
  Freund     4          59     0.5885246  0.8754098  0.250
  Freund     4          60     0.5823770  0.8819672  0.250
  Freund     4          61     0.5881148  0.8819672  0.250
  Freund     4          62     0.5938525  0.8950820  0.250
  Freund     4          63     0.5926230  0.8852459  0.250
  Freund     4          64     0.5860656  0.8852459  0.250
  Freund     4          65     0.5963115  0.8885246  0.250
  Freund     4          66     0.6000000  0.8819672  0.250
  Freund     4          67     0.6118852  0.8918033  0.125
  Freund     4          68     0.6102459  0.8918033  0.125
  Freund     4          69     0.6069672  0.8918033  0.250
  Freund     4          70     0.6073770  0.8852459  0.125
  Freund     4          71     0.6106557  0.8918033  0.125
  Freund     4          72     0.6053279  0.8885246  0.125
  Freund     4          73     0.6102459  0.8885246  0.125
  Freund     4          74     0.6106557  0.8950820  0.125
  Freund     4          75     0.6057377  0.8918033  0.125
  Freund     4          76     0.6073770  0.8918033  0.250
  Freund     4          77     0.6118852  0.8950820  0.125
  Freund     4          78     0.6106557  0.8983607  0.125
  Freund     4          79     0.6045082  0.8950820  0.125
  Freund     4          80     0.5954918  0.9049180  0.125
  Freund     4          81     0.5901639  0.8950820  0.125
  Freund     4          82     0.5918033  0.8983607  0.125
  Freund     4          83     0.5897541  0.8950820  0.125
  Freund     4          84     0.5918033  0.9016393  0.125
  Freund     4          85     0.5995902  0.9016393  0.125
  Freund     4          86     0.6004098  0.8983607  0.125
  Freund     4          87     0.5987705  0.9049180  0.125
  Freund     4          88     0.6045082  0.9016393  0.250
  Freund     4          89     0.5979508  0.9049180  0.250
  Freund     4          90     0.5913934  0.8983607  0.250
  Freund     4          91     0.5963115  0.9081967  0.250
  Freund     4          92     0.6053279  0.9049180  0.250
  Freund     4          93     0.6028689  0.9049180  0.250
  Freund     4          94     0.6036885  0.8983607  0.250
  Freund     4          95     0.6016393  0.9081967  0.250
  Freund     4          96     0.5995902  0.8950820  0.250
  Freund     4          97     0.6040984  0.9081967  0.250
  Freund     4          98     0.6106557  0.8983607  0.250
  Freund     4          99     0.6163934  0.9147541  0.250
  Freund     4         100     0.6209016  0.9016393  0.250
  Zhu        2          20     0.5911885  0.8557377  0.125
  Zhu        2          21     0.5956967  0.8229508  0.375
  Zhu        2          22     0.6010246  0.8557377  0.250
  Zhu        2          23     0.6063525  0.8426230  0.250
  Zhu        2          24     0.6038934  0.8426230  0.375
  Zhu        2          25     0.6116803  0.8327869  0.250
  Zhu        2          26     0.6104508  0.8196721  0.375
  Zhu        2          27     0.6022541  0.8229508  0.250
  Zhu        2          28     0.6063525  0.8426230  0.250
  Zhu        2          29     0.6172131  0.8360656  0.375
  Zhu        2          30     0.6368852  0.8295082  0.375
  Zhu        2          31     0.6303279  0.8163934  0.375
  Zhu        2          32     0.6098361  0.8262295  0.375
  Zhu        2          33     0.6319672  0.8360656  0.250
  Zhu        2          34     0.6454918  0.8327869  0.375
  Zhu        2          35     0.6372951  0.8262295  0.250
  Zhu        2          36     0.6750000  0.8163934  0.375
  Zhu        2          37     0.6803279  0.8262295  0.375
  Zhu        2          38     0.6815574  0.8327869  0.375
  Zhu        2          39     0.6786885  0.8393443  0.375
  Zhu        2          40     0.6852459  0.8262295  0.375
  Zhu        2          41     0.6799180  0.8229508  0.375
  Zhu        2          42     0.6815574  0.8426230  0.375
  Zhu        2          43     0.6848361  0.8426230  0.375
  Zhu        2          44     0.6926230  0.8459016  0.375
  Zhu        2          45     0.6959016  0.8393443  0.375
  Zhu        2          46     0.7045082  0.8360656  0.375
  Zhu        2          47     0.6954918  0.8295082  0.375
  Zhu        2          48     0.6938525  0.8459016  0.375
  Zhu        2          49     0.6995902  0.8393443  0.375
  Zhu        2          50     0.6959016  0.8327869  0.375
  Zhu        2          51     0.6946721  0.8360656  0.375
  Zhu        2          52     0.6967213  0.8295082  0.375
  Zhu        2          53     0.7004098  0.8459016  0.375
  Zhu        2          54     0.7143443  0.8393443  0.375
  Zhu        2          55     0.7188525  0.8491803  0.375
  Zhu        2          56     0.7163934  0.8360656  0.375
  Zhu        2          57     0.7139344  0.8491803  0.375
  Zhu        2          58     0.7209016  0.8491803  0.375
  Zhu        2          59     0.7217213  0.8360656  0.500
  Zhu        2          60     0.7225410  0.8295082  0.500
  Zhu        2          61     0.7176230  0.8393443  0.375
  Zhu        2          62     0.7307377  0.8393443  0.500
  Zhu        2          63     0.7245902  0.8360656  0.500
  Zhu        2          64     0.7168033  0.8360656  0.500
  Zhu        2          65     0.6950820  0.8229508  0.375
  Zhu        2          66     0.6852459  0.8163934  0.375
  Zhu        2          67     0.6889344  0.8196721  0.375
  Zhu        2          68     0.7069672  0.8196721  0.375
  Zhu        2          69     0.7364754  0.8196721  0.375
  Zhu        2          70     0.7397541  0.8163934  0.375
  Zhu        2          71     0.7483607  0.8163934  0.500
  Zhu        2          72     0.7459016  0.8196721  0.375
  Zhu        2          73     0.7446721  0.8327869  0.375
  Zhu        2          74     0.7442623  0.8295082  0.500
  Zhu        2          75     0.7450820  0.8459016  0.375
  Zhu        2          76     0.7327869  0.8196721  0.500
  Zhu        2          77     0.7315574  0.8262295  0.500
  Zhu        2          78     0.7405738  0.8295082  0.500
  Zhu        2          79     0.7385246  0.8327869  0.500
  Zhu        2          80     0.7295082  0.8459016  0.500
  Zhu        2          81     0.7262295  0.8459016  0.500
  Zhu        2          82     0.7229508  0.8459016  0.375
  Zhu        2          83     0.7295082  0.8459016  0.500
  Zhu        2          84     0.7303279  0.8426230  0.500
  Zhu        2          85     0.7299180  0.8524590  0.500
  Zhu        2          86     0.7229508  0.8426230  0.500
  Zhu        2          87     0.7266393  0.8393443  0.375
  Zhu        2          88     0.7352459  0.8229508  0.375
  Zhu        2          89     0.7536885  0.8393443  0.500
  Zhu        2          90     0.7491803  0.8426230  0.500
  Zhu        2          91     0.7364754  0.8360656  0.500
  Zhu        2          92     0.7315574  0.8262295  0.500
  Zhu        2          93     0.7631148  0.8426230  0.500
  Zhu        2          94     0.7622951  0.8557377  0.500
  Zhu        2          95     0.7614754  0.8459016  0.500
  Zhu        2          96     0.7622951  0.8491803  0.500
  Zhu        2          97     0.7594262  0.8360656  0.500
  Zhu        2          98     0.7540984  0.8393443  0.500
  Zhu        2          99     0.7557377  0.8360656  0.500
  Zhu        2         100     0.7553279  0.8590164  0.500
  Zhu        3          20     0.6778689  0.8360656  0.375
  Zhu        3          21     0.6827869  0.8196721  0.375
  Zhu        3          22     0.6823770  0.8524590  0.375
  Zhu        3          23     0.6811475  0.8459016  0.375
  Zhu        3          24     0.6762295  0.8622951  0.375
  Zhu        3          25     0.6737705  0.8557377  0.375
  Zhu        3          26     0.6987705  0.8524590  0.375
  Zhu        3          27     0.7098361  0.8557377  0.375
  Zhu        3          28     0.7303279  0.8491803  0.375
  Zhu        3          29     0.7241803  0.8459016  0.375
  Zhu        3          30     0.7377049  0.8459016  0.375
  Zhu        3          31     0.7397541  0.8491803  0.375
  Zhu        3          32     0.7184426  0.8393443  0.375
  Zhu        3          33     0.7266393  0.8557377  0.375
  Zhu        3          34     0.7200820  0.8491803  0.375
  Zhu        3          35     0.6979508  0.8557377  0.375
  Zhu        3          36     0.6971311  0.8491803  0.375
  Zhu        3          37     0.6901639  0.8557377  0.375
  Zhu        3          38     0.6926230  0.8393443  0.375
  Zhu        3          39     0.6877049  0.8524590  0.375
  Zhu        3          40     0.6938525  0.8557377  0.375
  Zhu        3          41     0.7233607  0.8590164  0.375
  Zhu        3          42     0.7061475  0.8491803  0.375
  Zhu        3          43     0.7233607  0.8524590  0.375
  Zhu        3          44     0.7237705  0.8622951  0.375
  Zhu        3          45     0.7192623  0.8590164  0.375
  Zhu        3          46     0.7135246  0.8557377  0.375
  Zhu        3          47     0.7364754  0.8655738  0.375
  Zhu        3          48     0.7438525  0.8557377  0.375
  Zhu        3          49     0.7471311  0.8590164  0.375
  Zhu        3          50     0.7352459  0.8491803  0.375
  Zhu        3          51     0.7372951  0.8590164  0.375
  Zhu        3          52     0.7381148  0.8655738  0.375
  Zhu        3          53     0.7233607  0.8622951  0.375
  Zhu        3          54     0.7258197  0.8622951  0.375
  Zhu        3          55     0.7290984  0.8557377  0.375
  Zhu        3          56     0.7360656  0.8721311  0.375
  Zhu        3          57     0.7360656  0.8721311  0.375
  Zhu        3          58     0.7385246  0.8622951  0.375
  Zhu        3          59     0.7344262  0.8557377  0.375
  Zhu        3          60     0.7348361  0.8655738  0.375
  Zhu        3          61     0.7274590  0.8655738  0.375
  Zhu        3          62     0.7266393  0.8688525  0.375
  Zhu        3          63     0.7196721  0.8622951  0.375
  Zhu        3          64     0.7155738  0.8590164  0.375
  Zhu        3          65     0.7229508  0.8655738  0.375
  Zhu        3          66     0.7270492  0.8524590  0.375
  Zhu        3          67     0.7258197  0.8622951  0.375
  Zhu        3          68     0.7225410  0.8491803  0.375
  Zhu        3          69     0.7225410  0.8655738  0.375
  Zhu        3          70     0.7168033  0.8524590  0.375
  Zhu        3          71     0.7143443  0.8524590  0.375
  Zhu        3          72     0.7180328  0.8524590  0.375
  Zhu        3          73     0.7163934  0.8524590  0.375
  Zhu        3          74     0.7139344  0.8557377  0.375
  Zhu        3          75     0.7139344  0.8622951  0.375
  Zhu        3          76     0.7114754  0.8622951  0.375
  Zhu        3          77     0.7036885  0.8590164  0.375
  Zhu        3          78     0.7020492  0.8655738  0.375
  Zhu        3          79     0.7053279  0.8590164  0.375
  Zhu        3          80     0.7118852  0.8590164  0.375
  Zhu        3          81     0.7061475  0.8655738  0.375
  Zhu        3          82     0.7188525  0.8557377  0.375
  Zhu        3          83     0.7102459  0.8622951  0.375
  Zhu        3          84     0.7081967  0.8590164  0.375
  Zhu        3          85     0.7192623  0.8524590  0.375
  Zhu        3          86     0.7192623  0.8557377  0.375
  Zhu        3          87     0.7188525  0.8557377  0.375
  Zhu        3          88     0.7069672  0.8557377  0.375
  Zhu        3          89     0.7065574  0.8557377  0.375
  Zhu        3          90     0.7094262  0.8557377  0.375
  Zhu        3          91     0.7151639  0.8491803  0.375
  Zhu        3          92     0.7180328  0.8524590  0.375
  Zhu        3          93     0.7245902  0.8557377  0.375
  Zhu        3          94     0.7245902  0.8557377  0.375
  Zhu        3          95     0.7221311  0.8524590  0.375
  Zhu        3          96     0.7168033  0.8459016  0.375
  Zhu        3          97     0.7168033  0.8459016  0.375
  Zhu        3          98     0.7139344  0.8491803  0.375
  Zhu        3          99     0.7151639  0.8557377  0.375
  Zhu        3         100     0.7196721  0.8491803  0.375
  Zhu        4          20     0.6426230  0.8491803  0.250
  Zhu        4          21     0.6463115  0.8491803  0.250
  Zhu        4          22     0.6557377  0.8393443  0.250
  Zhu        4          23     0.6159836  0.8524590  0.250
  Zhu        4          24     0.6143443  0.8524590  0.250
  Zhu        4          25     0.6225410  0.8622951  0.250
  Zhu        4          26     0.6036885  0.8590164  0.250
  Zhu        4          27     0.6028689  0.8524590  0.250
  Zhu        4          28     0.6086066  0.8590164  0.250
  Zhu        4          29     0.6163934  0.8459016  0.250
  Zhu        4          30     0.6118852  0.8393443  0.250
  Zhu        4          31     0.6139344  0.8491803  0.250
  Zhu        4          32     0.6122951  0.8524590  0.250
  Zhu        4          33     0.6086066  0.8491803  0.250
  Zhu        4          34     0.6090164  0.8491803  0.250
  Zhu        4          35     0.5905738  0.8524590  0.250
  Zhu        4          36     0.6004098  0.8491803  0.250
  Zhu        4          37     0.6032787  0.8393443  0.250
  Zhu        4          38     0.6020492  0.8459016  0.250
  Zhu        4          39     0.5877049  0.8491803  0.250
  Zhu        4          40     0.5815574  0.8491803  0.250
  Zhu        4          41     0.5844262  0.8524590  0.250
  Zhu        4          42     0.5864754  0.8459016  0.250
  Zhu        4          43     0.5807377  0.8524590  0.250
  Zhu        4          44     0.5627049  0.8491803  0.250
  Zhu        4          45     0.5614754  0.8491803  0.250
  Zhu        4          46     0.5540984  0.8491803  0.250
  Zhu        4          47     0.5680328  0.8491803  0.250
  Zhu        4          48     0.5721311  0.8524590  0.250
  Zhu        4          49     0.5762295  0.8491803  0.250
  Zhu        4          50     0.5774590  0.8524590  0.250
  Zhu        4          51     0.5741803  0.8524590  0.250
  Zhu        4          52     0.5704918  0.8557377  0.250
  Zhu        4          53     0.5713115  0.8622951  0.250
  Zhu        4          54     0.5815574  0.8622951  0.250
  Zhu        4          55     0.5815574  0.8590164  0.250
  Zhu        4          56     0.5668033  0.8590164  0.250
  Zhu        4          57     0.5663934  0.8590164  0.250
  Zhu        4          58     0.5782787  0.8590164  0.250
  Zhu        4          59     0.5737705  0.8557377  0.250
  Zhu        4          60     0.5778689  0.8557377  0.250
  Zhu        4          61     0.5754098  0.8557377  0.250
  Zhu        4          62     0.5762295  0.8622951  0.250
  Zhu        4          63     0.5676230  0.8754098  0.250
  Zhu        4          64     0.5655738  0.8688525  0.250
  Zhu        4          65     0.5610656  0.8688525  0.250
  Zhu        4          66     0.5729508  0.8590164  0.250
  Zhu        4          67     0.5774590  0.8491803  0.250
  Zhu        4          68     0.5893443  0.8524590  0.250
  Zhu        4          69     0.5836066  0.8524590  0.250
  Zhu        4          70     0.5676230  0.8557377  0.250
  Zhu        4          71     0.5668033  0.8557377  0.250
  Zhu        4          72     0.5684426  0.8557377  0.250
  Zhu        4          73     0.5819672  0.8590164  0.250
  Zhu        4          74     0.5741803  0.8491803  0.250
  Zhu        4          75     0.5766393  0.8524590  0.250
  Zhu        4          76     0.5581967  0.8557377  0.250
  Zhu        4          77     0.5672131  0.8557377  0.250
  Zhu        4          78     0.5651639  0.8655738  0.250
  Zhu        4          79     0.5602459  0.8590164  0.250
  Zhu        4          80     0.5639344  0.8524590  0.250
  Zhu        4          81     0.5602459  0.8557377  0.250
  Zhu        4          82     0.5631148  0.8590164  0.250
  Zhu        4          83     0.5631148  0.8590164  0.250
  Zhu        4          84     0.5643443  0.8590164  0.250
  Zhu        4          85     0.5643443  0.8622951  0.250
  Zhu        4          86     0.5647541  0.8590164  0.250
  Zhu        4          87     0.5590164  0.8557377  0.250
  Zhu        4          88     0.5590164  0.8491803  0.250
  Zhu        4          89     0.5500000  0.8524590  0.250
  Zhu        4          90     0.5434426  0.8524590  0.250
  Zhu        4          91     0.5651639  0.8524590  0.250
  Zhu        4          92     0.5807377  0.8491803  0.250
  Zhu        4          93     0.5704918  0.8524590  0.250
  Zhu        4          94     0.5745902  0.8491803  0.250
  Zhu        4          95     0.5762295  0.8557377  0.250
  Zhu        4          96     0.5754098  0.8491803  0.250
  Zhu        4          97     0.5848361  0.8524590  0.250
  Zhu        4          98     0.5795082  0.8491803  0.250
  Zhu        4          99     0.5676230  0.8524590  0.250
  Zhu        4         100     0.5684426  0.8491803  0.250

ROC was used to select the optimal model using the largest value.
The final values used for the model were mfinal = 23, maxdepth = 3
 and coeflearn = Freund.

    \end{Verbatim}

    
    \begin{verbatim}
Bootstrapped (1 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   No  Yes
       No  84.3  1.6
       Yes 13.1  1.0
                           
 Accuracy (average) : 0.853

    \end{verbatim}

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_113_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Confusion Matrix for adaboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}61}]:} caretPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}ada\PYZus{}smote\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{raw\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}caretPredictedClass\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  289   4
       Yes  71   7
                                          
               Accuracy : 0.7978          
                 95% CI : (0.7533, 0.8375)
    No Information Rate : 0.9704          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.1111          
 Mcnemar's Test P-Value : 2.517e-14       
                                          
            Sensitivity : 0.80278         
            Specificity : 0.63636         
         Pos Pred Value : 0.98635         
         Neg Pred Value : 0.08974         
             Prevalence : 0.97035         
         Detection Rate : 0.77898         
   Detection Prevalence : 0.78976         
      Balanced Accuracy : 0.71957         
                                          
       'Positive' Class : No              
                                          
    \end{verbatim}

    
    ROC plot for adaboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}62}]:} ada\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}ada\PYZus{}smote\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         ada\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}ada\PYZus{}pred\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         ada\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}ada\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}ada\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for adaboost with SMOTE\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}ada\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.8828283


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_117_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{boosting-with-xgboost-normal}{%
\subsubsection{Boosting with xgboost
(normal)}\label{boosting-with-xgboost-normal}}

Look for the documentation of library \textbf{xgboost}. The
\textbf{xgb.train()} function of xgboost implments `xgbTree'(Default)
and `xgbLinear'.

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/}{Refer}
  to know about the fine tuning parameters.
\item
  \href{http://topepo.github.io/caret/train-models-by-tag.html\#Boosting}{This}
  can also be referred to know about the parameter fine tuning.
\end{enumerate}

For xgbTree the fine tuning paramter consists of:

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  eta control the learning rate: scale the contribution of each tree by
  a factor of 0 \textless{} eta \textless{} 1 when it is added to the
  current approximation. Used to prevent overfitting by making the
  boosting process more conservative. Lower value for eta implies larger
  value for nrounds: low eta value means model more robust to
  overfitting but slower to compute. Default: 0.3
\item
  gamma minimum loss reduction required to make a further partition on a
  leaf node of the tree. the larger, the more conservative the algorithm
  will be.
\item
  max\_depth maximum depth of a tree. Default: 6
\item
  min\_child\_weight minimum sum of instance weight(hessian) needed in a
  child. If the tree partition step results in a leaf node with the sum
  of instance weight less than min\_child\_weight, then the building
  process will give up further partitioning. In linear regression mode,
  this simply corresponds to minimum number of instances needed to be in
  each node. The larger, the more conservative the algorithm will be.
  Default: 1
\item
  subsample subsample ratio of the training instance. Setting it to 0.5
  means that xgboost randomly collected half of the data instances to
  grow trees and this will prevent overfitting. It makes computation
  shorter (because less data to analyse). It is advised to use this
  parameter with eta and increase nround. Default: 1
\item
  colsample\_bytree subsample ratio of columns when constructing each
  tree. Default: 1
\item
  num\_parallel\_tree Experimental parameter. number of trees to grow
  per round. Useful to test Random Forest through Xgboost (set
  colsample\_bytree \textless{} 1, subsample \textless{} 1 and round =
  1) accordingly. Default: 1
\end{enumerate}
\end{quote}

The below code chunk sets some of the control parameters for adaboost

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}63}]:} objControl \PY{o}{\PYZlt{}\PYZhy{}} trainControl\PY{p}{(}method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{boot\PYZsq{}}\PY{p}{,} number \PY{o}{=} \PY{l+m}{1}\PY{p}{,}
                                    returnResamp\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{final\PYZsq{}}\PY{p}{,}
                                    summaryFunction \PY{o}{=} twoClassSummary\PY{p}{,}
                                    savePredictions \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    classProbs \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}64}]:} search\PYZus{}grid \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{expand.grid}\PY{p}{(}nrounds \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{70}\PY{o}{:}\PY{l+m}{150}\PY{p}{)}\PY{p}{,} max\PYZus{}depth \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{2}\PY{o}{:}\PY{l+m}{4}\PY{p}{)}\PY{p}{,}
                             eta \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.3}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}
                             gamma \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.03}\PY{p}{,}\PY{l+m}{0.09}\PY{p}{,} \PY{l+m}{0.12}\PY{p}{)}\PY{p}{,}
                             colsample\PYZus{}bytree \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{5}\PY{o}{:}\PY{l+m}{10}\PY{p}{)}\PY{o}{/}\PY{l+m}{10}\PY{p}{,}
                             min\PYZus{}child\PYZus{}weight \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{)}\PY{p}{,}
                             subsample \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    After setting the control paramters, the model is run

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}65}]:} num\PYZus{}cores \PY{o}{\PYZlt{}\PYZhy{}} makeCluster\PY{p}{(}detectCores\PY{p}{(}\PY{p}{)}\PY{l+m}{\PYZhy{}5}\PY{p}{)}
         registerDoParallel\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         tic\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Xtreme Boosting with Bootstrap Sampling\PYZdq{}}\PY{p}{)}
         
         \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         xg\PYZus{}model \PY{o}{\PYZlt{}\PYZhy{}} train\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{,}
                           method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{xgbTree\PYZsq{}}\PY{p}{,}
                           trControl\PY{o}{=}objControl\PY{p}{,}
                           tuneGrid \PY{o}{=} search\PYZus{}grid\PY{p}{,}
                           metric \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{ROC\PYZdq{}}\PY{p}{)}
         stopCluster\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         toc\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Xtreme Boosting with Bootstrap Sampling: 246.938 sec elapsed

    \end{Verbatim}

    Confusion Matrix for xgboost on train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}66}]:} xg\PYZus{}model\PY{o}{\PYZdl{}}bestTune
         confusionMatrix.train\PY{p}{(}xg\PYZus{}model\PY{p}{)}
         
         plot\PY{p}{(}varImp\PY{p}{(}xg\PYZus{}model\PY{p}{)}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable importance from xgboost\PYZdq{}}\PY{p}{,} col \PY{o}{=} \PY{l+m}{2}\PY{p}{,} lwd \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|lllllll}
  & nrounds & max\_depth & eta & gamma & colsample\_bytree & min\_child\_weight & subsample\\
\hline
	16628 & 92   & 4    & 0.1  & 0.03 & 1    & 1    & 0.5 \\
\end{tabular}


    
    
    \begin{verbatim}
Bootstrapped (1 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   No  Yes
       No  97.1  2.6
       Yes  0.3  0.0
                            
 Accuracy (average) : 0.9712

    \end{verbatim}

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_125_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Confusion Matrix for xgboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} caretPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}xg\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{raw\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}caretPredictedClass\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  359  10
       Yes   1   1
                                          
               Accuracy : 0.9704          
                 95% CI : (0.9476, 0.9851)
    No Information Rate : 0.9704          
    P-Value [Acc > NIR] : 0.57928         
                                          
                  Kappa : 0.1461          
 Mcnemar's Test P-Value : 0.01586         
                                          
            Sensitivity : 0.99722         
            Specificity : 0.09091         
         Pos Pred Value : 0.97290         
         Neg Pred Value : 0.50000         
             Prevalence : 0.97035         
         Detection Rate : 0.96765         
   Detection Prevalence : 0.99461         
      Balanced Accuracy : 0.54407         
                                          
       'Positive' Class : No              
                                          
    \end{verbatim}

    
    ROC plot for xgboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}68}]:} xg\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}xg\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         xg\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}xg\PYZus{}pred\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         xg\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}xg\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}xg\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for xgboost\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}xg\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.8805556


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_129_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{boosting-with-xgboost-up-sample}{%
\subsubsection{Boosting with xgboost (up
sample)}\label{boosting-with-xgboost-up-sample}}

The below code chunk sets some of the control parameters for adaboost

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}69}]:} objControl \PY{o}{\PYZlt{}\PYZhy{}} trainControl\PY{p}{(}method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{boot\PYZsq{}}\PY{p}{,} number \PY{o}{=} \PY{l+m}{1}\PY{p}{,}
                                    returnResamp\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{final\PYZsq{}}\PY{p}{,}
                                    summaryFunction \PY{o}{=} twoClassSummary\PY{p}{,}
                                    savePredictions \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    classProbs \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} sampling \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{up\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}70}]:} search\PYZus{}grid \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{expand.grid}\PY{p}{(}nrounds \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{70}\PY{o}{:}\PY{l+m}{150}\PY{p}{)}\PY{p}{,} max\PYZus{}depth \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{2}\PY{o}{:}\PY{l+m}{4}\PY{p}{)}\PY{p}{,}
                             eta \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.3}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}
                             gamma \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.03}\PY{p}{,}\PY{l+m}{0.09}\PY{p}{,} \PY{l+m}{0.12}\PY{p}{)}\PY{p}{,}
                             colsample\PYZus{}bytree \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{5}\PY{o}{:}\PY{l+m}{10}\PY{p}{)}\PY{o}{/}\PY{l+m}{10}\PY{p}{,}
                             min\PYZus{}child\PYZus{}weight \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{)}\PY{p}{,}
                             subsample \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    After setting the control paramters, the model is run

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}71}]:} num\PYZus{}cores \PY{o}{\PYZlt{}\PYZhy{}} makeCluster\PY{p}{(}detectCores\PY{p}{(}\PY{p}{)}\PY{l+m}{\PYZhy{}5}\PY{p}{)}
         registerDoParallel\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         tic\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Xtreme Boosting with Up Sampling\PYZdq{}}\PY{p}{)}
         
         \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         xg\PYZus{}up\PYZus{}model \PY{o}{\PYZlt{}\PYZhy{}} train\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{,}
                           method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{xgbTree\PYZsq{}}\PY{p}{,}
                           trControl\PY{o}{=}objControl\PY{p}{,}
                           tuneGrid \PY{o}{=} search\PYZus{}grid\PY{p}{,}
                           metric \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{ROC\PYZdq{}}\PY{p}{)}
         stopCluster\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         toc\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Xtreme Boosting with Up Sampling: 309.228 sec elapsed

    \end{Verbatim}

    Confusion Matrix for xgboost on train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}72}]:} xg\PYZus{}up\PYZus{}model\PY{o}{\PYZdl{}}bestTune
         confusionMatrix.train\PY{p}{(}xg\PYZus{}up\PYZus{}model\PY{p}{)}
         
         plot\PY{p}{(}varImp\PY{p}{(}xg\PYZus{}up\PYZus{}model\PY{p}{)}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable importance from xgboost with Up Sample\PYZdq{}}\PY{p}{,} col \PY{o}{=} \PY{l+m}{2}\PY{p}{,} lwd \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|lllllll}
  & nrounds & max\_depth & eta & gamma & colsample\_bytree & min\_child\_weight & subsample\\
\hline
	28595 & 71   & 2    & 0.3  & 0.12 & 0.9  & 4    & 0.5 \\
\end{tabular}


    
    
    \begin{verbatim}
Bootstrapped (1 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   No  Yes
       No  96.5  2.6
       Yes  1.0  0.0
                            
 Accuracy (average) : 0.9649

    \end{verbatim}

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_136_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Confusion Matrix for xgboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}73}]:} caretPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}xg\PYZus{}up\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{raw\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}caretPredictedClass\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  339   8
       Yes  21   3
                                         
               Accuracy : 0.9218         
                 95% CI : (0.8897, 0.947)
    No Information Rate : 0.9704         
    P-Value [Acc > NIR] : 1.00000        
                                         
                  Kappa : 0.1363         
 Mcnemar's Test P-Value : 0.02586        
                                         
            Sensitivity : 0.9417         
            Specificity : 0.2727         
         Pos Pred Value : 0.9769         
         Neg Pred Value : 0.1250         
             Prevalence : 0.9704         
         Detection Rate : 0.9137         
   Detection Prevalence : 0.9353         
      Balanced Accuracy : 0.6072         
                                         
       'Positive' Class : No             
                                         
    \end{verbatim}

    
    ROC plot for xgboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}74}]:} xg\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}xg\PYZus{}up\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         xg\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}xg\PYZus{}pred\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         xg\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}xg\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}xg\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for xgboost with Up Sample\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}xg\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.7833333


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_140_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{boosting-with-xgboost-down-sample}{%
\subsubsection{Boosting with xgboost (down
sample)}\label{boosting-with-xgboost-down-sample}}

The below code chunk sets some of the control parameters for adaboost

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}75}]:} objControl \PY{o}{\PYZlt{}\PYZhy{}} trainControl\PY{p}{(}method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{boot\PYZsq{}}\PY{p}{,} number \PY{o}{=} \PY{l+m}{1}\PY{p}{,}
                                    returnResamp\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{final\PYZsq{}}\PY{p}{,}
                                    summaryFunction \PY{o}{=} twoClassSummary\PY{p}{,}
                                    savePredictions \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    classProbs \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} sampling \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{down\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}76}]:} search\PYZus{}grid \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{expand.grid}\PY{p}{(}nrounds \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{70}\PY{o}{:}\PY{l+m}{150}\PY{p}{)}\PY{p}{,} max\PYZus{}depth \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{2}\PY{o}{:}\PY{l+m}{4}\PY{p}{)}\PY{p}{,}
                             eta \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.3}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}
                             gamma \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.03}\PY{p}{,}\PY{l+m}{0.09}\PY{p}{,} \PY{l+m}{0.12}\PY{p}{)}\PY{p}{,}
                             colsample\PYZus{}bytree \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{5}\PY{o}{:}\PY{l+m}{10}\PY{p}{)}\PY{o}{/}\PY{l+m}{10}\PY{p}{,}
                             min\PYZus{}child\PYZus{}weight \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{)}\PY{p}{,}
                             subsample \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    After setting the control paramters, the model is run

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}77}]:} num\PYZus{}cores \PY{o}{\PYZlt{}\PYZhy{}} makeCluster\PY{p}{(}detectCores\PY{p}{(}\PY{p}{)}\PY{l+m}{\PYZhy{}5}\PY{p}{)}
         registerDoParallel\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         tic\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Xtreme Boosting with Down Sampling\PYZdq{}}\PY{p}{)}
         \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         xg\PYZus{}down\PYZus{}model \PY{o}{\PYZlt{}\PYZhy{}} train\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{,}
                           method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{xgbTree\PYZsq{}}\PY{p}{,}
                           trControl\PY{o}{=}objControl\PY{p}{,}
                           tuneGrid \PY{o}{=} search\PYZus{}grid\PY{p}{,}
                           metric \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{ROC\PYZdq{}}\PY{p}{)}
         stopCluster\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         toc\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Xtreme Boosting with Down Sampling: 221.04 sec elapsed

    \end{Verbatim}

    Confusion Matrix for xgboost on train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}78}]:} xg\PYZus{}down\PYZus{}model\PY{o}{\PYZdl{}}bestTune
         confusionMatrix.train\PY{p}{(}xg\PYZus{}down\PYZus{}model\PY{p}{)}
         
         plot\PY{p}{(}varImp\PY{p}{(}xg\PYZus{}down\PYZus{}model\PY{p}{)}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable importance from xgboost with down sample\PYZdq{}}\PY{p}{,} col \PY{o}{=} \PY{l+m}{2}\PY{p}{,} lwd \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|lllllll}
  & nrounds & max\_depth & eta & gamma & colsample\_bytree & min\_child\_weight & subsample\\
\hline
	36099 & 123  & 3    & 0.3  & 0.12 & 1    & 1    & 0.5 \\
\end{tabular}


    
    
    \begin{verbatim}
Bootstrapped (1 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   No  Yes
       No  70.6  0.6
       Yes 26.8  1.9
                            
 Accuracy (average) : 0.7252

    \end{verbatim}

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_147_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Confusion Matrix for xgboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} caretPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}xg\PYZus{}down\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{raw\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}caretPredictedClass\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  232   1
       Yes 128  10
                                          
               Accuracy : 0.6523          
                 95% CI : (0.6014, 0.7007)
    No Information Rate : 0.9704          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.0839          
 Mcnemar's Test P-Value : <2e-16          
                                          
            Sensitivity : 0.64444         
            Specificity : 0.90909         
         Pos Pred Value : 0.99571         
         Neg Pred Value : 0.07246         
             Prevalence : 0.97035         
         Detection Rate : 0.62534         
   Detection Prevalence : 0.62803         
      Balanced Accuracy : 0.77677         
                                          
       'Positive' Class : No              
                                          
    \end{verbatim}

    
    ROC plot for xgboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}80}]:} xg\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}xg\PYZus{}down\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         xg\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}xg\PYZus{}pred\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         xg\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}xg\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}xg\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for xgboost with down sample\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}xg\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.8098485


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_151_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{boosting-with-xgboost-smote}{%
\subsubsection{Boosting with xgboost
(SMOTE)}\label{boosting-with-xgboost-smote}}

The below code chunk sets some of the control parameters for adaboost

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} objControl \PY{o}{\PYZlt{}\PYZhy{}} trainControl\PY{p}{(}method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{boot\PYZsq{}}\PY{p}{,} number \PY{o}{=} \PY{l+m}{1}\PY{p}{,}
                                    returnResamp\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{final\PYZsq{}}\PY{p}{,}
                                    summaryFunction \PY{o}{=} twoClassSummary\PY{p}{,}
                                    savePredictions \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    classProbs \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} sampling \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{smote\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}82}]:} search\PYZus{}grid \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{expand.grid}\PY{p}{(}nrounds \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{70}\PY{o}{:}\PY{l+m}{150}\PY{p}{)}\PY{p}{,} max\PYZus{}depth \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{2}\PY{o}{:}\PY{l+m}{4}\PY{p}{)}\PY{p}{,}
                             eta \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.3}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}
                             gamma \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.03}\PY{p}{,}\PY{l+m}{0.09}\PY{p}{,} \PY{l+m}{0.12}\PY{p}{)}\PY{p}{,}
                             colsample\PYZus{}bytree \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{5}\PY{o}{:}\PY{l+m}{10}\PY{p}{)}\PY{o}{/}\PY{l+m}{10}\PY{p}{,}
                             min\PYZus{}child\PYZus{}weight \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{)}\PY{p}{,}
                             subsample \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    After setting the control paramters, the model is run

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}83}]:} num\PYZus{}cores \PY{o}{\PYZlt{}\PYZhy{}} makeCluster\PY{p}{(}detectCores\PY{p}{(}\PY{p}{)}\PY{l+m}{\PYZhy{}5}\PY{p}{)}
         registerDoParallel\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         tic\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Xtreme Boosting with SMOTE Sampling\PYZdq{}}\PY{p}{)}
         \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         xg\PYZus{}smote\PYZus{}model \PY{o}{\PYZlt{}\PYZhy{}} train\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{,}
                           method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{xgbTree\PYZsq{}}\PY{p}{,}
                           trControl\PY{o}{=}objControl\PY{p}{,}
                           tuneGrid \PY{o}{=} search\PYZus{}grid\PY{p}{,}
                           metric \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{ROC\PYZdq{}}\PY{p}{)}
         stopCluster\PY{p}{(}num\PYZus{}cores\PY{p}{)}
         toc\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Xtreme Boosting with SMOTE Sampling: 261.745 sec elapsed

    \end{Verbatim}

    Confusion Matrix for xgboost on train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}84}]:} xg\PYZus{}smote\PYZus{}model\PY{o}{\PYZdl{}}bestTune
         confusionMatrix.train\PY{p}{(}xg\PYZus{}smote\PYZus{}model\PY{p}{)}
         
         plot\PY{p}{(}varImp\PY{p}{(}xg\PYZus{}smote\PYZus{}model\PY{p}{)}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable importance from xgboost with SMOTE\PYZdq{}}\PY{p}{,} col \PY{o}{=} \PY{l+m}{2}\PY{p}{,} lwd \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|lllllll}
  & nrounds & max\_depth & eta & gamma & colsample\_bytree & min\_child\_weight & subsample\\
\hline
	45049 & 82   & 2    & 0.5  & 0.03 & 0.8  & 2    & 0.5 \\
\end{tabular}


    
    
    \begin{verbatim}
Bootstrapped (1 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   No  Yes
       No  85.6  1.6
       Yes 11.8  1.0
                            
 Accuracy (average) : 0.8658

    \end{verbatim}

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_158_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Confusion Matrix for xgboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}85}]:} caretPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}xg\PYZus{}smote\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{raw\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}caretPredictedClass\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  286   4
       Yes  74   7
                                          
               Accuracy : 0.7898          
                 95% CI : (0.7447, 0.8301)
    No Information Rate : 0.9704          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.1055          
 Mcnemar's Test P-Value : 5.597e-15       
                                          
            Sensitivity : 0.79444         
            Specificity : 0.63636         
         Pos Pred Value : 0.98621         
         Neg Pred Value : 0.08642         
             Prevalence : 0.97035         
         Detection Rate : 0.77089         
   Detection Prevalence : 0.78167         
      Balanced Accuracy : 0.71540         
                                          
       'Positive' Class : No              
                                          
    \end{verbatim}

    
    ROC plot for xgboost on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}86}]:} xg\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}xg\PYZus{}smote\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         xg\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}xg\PYZus{}pred\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         xg\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}xg\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}xg\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for xgboost with SMOTE\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}xg\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.8194444


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_162_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}87}]:} toc\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Total Time for Bagging and Boosting: 1577.322 sec elapsed

    \end{Verbatim}

    \hypertarget{neural-network}{%
\subsection{Neural Network}\label{neural-network}}

\hypertarget{neural-network-implementation-to-find-the-manipulaters}{%
\subsubsection{Neural network implementation to find the
manipulaters}\label{neural-network-implementation-to-find-the-manipulaters}}

The below code chunk sets some of the control parameters

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}88}]:} objControl \PY{o}{\PYZlt{}\PYZhy{}} trainControl\PY{p}{(}method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{boot\PYZsq{}}\PY{p}{,} number \PY{o}{=} \PY{l+m}{1}\PY{p}{,}
                                    returnResamp\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{none\PYZsq{}}\PY{p}{,}
                                    summaryFunction \PY{o}{=} twoClassSummary\PY{p}{,}
                                    savePredictions \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    classProbs \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} allowParallel\PY{o}{=}\PY{k+kc}{FALSE}\PY{p}{)}
\end{Verbatim}


    Using search grid to fine tune the neural network. \textbf{Size} fine
tunes number of hidden units to tune and \textbf{decay} fine tunes
weight decay

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}89}]:} search\PYZus{}grid \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{expand.grid}\PY{p}{(}\PY{l+m}{.}decay \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.5}\PY{p}{,} \PY{l+m}{0.1}\PY{p}{,} \PY{l+m}{0.05}\PY{p}{)}\PY{p}{,} \PY{l+m}{.}size \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{2}\PY{p}{,} \PY{l+m}{3}\PY{p}{,} \PY{l+m}{4}\PY{p}{,}\PY{l+m}{5}\PY{p}{,}\PY{l+m}{6}\PY{p}{,}\PY{l+m}{7}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    After setting the control paramters, the model is run. If we use
\textbf{linout=TRUE} in \textbf{train()} the neural network builds a
regression model. \textbf{linout=FALSE} will make \textbf{nnet} use a
sigmodial function and all the predictions will be constrained between
\textbf{{[}0,1{]}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}90}]:} \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         
         nn\PYZus{}model \PY{o}{\PYZlt{}\PYZhy{}} train\PY{p}{(}model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{5}\PY{p}{]}\PY{p}{,} model\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{6}\PY{p}{]}\PY{p}{,}
                           method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{nnet\PYZsq{}}\PY{p}{,}
                           trControl\PY{o}{=}objControl\PY{p}{,}
                           metric \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{ROC\PYZdq{}}\PY{p}{,}
                           maxit \PY{o}{=} \PY{l+m}{1000}\PY{p}{,}
                           tuneGrid \PY{o}{=} search\PYZus{}grid\PY{p}{,}
                           trace \PY{o}{=} \PY{k+kc}{FALSE}\PY{p}{,}
                           linout \PY{o}{=} \PY{k+kc}{FALSE}\PY{p}{)}
\end{Verbatim}


    Confusion Matrix for Neural Network on train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{c+c1}{\PYZsh{}nn\PYZus{}model\PYZdl{}finalModel \PYZsh{}nn\PYZus{}model\PYZdl{}results}
         \PY{k+kp}{print}\PY{p}{(}nn\PYZus{}model\PY{p}{)}
         confusionMatrix.train\PY{p}{(}nn\PYZus{}model\PY{p}{)}
         plot\PY{p}{(}varImp\PY{p}{(}nn\PYZus{}model\PY{p}{)}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable importance from Neural Network\PYZdq{}}\PY{p}{,} col \PY{o}{=} \PY{l+m}{2}\PY{p}{,} lwd \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Neural Network 

868 samples
  5 predictor
  2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Bootstrapped (1 reps) 
Summary of sample sizes: 868 
Resampling results across tuning parameters:

  decay  size  ROC        Sens       Spec
  0.05   2     0.6418033  0.9967213  0   
  0.05   3     0.6221311  0.9967213  0   
  0.05   4     0.5393443  0.9967213  0   
  0.05   5     0.6524590  0.9967213  0   
  0.05   6     0.5799180  0.9967213  0   
  0.05   7     0.4901639  0.9967213  0   
  0.10   2     0.5676230  0.9967213  0   
  0.10   3     0.5991803  0.9967213  0   
  0.10   4     0.6245902  0.9967213  0   
  0.10   5     0.6131148  0.9934426  0   
  0.10   6     0.5979508  0.9934426  0   
  0.10   7     0.5778689  0.9967213  0   
  0.50   2     0.3692623  1.0000000  0   
  0.50   3     0.3905738  0.9967213  0   
  0.50   4     0.3942623  0.9967213  0   
  0.50   5     0.3840164  0.9967213  0   
  0.50   6     0.3926230  0.9967213  0   
  0.50   7     0.3770492  0.9967213  0   

ROC was used to select the optimal model using the largest value.
The final values used for the model were size = 5 and decay = 0.05.

    \end{Verbatim}

    
    \begin{verbatim}
Bootstrapped (1 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   No  Yes
       No  97.1  2.6
       Yes  0.3  0.0
                            
 Accuracy (average) : 0.9712

    \end{verbatim}

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_171_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Confusion Matrix for Neural Network on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}92}]:} caretPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}nn\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{raw\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}caretPredictedClass\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  359  11
       Yes   1   0
                                          
               Accuracy : 0.9677          
                 95% CI : (0.9442, 0.9832)
    No Information Rate : 0.9704          
    P-Value [Acc > NIR] : 0.690364        
                                          
                  Kappa : -0.005          
 Mcnemar's Test P-Value : 0.009375        
                                          
            Sensitivity : 0.9972          
            Specificity : 0.0000          
         Pos Pred Value : 0.9703          
         Neg Pred Value : 0.0000          
             Prevalence : 0.9704          
         Detection Rate : 0.9677          
   Detection Prevalence : 0.9973          
      Balanced Accuracy : 0.4986          
                                          
       'Positive' Class : No              
                                          
    \end{verbatim}

    
    ROC plot for Neural Network on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}93}]:} nn\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}nn\PYZus{}model\PY{p}{,} model\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         nn\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}nn\PYZus{}pred\PY{p}{,}model\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         nn\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}nn\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}nn\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for Neural Network\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}nn\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.830303


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_175_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{logistic-regression}{%
\subsection{Logistic Regression}\label{logistic-regression}}

The variables DSRI and GMI causes fitted probability to be numerically 0
or 1. Using less number of variables in the logistic regression.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}94}]:} lg\PYZus{}model\PYZus{}df \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{as.data.frame}\PY{p}{(}filter\PYZus{}data\PY{p}{[}\PY{p}{,}\PY{k+kt}{c}\PY{p}{(}\PY{c+c1}{\PYZsh{}\PYZdq{}DSRI\PYZdq{},}
                                                   \PY{c+c1}{\PYZsh{}\PYZdq{}GMI\PYZdq{},}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{AQI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{SGI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{DEPI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{SGAI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{ACCR\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{LEVI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{Manipulater\PYZdq{}}
         \PY{p}{)}\PY{p}{]}\PY{p}{)}
         lg\PYZus{}train\PYZus{}df \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{as.data.frame}\PY{p}{(}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{k+kt}{c}\PY{p}{(}\PY{c+c1}{\PYZsh{}\PYZdq{}DSRI\PYZdq{},}
                                                   \PY{c+c1}{\PYZsh{}\PYZdq{}GMI\PYZdq{},}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{AQI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{SGI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{DEPI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{SGAI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{ACCR\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{LEVI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{Manipulater\PYZdq{}}
         \PY{p}{)}\PY{p}{]}\PY{p}{)}
         lg\PYZus{}test\PYZus{}df \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{as.data.frame}\PY{p}{(}test\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{k+kt}{c}\PY{p}{(}\PY{c+c1}{\PYZsh{}\PYZdq{}DSRI\PYZdq{},}
                                                   \PY{c+c1}{\PYZsh{}\PYZdq{}GMI\PYZdq{},}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{AQI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{SGI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{DEPI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{SGAI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{ACCR\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{LEVI\PYZdq{}}\PY{p}{,}
                                                   \PY{l+s}{\PYZdq{}}\PY{l+s}{Manipulater\PYZdq{}}
         \PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    The below code chunk sets some of the control parameters

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}95}]:} objControl \PY{o}{\PYZlt{}\PYZhy{}} trainControl\PY{p}{(}method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{boot\PYZsq{}}\PY{p}{,} number\PY{o}{=}\PY{l+m}{1}\PY{p}{,}
                                    returnResamp\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{none\PYZsq{}}\PY{p}{,}
                                    summaryFunction \PY{o}{=} twoClassSummary\PY{p}{,}
                                    savePredictions \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}
                                    classProbs \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,}allowParallel\PY{o}{=}\PY{k+kc}{FALSE}\PY{p}{)}
\end{Verbatim}


    After setting the control paramters, the model is run

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}96}]:} \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{4121}\PY{p}{)}
         lg\PYZus{}model \PY{o}{\PYZlt{}\PYZhy{}} train\PY{p}{(}lg\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{6}\PY{p}{]}\PY{p}{,} lg\PYZus{}train\PYZus{}df\PY{p}{[}\PY{p}{,}\PY{l+m}{7}\PY{p}{]}\PY{p}{,}
                              method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{glmStepAIC\PYZsq{}}\PY{p}{,}
                              trControl\PY{o}{=}objControl\PY{p}{,}
                              metric \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{ROC\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Start:  AIC=196.08
.outcome \textasciitilde{} AQI + SGI + DEPI + SGAI + ACCR + LEVI

       Df Deviance    AIC
- LEVI  1   183.09 195.09
<none>      182.08 196.08
- DEPI  1   184.44 196.44
- AQI   1   186.75 198.75
- SGI   1   189.02 201.02
- SGAI  1   215.58 227.58
- ACCR  1   233.01 245.01

Step:  AIC=195.09
.outcome \textasciitilde{} AQI + SGI + DEPI + SGAI + ACCR

       Df Deviance    AIC
<none>      183.09 195.09
- DEPI  1   185.39 195.39
- AQI   1   187.75 197.75
- SGI   1   189.69 199.69
- SGAI  1   216.46 226.46
- ACCR  1   233.53 243.53
Start:  AIC=216.72
.outcome \textasciitilde{} AQI + SGI + DEPI + SGAI + ACCR + LEVI

       Df Deviance    AIC
- DEPI  1   203.35 215.35
<none>      202.72 216.72
- LEVI  1   206.04 218.04
- SGI   1   209.63 221.63
- AQI   1   214.90 226.90
- SGAI  1   215.20 227.20
- ACCR  1   217.84 229.84

Step:  AIC=215.35
.outcome \textasciitilde{} AQI + SGI + SGAI + ACCR + LEVI

       Df Deviance    AIC
<none>      203.35 215.35
- LEVI  1   206.72 216.72
- SGI   1   210.29 220.29
- AQI   1   217.09 227.09
- SGAI  1   217.61 227.61
- ACCR  1   218.99 228.99

    \end{Verbatim}

    Confusion Matrix for logistic regression on train set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}97}]:} \PY{k+kp}{print}\PY{p}{(}lg\PYZus{}model\PY{p}{)}
         confusionMatrix.train\PY{p}{(}lg\PYZus{}model\PY{p}{)}
         plot\PY{p}{(}varImp\PY{p}{(}lg\PYZus{}model\PY{p}{)}\PY{p}{,} main \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Variable importance from Logistic Regression\PYZdq{}}\PY{p}{,} col \PY{o}{=} \PY{l+m}{2}\PY{p}{,} lwd \PY{o}{=} \PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Generalized Linear Model with Stepwise Feature Selection 

868 samples
  6 predictor
  2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Bootstrapped (1 reps) 
Summary of sample sizes: 868 
Resampling results:

  ROC        Sens       Spec
  0.6983607  0.9934426  0.25


    \end{Verbatim}

    
    \begin{verbatim}
Bootstrapped (1 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   No  Yes
       No  96.8  1.9
       Yes  0.6  0.6
                            
 Accuracy (average) : 0.9744

    \end{verbatim}

    
    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_183_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Confusion Matrix for logistic regression on test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}98}]:} caretPredictedClass \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}lg\PYZus{}model\PY{p}{,} lg\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{raw\PYZdq{}}\PY{p}{)}
         confusionMatrix\PY{p}{(}caretPredictedClass\PY{p}{,}lg\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction  No Yes
       No  360   9
       Yes   0   2
                                          
               Accuracy : 0.9757          
                 95% CI : (0.9545, 0.9888)
    No Information Rate : 0.9704          
    P-Value [Acc > NIR] : 0.337237        
                                          
                  Kappa : 0.3013          
 Mcnemar's Test P-Value : 0.007661        
                                          
            Sensitivity : 1.0000          
            Specificity : 0.1818          
         Pos Pred Value : 0.9756          
         Neg Pred Value : 1.0000          
             Prevalence : 0.9704          
         Detection Rate : 0.9704          
   Detection Prevalence : 0.9946          
      Balanced Accuracy : 0.5909          
                                          
       'Positive' Class : No              
                                          
    \end{verbatim}

    
    ROC plot for logistic regression

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}99}]:} lg\PYZus{}pred \PY{o}{\PYZlt{}\PYZhy{}} predict\PY{p}{(}lg\PYZus{}model\PY{p}{,} lg\PYZus{}test\PYZus{}df\PY{p}{,} type \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{prob\PYZdq{}}\PY{p}{)}\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}
         lg\PYZus{}prediction \PY{o}{\PYZlt{}\PYZhy{}} prediction\PY{p}{(}lg\PYZus{}pred\PY{p}{,}lg\PYZus{}test\PYZus{}df\PY{o}{\PYZdl{}}Manipulater\PY{p}{)}
         lg\PYZus{}perf \PY{o}{\PYZlt{}\PYZhy{}} performance\PY{p}{(}lg\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{tpr\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{fpr\PYZdq{}}\PY{p}{)}
         
         plot\PY{p}{(}lg\PYZus{}perf\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ROC Curve for Logistic Regression\PYZdq{}}\PY{p}{,}col\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         abline\PY{p}{(}a\PY{o}{=}\PY{l+m}{0}\PY{p}{,}b\PY{o}{=}\PY{l+m}{1}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{2}\PY{p}{,}lty\PY{o}{=}\PY{l+m}{3}\PY{p}{,}col\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{black\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}AUC for the ROC plot}
         performance\PY{p}{(}lg\PYZus{}prediction\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{auc\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
An object of class "performance"
Slot "x.name":
[1] "None"

Slot "y.name":
[1] "Area under the ROC curve"

Slot "alpha.name":
[1] "none"

Slot "x.values":
list()

Slot "y.values":
[[1]]
[1] 0.935101


Slot "alpha.values":
list()

    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_187_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    End of document

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
