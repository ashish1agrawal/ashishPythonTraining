---
title: "Regression Concepts Using R"
author: "Kumar Rahul"
output: word_document
---

## In this exercise, we will use the Placement data and understand the following:

> 1. Importing the datset from a csv file
2. Understanding the strucutre and summary of the data
3. Typecasting a variable to a proper data type
4. Analyzing the corelation amongst variables
5. Releveling the factor variable and understand its impact
6. Building the regression model
7. Writing the model equation and interpreting the model summary
8. Analayzing the statistics to acertain the validity of the model

There are bugs/missing code in the entire exercise. The participants are expected to work upon them.
***
***

## Here are some useful links:

> 1. Refer [link](http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm) to know more about different ways of dummy variable coding
2. [Read](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/dummy.htm) about interaction variable coding
3. Refer [link](http://www.statmethods.net/input/valuelabels.html) to know about adding lables to factors
4. Refer [link](http://stackoverflow.com/questions/2342472/recode-relevel-data-frame-factors-with-different-levels) to relevel factor variables
5. [Read](http://stats.stackexchange.com/questions/88485/variable-is-significant-through-stepwise-regression-but-not-in-final-models-sum) about the issues in stepwise regression
6. The issues arising out of multi-colinearity is discussed  [here](http://blog.minitab.com/blog/understanding-statistics/handling-multicollinearity-in-regression-analysis) or  [here](https://onlinecourses.science.psu.edu/stat501/node/343)
7. The residual diagonstic can be interpreted from [here](http://data.library.virginia.edu/diagnostic-plots/)
8. [Read](https://onlinecourses.science.psu.edu/stat501/node/337) to understand the distinction between **outliers** and **influential cases**
9. Issues with rJava installation may get resolved by following [link](https://www.r-statistics.com/2012/08/how-to-load-the-rjava-package-after-the-error-java_home-cannot-be-determined-from-the-registry/) or by [link](http://stackoverflow.com/questions/27661325/unable-to-load-rjava-on-r)

***

# Code starts here

We are going to use below mentioned libraries for demonstrating logistic regression:

```{r libraries, echo=TRUE, message=FALSE, warning=FALSE}
library(stats)    #for regression
library(caret)    #for data partition
library(car)      #for VIF
library(sandwich) #for variance, covariance matrix
```


## Data Import and Manipulation

### 1. Importing a data set 

_Give the correct path to the data_
```{r readData, echo=TRUE,tidy=TRUE}
raw.data <- read.csv("/Users/Rahul/Documents/Datasets/Placement Raw Data.csv", header = TRUE,sep = ",",na.strings = c(""," ", "NA"))
```

Note that `echo = FALSE` parameter prevents printing the R code that generated the
plot.

### 2a. Structure and Summary of the dataset

```{r summarizeData, echo=TRUE,tidy=TRUE}
str(raw.data)
summary(raw.data)
```

Create a new data frame and store the raw data copy. This is being done to have a copy of the raw data intact for further manipulation if needed.

```{r createDataCopy, echo=TRUE,tidy=TRUE}
filter.data <- na.omit(raw.data) # listwise deletion of missing
```

### 3a. Typecasting of variables

The structure of the raw dataset showed that Normalized SSC attribute is defined as a factor variable. Normalized_SSC is the percentage of marks obtained by the students in SSC. It should have been a numeric variable. To convert this into numeric variable we may do *typecasting* of variables.

Here, typecasting will be done in two steps:

> 1. Remove the **%** sign from the column using **gsub()** and store it in a new variable
2. Typecast the new variable as a numeric variable using **as.numeric()**

```{r varTypecast, echo=TRUE,tidy=TRUE}
filter.data$IMP_Normalized_SSC <- gsub("%","", filter.data$Normalized_SSC)
filter.data$IMP_Normalized_SSC <- as.numeric(filter.data$IMP_Normalized_SSC)
```

### 3b. Correlation among Variables

From the numeric attribute in the data, it will of interest to analyze the variables which are corelated to each other. High corelation amongst variable may result in the issue of **multi-colinearity** in the model. 

```{r corMatrix, echo=TRUE,tidy=TRUE}
correlationMatrix <- cor(filter.data[,c(3,6,9,13,14,16:18,20)])
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.7)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff = 0.7, names = TRUE)
print(highlyCorrelated)
```

### 3c. Relevel

By default, the base category/reference category selected is ordered alphabetically. In this code chunk we are just changing the base category for Gender variable. In the first run, do not execute the below code chunk. Second run, execute the below code chunk and check its impact on the model equation.

The base category can be releveled using the function **relevel()**.

```{r relevelCategory, echo=TRUE,tidy=TRUE}
filter.data$Gender <- relevel(filter.data$Gender, ref = "M")
```


### 4. Create train and test datase vct

#### Reserve 80% for **_training_** and 20% of **_test_**

_Correct the error in the below code chunk_
```{r createDataPartition, echo=TRUE,tidy=TRUE}
set.seed(2341)
trainIndex <- createDataPartition(filter.data$Salary, p = 0.80, list = FALSE)
data.train <- filter.data[trainIndex,]
data.test <- filter.data[-trainIndex,]
```

We can pull the specific attribute needed to build the model in another data frame. This agian is more of a hygine practice to not touch the **train** and **test** data set directly. 

_Correct the error in the below code chunk_
```{r variableUsedinTraining, echo=TRUE,tidy=TRUE}
reg.train.data <- as.data.frame(data.train[,c("Gender",
                                             "Percent_SSC",
                                             "Board_SSC",
                                             "Percent_HSC",
                                             "Board_HSC",
                                             "Stream_HSC",
                                             "Percent_Degree", 
                                             "Course_Degree",
                                             "Experience_Yrs", 
                                             "Entrance_Test",
                                             "Percentile_ET", 
                                             "Percent_MBA",
                                             "Specialization_MBA",
                                             "Marks_Communication",
                                             "Marks_Projectwork",
                                             "Marks_BOCA",
                                             "Salary"
)])
```

_Correct the error in the below code chunk_
```{r variableUsedinTesting, echo=TRUE, tidy=TRUE}
reg.test.data <- as.data.frame(data.test[,c("Gender",
                                             "Percent_SSC",
                                             "Board_SSC",
                                             "Percent_HSC",
                                             "Board_HSC",
                                             "Stream_HSC",
                                             "Percent_Degree", 
                                             "Course_Degree",
                                             "Experience_Yrs", 
                                             "Entrance_Test",
                                             "Percentile_ET", 
                                             "Percent_MBA",
                                             "Specialization_MBA",
                                             "Marks_Communication",
                                             "Marks_Projectwork",
                                             "Marks_BOCA",
                                             "Salary"
)])
```

***

## Model building: Using the **lm()** function

The actual model building starts now. Note that we are demonstrating the strategy of building a step wise model (forward selection and backward elimination)  using the **lm()** function

```{r buildModel, echo=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}
#Null Model
noModel <- lm(Salary ~ 1,data = reg.train.data)

#Full Model
RegModelFull = lm(Salary ~ .
                     , data = reg.train.data)

#Stepwise - Forward selection backward elimination
RegModelStepwise <- step(noModel, list(lower = formula(noModel),
                                         upper = formula(RegModelFull)),
                           direction = "both",trace = 0)
```


***

## Model Evaluation

### 1. Model summary of Train Data

Checking the if the model satisfies the assumpations of Linear Regression Model. Note that this evaluation is on training data.

The model summary gives the equation of the model as well as helps test the assumption that beta coeffiecents are not statically zero.
```{r modelStats,tidy=TRUE}
summary(RegModelStepwise)
```


You may ignore the below code chunk. This code is more to understand how the standard error of beta coefficients are calculated. **vcov()** is used to compute the variance covariance matrix of the fitted object. **cov2cor()** is used to scale the covariance matrix into the correponding correlation matrix. In the matrix generated out of the below code chunk:

> 1. The diagonal values are the variance of the variable to itself. 
2. The square root of the diagonal values gives the standard error associated with the estimates. 
3. The non diagonal elements are the covariance of the estimated  

```{r, echo=TRUE}
vcov(RegModelStepwise)
sqrt(diag(vcov(RegModelStepwise)))
#cov2cor(vcov(RegModelStepwise))
```

### 2. The residual analysis

The error term diagnostic is critical to understanding the behaviour of linear regression models. The two critical assumptions of linear regression are:

>1. Error term should be normally distributed
2. Error term should have constant variance (**homoscedasticity**)

The **plot()** function when used on the regression object model gives us four different plots. The two important one to analyze there are:

1. Normal Q-Q
2. Scale-Location

#####1. Normal Q_Q plot
This plot shows if the error terms are normally distributed. In case, of normal distribution, the dots should appear close to the straight line with not much of a deviation.

#####2. Scale-Location
Also known as spread location plot, it shows if the residuals are equally spread along the range of predictors. It is desirable to see a horizontal straight line with with randomly spread points.

**The other two plots are:**

#####3. Residual vs. Fitted
There could be a non linear relationship between predictor variable (Xs) and the outcome variable (Y). This non linear relationship can show up in this plot which may suggest that the model is mis-specified. It is desirable to see a horizontal straight line with with randomly spread points.

#####4. Residual vs. Leverage
The regression line can be influenced by outliers (extreme values in Y) or by data points with high leverage (extreme values in X). Not all the extreme values are influential cases in regression analysis.

Even if data has extreme values, it may not be influential to determine the regression line. On the flip side, some cases could be very influential even if they do not seem to be an outlier. Influential cases are identified by cook's distance. In the plot, look for for outlying values at the upper right corner or at the lower right corner (cases outside of a dashed line i.e. Cookâ€™s distance). 

```{r variableDeclaration, echo=TRUE}
plot(RegModelStepwise)

#hist(residuals(RegModelStepwise), main = "Residuals", col = 'blue')
```

##### Visual inspection to check for heteroscedasticity in error terms

You may ignore the below code chuck. This is an elaboration of the scale-location plot obtained before.

```{r modelOptimalCutOff, echo=FALSE,tidy=TRUE}
plot(predict(RegModelStepwise), residuals(RegModelStepwise), main = "Scale-Location")
#yhat <- RegModelStepwise$fitted.values
#plot(yhat, res) #same plot as above
```

##### Multi-colinearity

Variance Inflation Factor (VIF) is a measure of how much the variance of the estimated regrression coeffiecients are inflated as compared to when the predicator variable are not linearly related. 

> VIF = 1 : Not Correlated
> 1<VIF<5 : Moderately Correlated
> 5<VIF<=10: Highly Correlated

_The square root of the VIF tells you how much larger the standard error is, compared with what it would be if that variable were uncorrelated with the other predictor variables in the model._

Say, if the square root of the VIF is 2.5; this means that the standard error for the coefficient of that predictor variable is 2.5 times as large as it would be if the predictor variable were uncorrelated with the other predictor variables

Generally the issue of multi-colinearity wil not arise, if the corelation amongst variable has been analyzed before model building and the one amongst the corelated variable has been dropped from the data.
```{r}
vif(RegModelStepwise)
```

### 3. Model Validation on the Test Data

The **predict** function is used to get the predicted response on the new dataset. 
You may get an error message if the test data has got any new levels which was not there in the training set. This generally happens when the data has categorical variable with multiple levels.
```{r modelValidation, echo=TRUE,tidy=TRUE}
reg.test.data <- subset(reg.test.data, reg.test.data$Entrance_Test != "G-MAT")
RegTestPrediction = predict(RegModelStepwise, reg.test.data, 
                            interval = "confidence",
                            level = 0.95,
                            type = "response")
print(RegTestPrediction)
```


#### End of Document

***
***